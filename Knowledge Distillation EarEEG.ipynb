{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij4Cglx7hnfe"
      },
      "source": [
        "## Get Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLKQsBn-hgDN",
        "outputId": "86468511-afe8-4069-f2c1-f9ec470d2e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "1.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "from torch import optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "#import helpers\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils import data\n",
        "# import cv2\n",
        "import math\n",
        "from PIL import Image\n",
        "!pip install einops\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import glob\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEt0q1-Aw01_",
        "outputId": "9a592443-cff7-4949-839d-461d1772d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQALzcVPxnwe"
      },
      "outputs": [],
      "source": [
        "!cd '/content/drive/MyDrive/EarEEG'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YSLmONqhyVM",
        "outputId": "425f896b-1396-4f56-acd0-1c794a3f5772"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp-EzMvdOlRO"
      },
      "source": [
        "### Neptune (Ignore this block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fWY3FOxqv7",
        "outputId": "59906c3b-0ff5-410f-a399-345b5f85e3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.16.0.tar.gz (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 42.5 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.21.42-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.42\n",
            "  Downloading botocore-1.24.42-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 39.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.42->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 35.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (3.13)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.6.0)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.8.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.5)\n",
            "Building wheels for collected packages: neptune-client, future\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.16.0-py2.py3-none-any.whl size=565726 sha256=d34739c9737efa87ab4de91ca5c04a7e6380f8eed4766d0087d9804a2ad6200a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/96/77/26728c98b5324009116ccf98ab63768eecbd3f03bd60404e90\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=27aca8fa533c5c932ca4fbc65fef62b75396e6c3f544c3ffa8954ef63443e37b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built neptune-client future\n",
            "Installing collected packages: arrow, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, jsonpointer, jmespath, isoduration, fqdn, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 PyJWT-2.3.0 arrow-1.2.2 boto3-1.21.42 botocore-1.24.42 bravado-11.0.3 bravado-core-5.17.0 fqdn-1.5.1 future-0.18.2 gitdb-4.0.9 isoduration-20.11.0 jmespath-1.0.0 jsonpointer-2.3 jsonref-0.2 monotonic-1.6 neptune-client-0.16.0 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.5.2 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-2.7.4 uri-template-1.2.0 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install neptune-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EheutgQjObJ4",
        "outputId": "58a5828d-6076-4e32-d660-9ef5256b51f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/mithunjha/KD-v2/e/KDV2-147\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ],
      "source": [
        "import neptune.new as neptune\n",
        "\n",
        "# run = neptune.init(\n",
        "#     project=\"jathurshan0330/Ear-KD\",\n",
        "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYmRmNjE0Zi0xMDRkLTRlNzUtYmIxNi03NzM2ODBlZDc5NTMifQ==\",\n",
        "# )  # your credentials\n",
        "\n",
        "# run = neptune.init(\n",
        "#     project=\"jathurshan0330/Ear-Const\",\n",
        "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYmRmNjE0Zi0xMDRkLTRlNzUtYmIxNi03NzM2ODBlZDc5NTMifQ==\",\n",
        "# )  # your credentials\n",
        "\n",
        "# your credentials\n",
        "run = neptune.init(\n",
        "    project=\"mithunjha/KD-v2\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwZjA0YTVhOC02ZGVlLTQ0NTktOWY3NS03YzFhZWUxY2M4MTcifQ==\",\n",
        ") \n",
        "# run = neptune.init(\n",
        "#     project=\"mithunjha/EarEEG\",\n",
        "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwZjA0YTVhOC02ZGVlLTQ0NTktOWY3NS03YzFhZWUxY2M4MTcifQ==\",\n",
        "# ) \n",
        "\n",
        "# run = neptune.init(\n",
        "#     project=\"mithunjha/EEGcrossval\",\n",
        "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwZjA0YTVhOC02ZGVlLTQ0NTktOWY3NS03YzFhZWUxY2M4MTcifQ==\",\n",
        "# ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV_9lNe_x0Hd",
        "outputId": "c7075565-a053-4fe4-d0fb-094b0312c505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "experiment = \"KDV2-147\"   #Change This\n",
        "!mkdir \"/content/drive/MyDrive/EarEEG/KDV2-147\"  # Change This"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARyAHPPkl0JS"
      },
      "source": [
        "## Ear-EEG Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPhzU0VCr6s5",
        "outputId": "df712c45-c68b-4d45-8cd2-e4f82a028c39"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nnjsmn430zA"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/EarEEG\n",
        "import Datasets.sleepedf_dataset\n",
        "from Datasets.sleepedf_dataset import extract_stft, read_h5py,extract_stft_multiple_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRMUYvMcw02D"
      },
      "outputs": [],
      "source": [
        "def split_data(data_list,train_list,val_list):\n",
        "    data_list = np.array(data_list)\n",
        "    train_data_list = data_list[train_list]\n",
        "    val_data_list = data_list[val_list]\n",
        "    return train_data_list, val_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2ToCu67mVJp"
      },
      "outputs": [],
      "source": [
        "train_data_list = [1,2,3,5,6,7,8]  #4 discarded \n",
        "val_data_list = [0]\n",
        "\n",
        "psg_sig_list = glob.glob('/content/drive/MyDrive/EarEEG/x*.h5')\n",
        "psg_sig_list.sort()\n",
        "[train_psg_list, val_psg_list] = split_data(psg_sig_list,train_data_list,val_data_list)\n",
        "print(train_psg_list)\n",
        "print(val_psg_list)\n",
        "\n",
        "\n",
        "label_list = glob.glob('/content/drive/MyDrive/EarEEG/y*.h5')\n",
        "label_list.sort()\n",
        "[train_label_list, val_label_list] = split_data(label_list,train_data_list,val_data_list)\n",
        "print(train_label_list)\n",
        "print(val_label_list)\n",
        "\n",
        "mean_list = glob.glob('/content/drive/MyDrive/EarEEG/mean*.h5')\n",
        "mean_list.sort()\n",
        "[train_mean_list, val_mean_list] = split_data(mean_list,train_data_list,val_data_list)\n",
        "print(train_mean_list)\n",
        "print(val_mean_list)\n",
        "\n",
        "sd_list = glob.glob('/content/drive/MyDrive/EarEEG/std*.h5')\n",
        "sd_list.sort()\n",
        "[train_sd_list, val_sd_list] = split_data(sd_list,train_data_list,val_data_list)\n",
        "print(train_sd_list)\n",
        "print(val_sd_list)\n",
        "\n",
        "id_list = glob.glob('/content/drive/MyDrive/EarEEG/len*.h5')\n",
        "id_list.sort()\n",
        "[train_id_list, val_id_list] = split_data(id_list,train_data_list,val_data_list)\n",
        "print(train_id_list)\n",
        "print(val_id_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcfdfX1Jw02E"
      },
      "outputs": [],
      "source": [
        "rejection_list = read_h5py(\"/content/drive/MyDrive/EarEEG/rejected.h5\")\n",
        "print(rejection_list)\n",
        "\n",
        "[train_reject_list, val_reject_list] = split_data(rejection_list,train_data_list,val_data_list)\n",
        "print(train_reject_list)\n",
        "print(val_reject_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy4Dktagw02F"
      },
      "source": [
        "## Edited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5oaS7pTw02F"
      },
      "outputs": [],
      "source": [
        "def EEG_process(data,rejected_list,i=0):\n",
        "    _,_,_,_,_,ELA,ELE,ELI,ERA,ERG,ERE,ERI,_,_,_,_,_,ELB,ELG,ELK,ERB,ERK,_ = data\n",
        "    reject = rejected_list[i]\n",
        "    ear_eeg = [ELA,ELE,ELI,ERA,ERG,ERE,ERI,ELB,ELG,ELK,ERB,ERK]\n",
        "    \n",
        "    for j in range (len(reject)):\n",
        "        ear_eeg[j]=ear_eeg[j]*reject[j]\n",
        "    \n",
        "\n",
        "    Left_ear = (ear_eeg[0] + ear_eeg[1] + ear_eeg[2] + ear_eeg[7] + ear_eeg[8] + ear_eeg[9])/np.count_nonzero([reject[0],reject[1],reject[2],reject[7],reject[8],reject[9]])   # (ELA + ELE + ELI + ELB + ELG + ELK)/6\n",
        "    Right_ear = (ear_eeg[3] + ear_eeg[4] + ear_eeg[5] + ear_eeg[6] + ear_eeg[10] + ear_eeg[11])/np.count_nonzero([reject[3],reject[4],reject[5],reject[6],reject[10],reject[11]]) # (ERA + ERG + ERE + ERI + ERB + ERK)/6\n",
        "    L_R = Left_ear - Right_ear\n",
        "\n",
        "    if np.count_nonzero([reject[0],reject[7]]) != 0 :\n",
        "        L_E = (ear_eeg[0]+ear_eeg[7])/np.count_nonzero([reject[0],reject[7]]) - (ear_eeg[1]+ear_eeg[2]+ear_eeg[8]+ear_eeg[9])/np.count_nonzero([reject[1],reject[2],reject[8],reject[9]]) #(ELA + ELB)/2 - (ELE + ELI + ELG + ELK)/4\n",
        "    else:\n",
        "        L_E = np.zeros(data.shape[1])\n",
        "    if np.count_nonzero([reject[3],reject[10]]) != 0 :\n",
        "        R_E = (ear_eeg[3]+ear_eeg[10])/np.count_nonzero([reject[3],reject[10]]) - (ear_eeg[4]+ear_eeg[5]+ear_eeg[6]+ear_eeg[11])/np.count_nonzero([reject[4],reject[5],reject[6],reject[11]]) # (ERA + ERB)/2 - (ERE + ERI + ERG + ERK)/4\n",
        "    else:\n",
        "        R_E = np.zeros(data.shape[1])\n",
        "\n",
        "    return L_R, L_E, R_E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOxM-9p5w02G"
      },
      "outputs": [],
      "source": [
        "class EarEEG_MultiChan_Dataset(Dataset):\n",
        "    def __init__(self, psg_file, label_file, device, mean_l = None, sd_l = None,\n",
        "                reject_list = None, transform=None, target_transform=None, sub_wise_norm = False):\n",
        "        \"\"\"\n",
        "      # C3,O1,A1,F3,LOC,ELA,ELE,ELI,ERA,ERG,ERE,ERI,C4,O2,A2,F4,ROC,ELB,ELG,ELK,ERB,ERK,CHIN12\n",
        "        \"\"\"\n",
        "        # Get the data\n",
        "        for i in range(len(psg_file)):\n",
        "          if i == 0:\n",
        "            if reject_list.any():\n",
        "              psg_signal = read_h5py(psg_file[i])\n",
        "              L_R, L_E, R_E = EEG_process(psg_signal,reject_list,i=i)\n",
        "              # print(L_R.shape,L_E.shape,R_E.shape)\n",
        "              # break\n",
        "              L_R = np.reshape(L_R,(L_R.shape[0],1,L_R.shape[1]))\n",
        "              L_E = np.reshape(L_E,(L_E.shape[0],1,L_E.shape[1]))\n",
        "              R_E = np.reshape(R_E,(R_E.shape[0],1,R_E.shape[1]))\n",
        "\n",
        "              # L_R = (L_R - np.mean(L_R))/np.std(L_R)\n",
        "              # L_E = (L_E - np.mean(L_E))/np.std(L_E)\n",
        "              # R_E = (R_E - np.mean(R_E))/np.std(R_E)\n",
        "\n",
        "              # eeg_f3_o1 = np.reshape(psg_signal[3] - psg_signal[1],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_f4_o2 = np.reshape(psg_signal[15] - psg_signal[13],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_a1 = np.reshape(psg_signal[0] - psg_signal[2],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_a2 = np.reshape(psg_signal[12] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_o1 = np.reshape(psg_signal[0] - psg_signal[1],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_o2 = np.reshape(psg_signal[12] - psg_signal[13],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_a2 = np.reshape(psg_signal[0] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_a1 = np.reshape(psg_signal[12] - psg_signal[2],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              eeg_c3_c4 = np.reshape(psg_signal[0] - psg_signal[12],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              eeg_a1_a2 = np.reshape(psg_signal[2] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "\n",
        "\n",
        "              eog =  np.reshape(psg_signal[16] - psg_signal[4],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # emg = np.reshape(psg_signal[22],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # self.psg = np.concatenate((L_R,eog,emg),axis = 1) \n",
        "              # self.psg = np.concatenate((L_R,L_E,R_E),axis = 1) \n",
        "              # self.psg = np.concatenate((L_R,L_E,R_E,eeg_c3_a2,eeg_c4_a1,eog),axis = 1) \n",
        "              self.psg = np.concatenate((L_R,L_E,R_E,eeg_a1_a2,eeg_c3_c4,eog),axis = 1) \n",
        "              # self.psg = np.concatenate((L_R,L_E,R_E,eeg_a1_a2,eeg_c3_a1,eeg_c4_a2),axis = 1) \n",
        "\n",
        "              # self.psg = np.concatenate((eeg_c3_a1,eeg_c4_a2,eog),axis = 1) \n",
        "              self.labels = read_h5py(label_file[i])\n",
        "            else:  \n",
        "              self.psg = read_h5py(psg_file[i])\n",
        "              self.labels = read_h5py(label_file[i])\n",
        "          else:\n",
        "            if reject_list.any():\n",
        "              psg_signal = read_h5py(psg_file[i])\n",
        "              L_R, L_E, R_E = EEG_process(psg_signal,reject_list,i=i)\n",
        "              L_R = np.reshape(L_R,(L_R.shape[0],1,L_R.shape[1]))\n",
        "              L_E = np.reshape(L_E,(L_E.shape[0],1,L_E.shape[1]))\n",
        "              R_E = np.reshape(R_E,(R_E.shape[0],1,R_E.shape[1]))\n",
        "\n",
        "              # L_R = (L_R - np.mean(L_R))/np.std(L_R)\n",
        "              # L_E = (L_E - np.mean(L_E))/np.std(L_E)\n",
        "              # R_E = (R_E - np.mean(R_E))/np.std(R_E)\n",
        "\n",
        "              # eeg_f3_o1 = np.reshape(psg_signal[3] - psg_signal[1],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_f4_o2 = np.reshape(psg_signal[15] - psg_signal[13],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_a1 = np.reshape(psg_signal[0] - psg_signal[2],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_a2 = np.reshape(psg_signal[12] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_o1 = np.reshape(psg_signal[0] - psg_signal[1],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_o2 = np.reshape(psg_signal[12] - psg_signal[13],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              eeg_c3_c4 = np.reshape(psg_signal[0] - psg_signal[12],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c3_a2 = np.reshape(psg_signal[0] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # eeg_c4_a1 = np.reshape(psg_signal[12] - psg_signal[2],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              eeg_a1_a2 = np.reshape(psg_signal[2] - psg_signal[14],(L_R.shape[0],1,L_R.shape[-1])) ###Jathu edit\n",
        "\n",
        "\n",
        "              eog =  np.reshape(psg_signal[16] - psg_signal[4],(L_R.shape[0],1,L_R.shape[-1]))   ###Jathu edit\n",
        "              # emg = np.reshape(psg_signal[22],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "              # psg_comb = np.concatenate((L_R,eog,emg),axis = 1) \n",
        "              # psg_comb = np.concatenate((L_R,L_E,R_E),axis = 1) \n",
        "              # psg_comb = np.concatenate((L_R,L_E,R_E,eeg_c3_o1,eeg_c4_o2,eog),axis = 1) ###Jathu edit\n",
        "              psg_comb = np.concatenate((L_R,L_E,R_E,eeg_a1_a2,eeg_c3_c4,eog),axis = 1) \n",
        "              # psg_comb = np.concatenate((L_R,L_E,R_E,eeg_a1_a2,eeg_c3_a1,eeg_c4_a2),axis = 1)\n",
        "\n",
        "              # psg_comb = np.concatenate((L_R,L_E,R_E,eeg_c3_a2,eeg_c4_a1,eog),axis = 1) \n",
        "              # scalpeeg_comb = np.concatenate((eeg_c3_o1,eeg_c4_o2,eog),axis = 1) \n",
        "              # psg_comb = np.concatenate((eeg_c3_a1,eeg_c4_a2,eog),axis = 1) \n",
        "\n",
        "              # self.scalpeeg = np.concatenate((self.scalpeeg,scalpeeg_comb),axis = 0)\n",
        "              self.psg = np.concatenate((self.psg,psg_comb),axis = 0)\n",
        "              self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "            else:\n",
        "              self.psg = np.concatenate((self.psg, read_h5py(psg_file[i])),axis = 1)\n",
        "              self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "        self.labels = torch.from_numpy(self.labels)\n",
        "        print(f\"Data shape : {self.psg.shape}\")\n",
        "        print(f\"Labels shape : {self.labels.shape}\")\n",
        "        bin_labels = np.bincount(self.labels)\n",
        "        print(f\"Labels count: {bin_labels/self.labels.shape[0]}\")\n",
        "        print(f\"Labels count weights: {1/(bin_labels/self.labels.shape[0])}\")\n",
        "        \n",
        "        if sub_wise_norm == True:\n",
        "          print(f\"Reading Subject wise mean and sd\")\n",
        "          for i in range(len(mean_l)):\n",
        "            if i == 0:\n",
        "              self.mean_l  = read_h5py(mean_l[i])\n",
        "              self.sd_l = read_h5py(sd_l[i])\n",
        "            else:\n",
        "              self.mean_l = np.concatenate((self.mean_l, read_h5py(mean_l[i])),axis = 1)\n",
        "              self.sd_l = np.concatenate((self.sd_l, read_h5py(sd_l[i])),axis = 1)\n",
        "\n",
        "          print(f\"Shapes of Mean  : {self.mean_l.shape}\")\n",
        "          print(f\"Shapes of Sd    : {self.sd_l.shape}\")\n",
        "        else:     \n",
        "          self.mean = mean_l\n",
        "          self.sd = sd_l\n",
        "          print(f\"Mean : {self.mean} and SD {self.sd}\")  \n",
        "\n",
        "        self.sub_wise_norm = sub_wise_norm\n",
        "        self.device = device\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        psg_data = self.psg[idx] \n",
        "        # print(psg_data.shape)\n",
        "        # psg_data[0,:] = (psg_data[0,:]-np.min(psg_data[0,:]))/(np.max(psg_data[0,:])-np.min(psg_data[0,:]))    \n",
        "        # psg_data[1,:] = (psg_data[1,:]-np.min(psg_data[1,:]))/(np.max(psg_data[1,:])-np.min(psg_data[1,:])) \n",
        "        # psg_data[2,:] = (psg_data[2,:]-np.min(psg_data[2,:]))/(np.max(psg_data[2,:])-np.min(psg_data[2,:])) \n",
        "        # psg_data[3,:] = (psg_data[3,:]-np.min(psg_data[3,:]))/(np.max(psg_data[3,:])-np.min(psg_data[3,:]))    \n",
        "        # psg_data[4,:] = (psg_data[4,:]-np.min(psg_data[4,:]))/(np.max(psg_data[4,:])-np.min(psg_data[4,:])) \n",
        "        # psg_data[5,:] = (psg_data[5,:]-np.min(psg_data[5,:]))/(np.max(psg_data[5,:])-np.min(psg_data[5,:])) \n",
        "  \n",
        "        # eeg_data = self.scalpeeg[idx]   \n",
        "        # print(data.shape)\n",
        "        label = self.labels[idx,]\n",
        "        \n",
        "        if self.sub_wise_norm ==True:\n",
        "          psg_data = (psg_data - self.mean_l[idx]) / self.sd_l[idx]\n",
        "        elif self.mean and self.sd:\n",
        "          psg_data = (psg_data - self.mean_l) / self.sd_l\n",
        "\n",
        "        if self.transform:\n",
        "            psg_data = self.transform(psg_data)\n",
        "            # eeg_data = self.transform(eeg_data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return psg_data, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELCDezM1Qkjs"
      },
      "outputs": [],
      "source": [
        "train_dataset = EarEEG_MultiChan_Dataset(psg_file = train_psg_list, \n",
        "                                       label_file = train_label_list, device = device, \n",
        "                                    #    mean_l = val_mean_list, sd_l = val_sd_list,###Jathu\n",
        "                                       reject_list = train_reject_list,\n",
        "                                       sub_wise_norm = False,###Jathu\n",
        "                                       transform=transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        # #  transforms.Normalize(\n",
        "                                        #     (0.5,), (0.5,))\n",
        "                                        ]) )\n",
        "val_dataset = EarEEG_MultiChan_Dataset(psg_file = val_psg_list, \n",
        "                                       label_file = val_label_list, device = device, \n",
        "                                    #    mean_l = val_mean_list, sd_l = val_sd_list,###Jathu\n",
        "                                       reject_list = val_reject_list,###Jathu\n",
        "                                       sub_wise_norm = False,\n",
        "                                       transform=transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        # #  transforms.Normalize(\n",
        "                                        #     (0.5,), (0.5,))\n",
        "                                        ]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpXQ66ipml-q"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "val_data_loader = data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3c5Neaqmu6-"
      },
      "outputs": [],
      "source": [
        "psg_data, label = next(iter(train_data_loader))\n",
        "print(f\"PSG batch shape: {psg_data.size()}\")\n",
        "print(f\"Labels batch shape: {label.size()}\")\n",
        "\n",
        "t = np.arange(0,30,1/200)\n",
        "\n",
        "plt.figure(figsize = (40,20))\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(t,(psg_data[0,0,0,:]/psg_data[0,0,0,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Ear EEG : L-R\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.ticks('off')\n",
        "# plt.show()\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "# plt.figure(figsize = (20,10))\n",
        "plt.plot(t,(psg_data[0,0,1,:]/psg_data[0,0,1,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Ear EEG : L\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.show()\n",
        "plt.subplot(3, 2, 5)\n",
        "# plt.figure(figsize = (20,10))\n",
        "plt.plot(t,(psg_data[0,0,2,:]/psg_data[0,0,2,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Ear EEG : R\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.show()\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "# plt.figure(figsize = (20,10))6 C301\n",
        "plt.plot(t,(psg_data[0,0,3,:]/psg_data[0,0,3,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Scalp EEG : A1-A2\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.ylim((-200,200))\n",
        "# plt.show()\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "# plt.figure(figsize = (20,10))2,c4o2\n",
        "plt.plot(t,(psg_data[0,0,4,:]/psg_data[0,0,4,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Scalp EEG : C3-O1\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.ylim((-200,200))\n",
        "# plt.show()\n",
        "\n",
        "plt.subplot(3, 2, 6)\n",
        "# plt.figure(figsize = (20,10))4, a1a2\n",
        "plt.plot(t,(psg_data[0,0,5,:]/psg_data[0,0,5,:].max()).squeeze(),'k',linewidth=2)\n",
        "plt.title(\"Scalp EEG : C4-O2\",fontsize = 30)\n",
        "plt.ylim((-5,5))\n",
        "# plt.ylim((-200,200))\n",
        "plt.show()\n",
        "\n",
        "# plt.title(f\"Label {label[0].squeeze()}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"EarEEG Minimum :{psg_data[0,0,0,:].min()}\")\n",
        "print(f\"EarEEG Maximum :{psg_data[0,0,0,:].max()}\")\n",
        "print(f\"EOG Minimum :{psg_data[0,0,1,:].min()}\")\n",
        "print(f\"EOG Maximum :{psg_data[0,0,1,:].max()}\")\n",
        "print(f\"EMG Minimum :{psg_data[0,0,2,:].min()}\")\n",
        "print(f\"EMG Maximum :{psg_data[0,0,2,:].max()}\")\n",
        "\n",
        "\n",
        "print(f\"EarEEG Mean :{torch.mean(psg_data[0,0,0,:])}\")\n",
        "print(f\"EarEEG Standard Deviation :{torch.std(psg_data[0,0,0,:])}\")\n",
        "print(f\"EOG Mean :{torch.mean(psg_data[0,0,1,:])}\")\n",
        "print(f\"EOG Standard Deviation :{torch.std(psg_data[0,0,1,:])}\")\n",
        "print(f\"EMG Mean :{torch.mean(psg_data[0,0,2,:])}\")\n",
        "print(f\"EMG Standard Deviation :{torch.std(psg_data[0,0,2,:])}\")\n",
        "\n",
        "\n",
        "psg_data, label = next(iter(val_data_loader))\n",
        "print(f\"PSG batch shape: {psg_data.size()}\")\n",
        "print(f\"Labels batch shape: {label.size()}\")\n",
        "\n",
        "t = np.arange(0,30,1/200)\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.plot(t,psg_data[0,0,0,:].squeeze())\n",
        "plt.plot(t,psg_data[0,0,1,:].squeeze())\n",
        "plt.plot(t,psg_data[0,0,2,:].squeeze())\n",
        "plt.show()\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.plot(t,psg_data[0,0,3,:].squeeze())\n",
        "plt.plot(t,psg_data[0,0,4,:].squeeze())\n",
        "plt.plot(t,psg_data[0,0,5,:].squeeze())\n",
        "plt.title(f\"Label {label[0].squeeze()}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f\"EarEEG Minimum :{psg_data[0,0,0,:].min()}\")\n",
        "print(f\"EarEEG Maximum :{psg_data[0,0,0,:].max()}\")\n",
        "print(f\"EOG Minimum :{psg_data[0,0,1,:].min()}\")\n",
        "print(f\"EOG Maximum :{psg_data[0,0,1,:].max()}\")\n",
        "print(f\"EMG Minimum :{psg_data[0,0,2,:].min()}\")\n",
        "print(f\"EMG Maximum :{psg_data[0,0,2,:].max()}\")\n",
        "\n",
        "\n",
        "print(f\"EarEEG Mean :{torch.mean(psg_data[0,0,0,:])}\")\n",
        "print(f\"EarEEG Standard Deviation :{torch.std(psg_data[0,0,0,:])}\")\n",
        "print(f\"EOG Mean :{torch.mean(psg_data[0,0,1,:])}\")\n",
        "print(f\"EOG Standard Deviation :{torch.std(psg_data[0,0,1,:])}\")\n",
        "print(f\"EMG Mean :{torch.mean(psg_data[0,0,2,:])}\")\n",
        "print(f\"EMG Standard Deviation :{torch.std(psg_data[0,0,2,:])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBetoZVHrI_Y"
      },
      "source": [
        "### Classification Model Cross Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W951Jf11rI_a"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import MultiheadAttention\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNh0vvj5lD2l"
      },
      "outputs": [],
      "source": [
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPB9rA-PlWLA"
      },
      "outputs": [],
      "source": [
        "#input ==> 32, 1, 1, 3000,, b==> batch, e==> embedding, s==> seq length\n",
        "class Window_Embedding(nn.Module): \n",
        "    def __init__(self, in_channels: int = 1, window_size: int = 50, emb_size: int = 64):\n",
        "        super(Window_Embedding, self).__init__()\n",
        "\n",
        "        self.projection_1 =  nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//4, kernel_size = window_size, stride = window_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(emb_size//4),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        self.projection_2 =  nn.Sequential(#################\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//8, kernel_size = 5, stride = 5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//8, emb_size//4, kernel_size = 5, stride = 5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//4, (emb_size-emb_size//4)//2, kernel_size = 2, stride = 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d((emb_size-emb_size//4)//2),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        \n",
        "        self.projection_3 =  nn.Sequential(#################\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//4, kernel_size = 25, stride = 25),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//4, (emb_size-emb_size//4)//2, kernel_size =2, stride = 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d((emb_size-emb_size//4)//2),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        \n",
        "        \n",
        "        self.projection_4 = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(emb_size, emb_size, kernel_size = 1, stride = 1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(emb_size),\n",
        "            Rearrange('b e s -> b s e'),)\n",
        "            \n",
        "        #in=>B,64,60 out=>B,64,61\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "        self.arrange1 = Rearrange('b s e -> s b e')\n",
        "        #in=>61,B,64 out=>61,B,64\n",
        "        self.pos = PositionalEncoding(d_model=emb_size)\n",
        "        #in=>61,B,64 out=>B,61,64\n",
        "        self.arrange2 = Rearrange('s b e -> b s e ')\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x.squeeze().unsqueeze(dim = 1)\n",
        "        # print(x.shape)\n",
        "        b,_, _ = x.shape\n",
        "        x_1 = self.projection_1(x)  ########################\n",
        "        x_2 = self.projection_2(x) ###########\n",
        "        x_3 = self.projection_3(x) \n",
        "        # print(x_local.shape,x_global.shape)\n",
        "        x = torch.cat([x_1,x_2,x_3],dim = 1)##### 2)\n",
        "        x = self.projection_4(x) \n",
        "        # print(x.shape)\n",
        "        cls_tokens = repeat(self.cls_token, '() s e -> b s e', b=b)\n",
        "        # print(cls_tokens.shape)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # print(x.shape)\n",
        "        # add position embedding\n",
        "        x = self.arrange1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.pos(x)\n",
        "        # print(x.shape)\n",
        "        x = self.arrange2(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fGtBxkllbUR"
      },
      "outputs": [],
      "source": [
        "#input ==>(b,s,e)=>(32, 61, 64,) \n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Intra_modal_atten(nn.Module): \n",
        "    def __init__(self, d_model=64, nhead=8, dropout=0.1,\n",
        "                 layer_norm_eps=1e-5, window_size = 25, First = True,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        super(Intra_modal_atten, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "       \n",
        "        if First == True:\n",
        "            self.window_embed = Window_Embedding(in_channels = 1, window_size = window_size, emb_size = d_model)\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)  \n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True,\n",
        "                                            **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout) \n",
        "        self.First = First\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.First == True:\n",
        "            src = self.window_embed(x)\n",
        "        else:\n",
        "            src = x\n",
        "        # print(src.shape)\n",
        "        # src = self.norm(src)  #####\n",
        "        # print(src.shape)\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout(src2)\n",
        "        out = self.norm(out)   ########\n",
        "        return out                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNJFWByZlerI"
      },
      "outputs": [],
      "source": [
        "##Cross Modal Attention\n",
        "#input ==>(b,s,e)=>(32, 2, 64,) ==> Class tokens of EEG and EOG after intra modal attention\n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Cross_modal_atten(nn.Module): \n",
        "    def __init__(self, d_model=64, nhead=8, dropout=0.1,\n",
        "                 layer_norm_eps=1e-5, First = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "\n",
        "        super(Cross_modal_atten, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "\n",
        "        if First == True:\n",
        "            self.cls_token = nn.Parameter(torch.randn(1,1, d_model)) ######\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)  \n",
        "        self.cross_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True,\n",
        "                                            **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout) \n",
        "        self.First = First\n",
        "\n",
        "    def forward(self, x1: Tensor,x2: Tensor) -> Tensor:\n",
        "        # print(x1.shape,x2.shape)\n",
        "        if len(x1.shape) == 2:\n",
        "            x = torch.cat([x1.unsqueeze(dim=1), x2.unsqueeze(dim=1)], dim=1)\n",
        "        else:\n",
        "            x = torch.cat([x1, x2.unsqueeze(dim=1)], dim=1)\n",
        "        # print(x.shape)\n",
        "        b,_, _ = x.shape\n",
        "        if self.First == True:\n",
        "            cls_tokens = repeat(self.cls_token, '() s e -> b s e', b=b)  ######\n",
        "            # print(cls_tokens.shape)\n",
        "            # prepend the cls token to the input\n",
        "            src = torch.cat([cls_tokens, x], dim=1)  #####\n",
        "        else:\n",
        "            src = x\n",
        "        # print(src.shape)\n",
        "        # src = self.norm(src)#####(src)\n",
        "        # print(src.shape)\n",
        "        src2 = self.cross_attn(src, src, src)[0]\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout(src2)\n",
        "        out = self.norm(out)\n",
        "        return out  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeS3fLtTlhS1"
      },
      "outputs": [],
      "source": [
        "##Feed Forward Networks\n",
        "#input ==>(b,s,e)=>(32, 61, 64,) \n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Feed_forward(nn.Module): \n",
        "    def __init__(self, d_model=64,dropout=0.1,dim_feedforward=512,\n",
        "                 layer_norm_eps=1e-5,\n",
        "                 device=None, dtype=None) -> None:\n",
        "\n",
        "        super(Feed_forward, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # print(x.shape)\n",
        "        # src = self.norm(x)  ######\n",
        "        src = x\n",
        "        # print(src.shape)\n",
        "        src2 = self.linear2(self.dropout1(self.relu(self.linear1(src))))\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout2(src2)\n",
        "        out = self.norm(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u00fqCBDyVx8"
      },
      "outputs": [],
      "source": [
        "# # Best Model so far fine tuning\n",
        "# class Cross_Transformer_Network(nn.Module):\n",
        "#     def __init__(self,d_model = 64, dim_feedforward=512,window_size = 25): #  filt_ch = 4\n",
        "#         super(Cross_Transformer_Network, self).__init__()\n",
        "        \n",
        "#         self.eeg_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1,\n",
        "#                                             window_size =window_size, First = True )\n",
        "#         self.eog_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "#                                             window_size =window_size, First = True )\n",
        "#         self.eeg2_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "#                                             window_size =window_size, First = True )\n",
        "        \n",
        "#         self.cross_atten = Cross_modal_atten(d_model=d_model, nhead=8, dropout=0.1, First = False )\n",
        "        \n",
        "#         self.cross_ff= Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "#         # self.eog_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "#         # self.eeg2_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "\n",
        "\n",
        "\n",
        "#         self.mlp    = nn.Sequential(nn.Flatten(),\n",
        "#                                     nn.Linear(d_model*3,5))  ##################\n",
        "#         # \n",
        "\n",
        "#     def forward(self, eeg: Tensor,eog: Tensor,eeg2: Tensor,finetune = True): \n",
        "#         self_eeg = self.eeg_atten(eeg)\n",
        "#         self_eog = self.eog_atten(eog)\n",
        "#         self_eeg2 = self.eeg2_atten(eeg)\n",
        "#         # print(self_eeg.shape,self_eeg2.shape)\n",
        "#         self_eeg_new = torch.cat((self_eeg[:,0,:].unsqueeze(dim=1),self_eeg2[:,0,:].unsqueeze(dim=1)), dim=1)\n",
        "#         cross = self.cross_atten(self_eeg_new,self_eog[:,0,:])\n",
        "\n",
        "#         cross_cls = cross[:,0,:].unsqueeze(dim=1)\n",
        "#         cross_eeg = cross[:,1,:].unsqueeze(dim=1)\n",
        "#         cross_eog = cross[:,2,:].unsqueeze(dim=1)\n",
        "\n",
        "#         # eeg_new =  torch.cat([cross_cls, self_eeg[:,1:,:]], dim=1)\n",
        "#         # eog_new =  torch.cat([cross_cls, self_eog[:,1:,:]], dim=1)\n",
        "#         # eeg2_new =  torch.cat([cross_cls, self_eeg2[:,1:,:]], dim=1)\n",
        "\n",
        "#         ff_cross =  self.cross_ff(cross)\n",
        "#         # ff_eog = self.eog_ff(eog_new)\n",
        "#         # ff_eeg2 = self.eeg2_ff(eeg2_new)\n",
        "\n",
        "        \n",
        "\n",
        "#         # cls_out = torch.cat([cross_cls[:,0,:],ff_eeg[:,0,:], ff_eog[:,0,:]], dim=1).unsqueeze(dim=1) ######\n",
        "#         # cls_out = #torch.cat([ff_eeg[:,0,:], ff_eog[:,0,:],ff_eeg2[:,0,:]], dim=1).unsqueeze(dim=1) \n",
        "\n",
        "#         feat_list = [ff_cross,self_eeg,self_eog,self_eeg2]\n",
        "#         if finetune == True:\n",
        "#             out = self.mlp(ff_cross)  #########\n",
        "#             return out,ff_cross,feat_list\n",
        "#         else:\n",
        "#             return ff_cross[:,0,:]#feat_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUBEquUBljod"
      },
      "outputs": [],
      "source": [
        "# Best Model so far fine tuning\n",
        "class Cross_Transformer_Network(nn.Module):\n",
        "    def __init__(self,d_model = 64, dim_feedforward=512,window_size = 25): #  filt_ch = 4\n",
        "        super(Cross_Transformer_Network, self).__init__()\n",
        "        \n",
        "        self.eeg_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1,\n",
        "                                            window_size =window_size, First = True )\n",
        "        self.eog_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "                                            window_size =window_size, First = True )\n",
        "        self.eeg2_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "                                            window_size =window_size, First = True )\n",
        "        \n",
        "        self.cross_atten = Cross_modal_atten(d_model=d_model, nhead=8, dropout=0.1, First = True )\n",
        "        \n",
        "        self.eeg_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "        self.eog_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "        self.eeg2_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "\n",
        "\n",
        "\n",
        "        self.mlp    = nn.Sequential(nn.Flatten(),\n",
        "                                    nn.Linear(d_model*3,5))  ##################\n",
        "        # \n",
        "\n",
        "    def forward(self, eeg: Tensor,eog: Tensor,eeg2: Tensor,finetune = True): \n",
        "        self_eeg = self.eeg_atten(eeg)\n",
        "        self_eog = self.eog_atten(eog)\n",
        "        self_eeg2 = self.eeg2_atten(eeg)\n",
        "        # print(self_eeg.shape,self_eeg2.shape)\n",
        "        self_eeg_new = torch.cat((self_eeg[:,0,:].unsqueeze(dim=1),self_eeg2[:,0,:].unsqueeze(dim=1)), dim=1)\n",
        "        cross = self.cross_atten(self_eeg_new,self_eog[:,0,:])\n",
        "\n",
        "        cross_cls = cross[:,0,:].unsqueeze(dim=1)\n",
        "        cross_eeg = cross[:,1,:].unsqueeze(dim=1)\n",
        "        cross_eog = cross[:,2,:].unsqueeze(dim=1)\n",
        "\n",
        "        eeg_new =  torch.cat([cross_cls, self_eeg[:,1:,:]], dim=1)\n",
        "        eog_new =  torch.cat([cross_cls, self_eog[:,1:,:]], dim=1)\n",
        "        eeg2_new =  torch.cat([cross_cls, self_eeg2[:,1:,:]], dim=1)\n",
        "\n",
        "        ff_eeg = self.eeg_ff(eeg_new)\n",
        "        ff_eog = self.eog_ff(eog_new)\n",
        "        ff_eeg2 = self.eeg2_ff(eeg2_new)\n",
        "\n",
        "        \n",
        "\n",
        "        # cls_out = torch.cat([cross_cls[:,0,:],ff_eeg[:,0,:], ff_eog[:,0,:]], dim=1).unsqueeze(dim=1) ######\n",
        "        cls_out = torch.cat([ff_eeg[:,0,:], ff_eog[:,0,:],ff_eeg2[:,0,:]], dim=1).unsqueeze(dim=1) \n",
        "\n",
        "        feat_list = [cross_cls,ff_eeg,ff_eog,ff_eeg2]\n",
        "        if finetune == True:\n",
        "            out = self.mlp(cls_out)  #########\n",
        "            return out,cls_out,feat_list\n",
        "        else:\n",
        "            return cls_out#feat_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSYj2tzGz9Ck"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUbb0Artw02L"
      },
      "outputs": [],
      "source": [
        "# # teacher model - using c4 o2 eeg channel\n",
        "# Net_t = torch.load('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Ear_EEG/EAREEG-29/checkpoint_model_epoch_17.pth.tar')\n",
        "# # ear EEG model - using L, R, EOG\n",
        "# Net_s = torch.load('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Ear_EEG/EAREEG-39/checkpoint_model_epoch_44.pth.tar')\n",
        "\n",
        "\n",
        "# Net_t = Cross_Transformer_Network(d_model = d_model, dim_feedforward=dim_feedforward,\n",
        "#                                 window_size = window_size ).to(device)\n",
        "# Net_s = torch.load('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Sleep_edfx/V2-Cros-48/checkpoint_model_best_kappa.pth.tar').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ-Efz04w02L"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "d_model = 256\n",
        "dim_feedforward=1024  #\n",
        "window_size = 50#25 50\n",
        "\n",
        "# Net_t = torch.load('/content/drive/MyDrive/EarEEG/EAREEGV2-121/checkpoint_model_epoch_best_kappa2.pth.tar').to(device)\n",
        "# Net_s = torch.load('/content/drive/MyDrive/EarEEG/EARCMT-58/checkpoint_model_epoch_best_kappa2.pth.tar').to(device)\n",
        "Net_t = Cross_Transformer_Network(d_model = d_model, dim_feedforward=dim_feedforward, window_size = window_size).to(device)\n",
        "Net_s = Cross_Transformer_Network(d_model = d_model, dim_feedforward=dim_feedforward, window_size = window_size).to(device)\n",
        "\n",
        "\n",
        "lr = 0.001#0.001\n",
        "beta_1 =  0.9    \n",
        "beta_2 =  0.999    \n",
        "eps = 1e-9\n",
        "n_epochs = 100\n",
        "\n",
        "criterion_mse =  nn.MSELoss()\n",
        "\n",
        "optimizer_t = torch.optim.Adam(Net_t.parameters(), lr=lr, betas=(beta_1, beta_2),eps = eps)\n",
        "optimizer_s = torch.optim.Adam(Net_s.parameters(), lr=lr, betas=(beta_1, beta_2),eps = eps)\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "psg_data, label = next(iter(train_data_loader))\n",
        "pred1,cls_outs,feat_list = Net_s(psg_data[:,:,0,:].float().to(device), psg_data[:,:,1,:].float().to(device), psg_data[:,:,2,:].float().to(device),finetune = True)\n",
        "print(pred1.shape,cls_outs.shape,len(feat_list))\n",
        "\n",
        "pred2,cls_outs,feat_list = Net_t(psg_data[:,:,3,:].float().to(device), psg_data[:,:,4,:].float().to(device), psg_data[:,:,5,:].float().to(device),finetune = True)\n",
        "print(pred2.shape,cls_outs.shape,len(feat_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WD4UQqqw02L"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "# class MSE_Loss(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(MSE_Loss, self).__init__()\n",
        "#         self.criterion = nn.MSELoss(size_average=True)\n",
        "      \n",
        "#     def forward(self, pred, target, T):\n",
        "#       pred = pred/T\n",
        "#       pred = F.softmax(pred, dim=0)\n",
        "#       # print(pred.shape)\n",
        "#       target = target/T\n",
        "#       target = F.softmax(target, dim=0)\n",
        "#       # print(target.shape)\n",
        "#       loss = self.criterion(pred,target)\n",
        "#       return loss \n",
        "\n",
        "# soft_loss = MSE_Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGN_8cvsUAVW"
      },
      "outputs": [],
      "source": [
        "# print(soft_loss(pred1,pred2,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z15TaaBcw02M"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"Experiment\" : \"Cross Modal Transformer\",\n",
        "    'Model Type' : \"Const-learn - scalp to EarEEG (teach pretrained, student start from teach weigths)\",\n",
        "    'Signals' : \"ScalpEEG - A1A2, C3C4, EOG, EarEEG - L,R,L-R\",\n",
        "    'Subject' : val_data_list[0]+1,\n",
        "    'd_model' : d_model,\n",
        "    'dim_feedforward' : dim_feedforward,\n",
        "    'window_size ':window_size ,\n",
        "    'Batch Size': batch_size,\n",
        "    'Loss': f\"Weighted Categorical Loss\",  # Check this every time\n",
        "    'Optimizer' : \"Adam\",        # Check this every time   \n",
        "    'Learning Rate': lr,\n",
        "    'eps' : eps,\n",
        "    \"LR Schduler\": \"StepLR\",\n",
        "    'Beta 1': beta_1,\n",
        "    'Beta 2': beta_2,\n",
        "    'n_epochs': n_epochs,\n",
        "    'Subject' : val_data_list[0]+1,\n",
        "    'threshold': 0.5\n",
        "}\n",
        "run['model/parameters'] = parameters\n",
        "run['model/model_architecture'] = Net_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdFCvQySw02M"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EERHHq_jz9Cm"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    pred = torch.argmax(outputs, 1)\n",
        "\n",
        "    # print(pred)\n",
        "    correct = pred.eq(labels.view_as(pred)).sum().item()\n",
        "    total = int(labels.shape[0])\n",
        "    return correct / total\n",
        "\n",
        "def kappa(output, label):  \n",
        "  preds = torch.argmax(output, 1)\n",
        "  return cohen_kappa_score(label, preds)\n",
        "\n",
        "\n",
        "def g_mean(sensitivity, specificity):\n",
        "    return (sensitivity*specificity)**0.5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdO3NArhz9Cm"
      },
      "outputs": [],
      "source": [
        "# batch_size = 10 # nb_samples\n",
        "# n_classes = 4\n",
        "# output = torch.randn(batch_size,5) # refer to output after softmax\n",
        "# lab = torch.randn(batch_size,5) # labels\n",
        "# #print(label)\n",
        "\n",
        "\n",
        "def confusion_matrix(output, label, n_classes,  batch_size, print_conf_mat = False):\n",
        "\n",
        "    preds = torch.argmax(output, 1)\n",
        "  \n",
        "    # print(preds)\n",
        "    # print(label)\n",
        "    conf_matrix = torch.zeros(n_classes, n_classes)\n",
        "    avg_sensitivity = 0\n",
        "    avg_specificity = 0\n",
        "    avg_F1_score = 0\n",
        "    avg_precision = 0\n",
        "    sens_list = []\n",
        "    spec_list = []\n",
        "    F1_list = []\n",
        "    precision_list = []\n",
        "\n",
        "    for p, t in zip(preds, label):\n",
        "        conf_matrix[p, t] += 1\n",
        "    # if print_conf_mat==True:    ##Jathu made this edit\n",
        "    #   print(conf_matrix)\n",
        "\n",
        "    #   plot_confusion_matrix(cm = conf_matrix.cpu().numpy(),\n",
        "    #                   normalize    = True,\n",
        "    #                   target_names = ['Wake', 'N1', 'N2','N3','REM'],\n",
        "    #                   title        = \"Confusion Matrix (5-Class)\")\n",
        "\n",
        "    #   plt.show()\n",
        "\n",
        "    TP = conf_matrix.diag()\n",
        "    for c in range(n_classes):\n",
        "        idx = torch.ones(n_classes).byte()\n",
        "        idx[c] = 0\n",
        "        TN = conf_matrix[idx.nonzero()[:,None], idx.nonzero()].sum()\n",
        "        FP = conf_matrix[c, idx].sum()\n",
        "        FN = conf_matrix[idx, c].sum()\n",
        "\n",
        "        if (TP[c]+FN) != 0:\n",
        "          sensitivity = (TP[c] / (TP[c]+FN))\n",
        "        else:\n",
        "          sensitivity = 0\n",
        "\n",
        "        if (TN+FP) != 0:\n",
        "          specificity = (TN / (TN+FP))\n",
        "        else:\n",
        "          specificity = 0\n",
        "\n",
        "        if ((2*TP[c]) + (FN + FP)) !=0:\n",
        "          F1_score = (2*TP[c])/((2*TP[c]) + (FN + FP))\n",
        "        else:\n",
        "          F1_score = 0\n",
        "        \n",
        "        if (TP[c]+FP) !=0:\n",
        "          precision = (TP[c]/(TP[c]+FP))\n",
        "        else:\n",
        "          precision = 0\n",
        "\n",
        "\n",
        "        sens_list.append(float(sensitivity))\n",
        "        spec_list.append(float(specificity))\n",
        "        F1_list.append(float(F1_score))\n",
        "        precision_list.append(float(precision))\n",
        "\n",
        "        avg_sensitivity += float(sensitivity)\n",
        "        avg_specificity += float(specificity)\n",
        "        avg_F1_score += float(F1_score)\n",
        "        avg_precision +=float(precision)\n",
        "\n",
        "        # print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(c, TP[c], TN, FP, FN))\n",
        "        # print('Sensitivity = {}'.format(sensitivity))\n",
        "        # print('Specificity = {}'.format(specificity))\n",
        "    return sens_list, spec_list,F1_list, precision_list, avg_sensitivity/5, avg_specificity/5, avg_F1_score/5, avg_precision/5 \n",
        "\n",
        "# confusion_matrix(output, lab,5,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwTvsLYAz9Cn"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count if self.count != 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxIizcI8KH4g"
      },
      "source": [
        "# Constrastive Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GhXuIpgKLsG",
        "outputId": "28c0154b-7963-4ceb-9725-027c35869752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================Training Epoch : [1/100] ===========================================================================================================>\n",
            "Epoch: [1/100][0/184]\tTrain_t_Loss 1.52267 (1.52267)\tTrain_s_Loss 3.39184 (3.39184)\tTrain_t_Acc 0.34375 (0.34375)\tTrain_s_Acc 0.34375 (0.34375)\tTrain_t_Kappa 0.04545(0.04545)\tTrain_s_Kappa 0.10160(0.10160)\tTime 5.044s (5.044s)\tSpeed 6.3 samples/s\tData 0.008s (0.008s)\t\n",
            "Epoch: [1/100][100/184]\tTrain_t_Loss 0.90283 (1.18419)\tTrain_s_Loss 1.37320 (1.73335)\tTrain_t_Acc 0.65625 (0.54486)\tTrain_s_Acc 0.62500 (0.45514)\tTrain_t_Kappa 0.47305(0.34815)\tTrain_s_Kappa 0.37357(0.18689)\tTime 4.512s (4.578s)\tSpeed 7.1 samples/s\tData 0.007s (0.006s)\t\n",
            "===========================================================Epoch : [1/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training T Loss     : 1.0723072109015093, Training T Accuracy      : 0.5875792572463768, Training T Kappa      : 0.4039737371266833\n",
            "Training S Loss     : 1.5778346716061882, Training S Accuracy      : 0.5126811594202898, Training S Kappa      : 0.27199030603343843\n",
            "Validation Results : \n",
            "Validation T Loss   : 1.7643585999806721, Validation T Accuracy : 0.41729797979797983, Validation T Kappa     : 0.15550787915928305\n",
            "Validation S Loss   : 2.1656441869157734, Validation S Accuracy : 0.29583333333333334, Validation S Kappa     : 0.09161328650255907\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [2/100] ===========================================================================================================>\n",
            "Epoch: [2/100][0/184]\tTrain_t_Loss 1.05480 (1.05480)\tTrain_s_Loss 1.75954 (1.75954)\tTrain_t_Acc 0.56250 (0.56250)\tTrain_s_Acc 0.43750 (0.43750)\tTrain_t_Kappa 0.40346(0.40346)\tTrain_s_Kappa 0.23607(0.23607)\tTime 5.004s (5.004s)\tSpeed 6.4 samples/s\tData 0.007s (0.007s)\t\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "best_val_acc = 0\n",
        "best_val_kappa = 0\n",
        "for epoch_idx in range(n_epochs):  # loop over the dataset multiple times\n",
        "    # run['train/epoch/learning_Rate'].log(optimizer.param_groups[0][\"lr\"]) \n",
        "    Net_s.train()\n",
        "    Net_t.train()        ### Check whether weights of the teacher gets updated\n",
        "    print(f'===========================================================Training Epoch : [{epoch_idx+1}/{n_epochs}] ===========================================================================================================>')\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    \n",
        "    losses_t = AverageMeter()\n",
        "    losses_s = AverageMeter()\n",
        "    val_t_losses = AverageMeter()\n",
        "    val_s_losses = AverageMeter()\n",
        "    \n",
        "    train_t_accuracy = AverageMeter()\n",
        "    train_s_accuracy = AverageMeter()\n",
        "    val_t_accuracy = AverageMeter()\n",
        "    val_s_accuracy = AverageMeter()\n",
        "\n",
        "    train_t_kappa = AverageMeter()\n",
        "    train_s_kappa = AverageMeter()\n",
        "    val_t_kappa = AverageMeter()\n",
        "    val_s_kappa = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for batch_idx, data_input in enumerate(train_data_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        data_time.update(time.time() - end)\n",
        "        psg, labels = data_input\n",
        "        sig1 = psg[:,:,0,:]# L-R\n",
        "        sig2 = psg[:,:,1,:]# L\n",
        "        sig3 = psg[:,:,2,:]# R\n",
        "        sig4 = psg[:,:,3,:]# c3-01\n",
        "        sig5 = psg[:,:,4,:]# c4-o2\n",
        "        sig6 = psg[:,:,5,:]# eog\n",
        "        cur_batch_size = len(sig1)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        \n",
        "       \n",
        "        # Temp = 1.2 #1.5\n",
        "        # alpha = 1e3\n",
        "        targets,cls_t_feat,_ = Net_t(sig4.float().to(device), sig5.float().to(device), sig6.float().to(device),finetune = True)\n",
        "        \n",
        "        \n",
        "        outputs,cls_s_feat,_ = Net_s(sig1.float().to(device), sig2.float().to(device), sig3.float().to(device),finetune = True)\n",
        "\n",
        "        loss_t =  criterion_ce(targets.cpu(),labels) #+ criterion_mse(cls_s_feat.cpu(), cls_t_feat.cpu())\n",
        "        loss_s =  criterion_ce(outputs.cpu(),labels) + criterion_mse(cls_s_feat.cpu(), cls_t_feat.detach().cpu())\n",
        "        # loss_s =  alpha*criterion_ce(outputs.cpu(),labels) + (1-alpha)*soft_loss(outputs,targets, Temp)\n",
        "        # loss_s =  criterion_ce(outputs.cpu(),labels) + soft_loss(cls_s_feat.cpu(),cls_t_feat.detach().cpu(), Temp)\n",
        "\n",
        "\n",
        "\n",
        "        # loss = loss_function(outputs.cpu(), targets.cpu(),weights)\n",
        "        \n",
        "        optimizer_t.zero_grad()\n",
        "        \n",
        "        loss_t.backward()\n",
        "        optimizer_t.step()\n",
        "        # scheduler.step()\n",
        "        optimizer_s.zero_grad()\n",
        "        loss_s.backward()\n",
        "        optimizer_s.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        losses_t.update(loss_t.data.item())\n",
        "        losses_s.update(loss_s.data.item())\n",
        "\n",
        "        train_t_accuracy.update(accuracy(targets.cpu(), labels))\n",
        "        train_s_accuracy.update(accuracy(outputs.cpu(), labels))\n",
        "\n",
        "        train_t_kappa.update(kappa(targets.cpu(), labels))\n",
        "        train_s_kappa.update(kappa(outputs.cpu(), labels))\n",
        "        # print(outputs.shape, labels.shape)\n",
        "\n",
        "        run['train/epoch/batch_t_loss'].log(losses_t.val)     #1\n",
        "        run['train/epoch/batch_s_loss'].log(losses_s.val) \n",
        "        run['train/epoch/batch_t_accuracy'].log(train_t_accuracy.val)\n",
        "        run['train/epoch/batch_s_accuracy'].log(train_s_accuracy.val)\n",
        "        run['epoch'].log(epoch_idx)\n",
        "        \n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            \n",
        "            msg = 'Epoch: [{0}/{3}][{1}/{2}]\\t' \\\n",
        "                  'Train_t_Loss {loss_t.val:.5f} ({loss_t.avg:.5f})\\t'\\\n",
        "                  'Train_s_Loss {loss_s.val:.5f} ({loss_s.avg:.5f})\\t'\\\n",
        "                  'Train_t_Acc {train_t_acc.val:.5f} ({train_t_acc.avg:.5f})\\t'\\\n",
        "                  'Train_s_Acc {train_s_acc.val:.5f} ({train_s_acc.avg:.5f})\\t'\\\n",
        "                  'Train_t_Kappa {train_t_kap.val:.5f}({train_t_kap.avg:.5f})\\t'\\\n",
        "                  'Train_s_Kappa {train_s_kap.val:.5f}({train_s_kap.avg:.5f})\\t'\\\n",
        "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t' \\\n",
        "                  'Speed {speed:.1f} samples/s\\t' \\\n",
        "                  'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t'.format(\n",
        "                      epoch_idx+1, batch_idx, len(train_data_loader),n_epochs, batch_time=batch_time,\n",
        "                      speed=data_input[0].size(0)/batch_time.val,\n",
        "                      data_time=data_time, loss_t=losses_t, loss_s=losses_s, train_t_acc = train_t_accuracy, train_s_acc = train_s_accuracy,\n",
        "                      train_t_kap = train_t_kappa,train_s_kap = train_s_kappa)\n",
        "            print(msg)\n",
        "\n",
        "\n",
        "    #evaluation\n",
        "    with torch.no_grad():\n",
        "      Net_s.eval()\n",
        "      Net_t.eval()\n",
        "      for batch_val_idx, data_val in enumerate(val_data_loader):\n",
        "        val_psg, val_labels = data_val\n",
        "        val_sig1 = val_psg[:,:,0,:]\n",
        "        val_sig2 = val_psg[:,:,1,:]\n",
        "        val_sig3 = val_psg[:,:,2,:]\n",
        "        cur_val_batch_size = len(val_sig1)\n",
        "        \n",
        "        val_targets,cls_val_t,_ = Net_t(val_psg[:,:,3,:].float().to(device), val_psg[:,:,4,:].float().to(device), val_psg[:,:,5,:].float().to(device),finetune = True)\n",
        "        pred,cls_val_s,_ = Net_s(val_sig1.float().to(device), val_sig2.float().to(device), val_sig3.float().to(device))\n",
        "\n",
        "\n",
        "        val_t_loss = criterion_ce(val_targets.cpu(),val_labels)\n",
        "        val_s_loss = criterion_ce(pred.cpu(),val_labels) + criterion_mse(cls_val_s.cpu(), cls_val_t.detach().cpu())\n",
        "        #soft_loss(pred.cpu(),val_targets.detach().cpu(), Temp)#\n",
        "\n",
        "\n",
        "        \n",
        "        val_t_losses.update(val_t_loss.data.item())\n",
        "        val_s_losses.update(val_s_loss.data.item())\n",
        "        val_t_accuracy.update(accuracy(val_targets.cpu(), val_labels))\n",
        "        val_s_accuracy.update(accuracy(pred.cpu(), val_labels))\n",
        "\n",
        "        val_t_kappa.update(kappa(val_targets.cpu(), val_labels))\n",
        "        val_s_kappa.update(kappa(pred.cpu(), val_labels))\n",
        "\n",
        "     \n",
        "\n",
        "      print(f'===========================================================Epoch : [{epoch_idx+1}/{n_epochs}]  Evaluation ===========================================================================================================>')\n",
        "      print(\"Training Results : \")\n",
        "      print(f\"Training T Loss     : {losses_t.avg}, Training T Accuracy      : {train_t_accuracy.avg}, Training T Kappa      : {train_t_kappa.avg}\")\n",
        "      print(f\"Training S Loss     : {losses_s.avg}, Training S Accuracy      : {train_s_accuracy.avg}, Training S Kappa      : {train_s_kappa.avg}\")\n",
        "      print(\"Validation Results : \")\n",
        "      print(f\"Validation T Loss   : {val_t_losses.avg}, Validation T Accuracy : {val_t_accuracy.avg}, Validation T Kappa     : {val_t_kappa.avg}\")\n",
        "      print(f\"Validation S Loss   : {val_s_losses.avg}, Validation S Accuracy : {val_s_accuracy.avg}, Validation S Kappa     : {val_s_kappa.avg}\")\n",
        "\n",
        " \n",
        "\n",
        "      run['train/epoch/epoch_train_t_loss'].log(losses_t.avg)\n",
        "      run['train/epoch/epoch_train_s_loss'].log(losses_s.avg)\n",
        "      run['train/epoch/epoch_val_t_loss'].log(val_t_losses.avg)\n",
        "      run['train/epoch/epoch_val_s_loss'].log(val_s_losses.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_t_accuracy'].log(train_t_accuracy.avg)\n",
        "      run['train/epoch/epoch_train_s_accuracy'].log(train_s_accuracy.avg)\n",
        "      run['train/epoch/epoch_val_t_accuracy'].log(val_t_accuracy.avg)\n",
        "      run['train/epoch/epoch_val_s_accuracy'].log(val_s_accuracy.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_t_Kappa'].log(train_t_kappa.avg)\n",
        "      run['train/epoch/epoch_train_s_Kappa'].log(train_s_kappa.avg)\n",
        "      run['train/epoch/epoch_val_t_Kappa'].log(val_t_kappa.avg)\n",
        "      run['train/epoch/epoch_val_s_Kappa'].log(val_s_kappa.avg)\n",
        "\n",
        "      #if val_accuracy.avg > best_val_acc or (epoch_idx+1)%10==0 or val_kappa.avg > best_val_kappa:\n",
        "      if val_s_accuracy.avg > best_val_acc or val_s_kappa.avg > best_val_kappa:\n",
        "          if val_s_accuracy.avg > best_val_acc:\n",
        "            run['model/bestmodel_acc'].log(epoch_idx+1)\n",
        "            best_val_acc = val_s_accuracy.avg\n",
        "            print(\"================================================================================================\")\n",
        "            print(\"                                          Saving Best Model (ACC)                                     \")\n",
        "            print(\"================================================================================================\")\n",
        "            torch.save(Net_t, f'/content/drive/MyDrive/EarEEG/{experiment}/teacher_checkpoint_acc.pth.tar')\n",
        "            torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_acc.pth.tar')\n",
        "\n",
        "          if val_s_kappa.avg > best_val_kappa:\n",
        "            run['model/bestmodel_kappa'].log(epoch_idx+1)\n",
        "            best_val_kappa = val_s_kappa.avg\n",
        "            print(\"================================================================================================\")\n",
        "            print(\"                                          Saving Best Model (Kappa)                                    \")\n",
        "            print(\"================================================================================================\")\n",
        "            torch.save(Net_t, f'/content/drive/MyDrive/EarEEG/{experiment}/teacher_checkpoint_kappa.pth.tar')\n",
        "            torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_kappa.pth.tar')\n",
        "\n",
        "          run['model/best_acc'].log(val_s_accuracy.avg)\n",
        "          run['model/best_kappa'].log(val_s_kappa.avg)\n",
        "          #torch.save(Net_t, f'/content/drive/MyDrive/EarEEG/{experiment}/teacher_checkpoint_model_epoch_{epoch_idx+1}.pth.tar')\n",
        "          #torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_model_epoch_{epoch_idx+1}.pth.tar')\n",
        "    # lr_scheduler.step()\n",
        "print('========================================Finished Training ===========================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGlbwPN1z9Co"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwEtiOwGz9Cp",
        "outputId": "37de2937-1bed-402c-d179-86acba0ea15d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cross_Transformer_Network(\n",
              "  (cnn_eeg): Sequential(\n",
              "    (0): Conv1d(1, 8, kernel_size=(10,), stride=(5,))\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Conv1d(8, 16, kernel_size=(10,), stride=(5,))\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(2,))\n",
              "    (7): LeakyReLU(negative_slope=0.01)\n",
              "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (cnn_eog): Sequential(\n",
              "    (0): Conv1d(1, 8, kernel_size=(10,), stride=(5,))\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Conv1d(8, 16, kernel_size=(10,), stride=(5,))\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(2,))\n",
              "    (7): LeakyReLU(negative_slope=0.01)\n",
              "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (cnn_emg): Sequential(\n",
              "    (0): Conv1d(1, 8, kernel_size=(10,), stride=(5,))\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Conv1d(8, 16, kernel_size=(10,), stride=(5,))\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(2,))\n",
              "    (7): LeakyReLU(negative_slope=0.01)\n",
              "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (positionalencoder_eeg): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (positionalencoder_eog): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (positionalencoder_eeg2): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (Cross_Trans_Encoder): Cross_Transformer_Enc_Layer(\n",
              "    (self_attn_eeg): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (dropout_eeg_1): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eeg): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (self_attn_eog): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (dropout_eog_1): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eog): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (self_attn_eareeg): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (dropout_eareeg_1): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eareeg): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (self_attn_cross): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=60, out_features=60, bias=True)\n",
              "    )\n",
              "    (dropout_eeg): Dropout(p=0.1, inplace=False)\n",
              "    (norm_eeg): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
              "    (linear_eeg2): Linear(in_features=1, out_features=64, bias=True)\n",
              "    (linear_eog2): Linear(in_features=1, out_features=64, bias=True)\n",
              "    (linear_eareeg2): Linear(in_features=1, out_features=64, bias=True)\n",
              "    (linear_eeg_ff1): Linear(in_features=64, out_features=512, bias=True)\n",
              "    (dropout_eeg_ff): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eeg_ff2): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (dropout_eeg_ff2): Dropout(p=0.1, inplace=False)\n",
              "    (norm_eeg_ff): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (linear_eog_ff1): Linear(in_features=64, out_features=512, bias=True)\n",
              "    (dropout_eog_ff): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eog_ff2): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (dropout_eog_ff2): Dropout(p=0.1, inplace=False)\n",
              "    (norm_eog_ff): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (linear_eareeg_ff1): Linear(in_features=64, out_features=512, bias=True)\n",
              "    (dropout_eareeg_ff): Dropout(p=0.1, inplace=False)\n",
              "    (linear_eareeg_ff2): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (dropout_eareeg_ff2): Dropout(p=0.1, inplace=False)\n",
              "    (norm_eareeg_ff): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (MLP_block_wake): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=11520, out_features=1024, bias=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): Linear(in_features=1024, out_features=32, bias=True)\n",
              "    (4): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              "  (MLP_block_sleep): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=11520, out_features=1024, bias=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): Linear(in_features=1024, out_features=32, bias=True)\n",
              "    (4): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              "  (wake_final): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (sleep_final): Linear(in_features=33, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_model = Cross_Transformer_Network().to(device)\n",
        "test_model = torch.load('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Ear_EEG/EEGCROSS-99/student_checkpoint_model_epoch.pth.tar')\n",
        "test_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0khXToTw02P",
        "outputId": "3d527b0b-c692-48f7-cec0-c4c70e39aafb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results : \n",
            "Validation Accuracy : 0.6032828282828283, Validation G-Mean      : 0.6414730674004561\n",
            "Validation Kappa     : 0.43447820290681616, Validation MF1      : 0.4413706427270716, Validation Precision      : 0.4550649942773761,  Validation Sensitivity      : 0.4667994491530187, Validation Specificity      : 0.8909160744060171\n",
            "Class wise F1  W: 0.30833661962639203, S1: 0.08653198724443262, S2: 0.7433328475012924, S3: 0.42417684016805707, R: 0.6444749190951838\n",
            "(1039,)\n",
            "(1039, 5)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "val_accuracy = AverageMeter()\n",
        "val_sensitivity = AverageMeter()\n",
        "val_specificity = AverageMeter()\n",
        "val_f1_score = AverageMeter()\n",
        "val_precision = AverageMeter()\n",
        "val_gmean = AverageMeter()\n",
        "val_kappa = AverageMeter()\n",
        "\n",
        "class1_f1 = AverageMeter()\n",
        "class2_f1 = AverageMeter()\n",
        "class3_f1 = AverageMeter()\n",
        "class4_f1 = AverageMeter()\n",
        "class5_f1 = AverageMeter()\n",
        "\n",
        "labels_val_main = []\n",
        "pred_val_main = []\n",
        "first = 0 \n",
        "with torch.no_grad():\n",
        "    test_model.eval()\n",
        "    for batch_val_idx, data_val in enumerate(val_data_loader):\n",
        "        val_psg, val_labels = data_val\n",
        "        val_eeg = val_psg[:,:,0,:]\n",
        "        val_eog = val_psg[:,:,1,:]\n",
        "        val_emg = val_psg[:,:,2,:]\n",
        "        cur_val_batch_size = len(val_eeg)\n",
        "        pred = test_model(val_eeg.float().to(device), val_eog.float().to(device), val_emg.float().to(device))\n",
        "        # val_loss = criterion(pred.cpu(), val_labels)#.to(device))\n",
        "        # val_losses.update(val_loss.data.item())\n",
        "        if first == 0:\n",
        "            labels_val_main = val_labels.cpu().numpy()\n",
        "            pred_val_main = pred.cpu().numpy()\n",
        "            first = 1\n",
        "        else:\n",
        "            labels_val_main = np.concatenate((labels_val_main, val_labels.cpu().numpy()))\n",
        "            pred_val_main =  np.concatenate((pred_val_main,pred.cpu().numpy()))\n",
        "\n",
        "        val_accuracy.update(accuracy(pred.cpu(), val_labels))\n",
        "\n",
        "        sens_list,spec_list,f1_list,prec_list, sens,spec,f1,prec = confusion_matrix(pred.cpu(), val_labels,  5, cur_val_batch_size)\n",
        "        val_sensitivity.update(sens)\n",
        "        val_specificity.update(spec)\n",
        "        val_f1_score.update(f1)\n",
        "        val_precision.update(prec)\n",
        "        val_gmean.update(g_mean(sens, spec))\n",
        "        val_kappa.update(kappa(pred.cpu(), val_labels))\n",
        "\n",
        "        class1_f1.update(f1_list[0])\n",
        "        class2_f1.update(f1_list[1])\n",
        "        class3_f1.update(f1_list[2])\n",
        "        class4_f1.update(f1_list[3])\n",
        "        class5_f1.update(f1_list[4])\n",
        "\n",
        "print(\"Validation Results : \")\n",
        "print(f\"Validation Accuracy : {val_accuracy.avg}, Validation G-Mean      : {val_gmean.avg}\") \n",
        "print(f\"Validation Kappa     : {val_kappa.avg}, Validation MF1      : {val_f1_score.avg}, Validation Precision      : {val_precision.avg},  Validation Sensitivity      : {val_sensitivity.avg}, Validation Specificity      : {val_specificity.avg}\")\n",
        "print(f\"Class wise F1  W: {class1_f1.avg}, S1: {class2_f1.avg}, S2: {class3_f1.avg}, S3: {class4_f1.avg}, R: {class5_f1.avg}\")\n",
        "print(labels_val_main.shape)\n",
        "print(pred_val_main.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sL2h7usw02P",
        "outputId": "4fad8fcc-fda1-47de-9e47-368e1f28fe3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.6044273339749759\n",
            "Kappa 0.4407474877909192\n",
            "Macro F1 Score 0.4764548778533936\n",
            "G Mean 0.6540839636715834\n",
            "Sensitivity 0.480249947309494\n",
            "Specificity 0.8908399343490601\n",
            "Class wise F1 Score [0.35862070322036743, 0.1118881106376648, 0.7491785287857056, 0.5081080794334412, 0.6544789671897888]\n"
          ]
        }
      ],
      "source": [
        "## 5 Class\n",
        "sens_l,spec_l,f1_l,prec_l, sens,spec,f1,prec = confusion_matrix(torch.from_numpy(pred_val_main), torch.from_numpy(labels_val_main),\n",
        "                                                5, labels_val_main.shape[0])#, print_conf_mat=True)\n",
        "\n",
        "\n",
        "g = g_mean(sens, spec)\n",
        "\n",
        "acc = accuracy(torch.from_numpy(pred_val_main), torch.from_numpy(labels_val_main))\n",
        "\n",
        "kap = kappa(torch.from_numpy(pred_val_main), torch.from_numpy(labels_val_main))\n",
        "\n",
        "print(f\"Accuracy {acc}\")\n",
        "print(f\"Kappa {kap}\")\n",
        "print(f\"Macro F1 Score {f1}\")\n",
        "print(f\"G Mean {g}\")\n",
        "print(f\"Sensitivity {sens}\")\n",
        "print(f\"Specificity {spec}\")\n",
        "print(f\"Class wise F1 Score {f1_l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhOogMkfw02P",
        "outputId": "3fd6da4a-a7c3-4bf1-cfc0-cf6cf20d2698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1039,)\n",
            "(1039, 5)\n"
          ]
        }
      ],
      "source": [
        "main_all_labels = labels_val_main\n",
        "main_all_pred = pred_val_main\n",
        "\n",
        "# main_all_labels = np.concatenate((main_all_labels, labels_val_main))\n",
        "# main_all_pred = np.concatenate((main_all_pred,pred_val_main))\n",
        "\n",
        "print(main_all_labels.shape)\n",
        "print(main_all_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VRBFn7w02N"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4Pl9HUrz9Cn",
        "outputId": "930b2a4b-161f-430e-db82-aa3ab224fbcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================Training Epoch : [1/100] ===========================================================================================================>\n",
            "Epoch: [1/100][0/190]\tTrain_Loss 13.78504 (13.78504)\tTrain_Acc 0.28125 (0.28125)\tTrain_G-Mean 0.31886(0.31886)\tTrain_Kappa -0.05595(-0.05595)\tTrain_MF1 0.10286(0.10286)\tTrain_Precision 0.08571(0.08571)\tTrain_Sensitivity 0.12857(0.12857)\tTrain_Specificity 0.79080(0.79080)\tTime 0.269s (0.269s)\tSpeed 118.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [1/100][100/190]\tTrain_Loss 6.92884 (8.15161)\tTrain_Acc 0.28125 (0.47246)\tTrain_G-Mean 0.52110(0.48402)\tTrain_Kappa 0.00136(0.13789)\tTrain_MF1 0.22873(0.24408)\tTrain_Precision 0.17763(0.26481)\tTrain_Sensitivity 0.34359(0.29022)\tTrain_Specificity 0.79032(0.82501)\tTime 0.140s (0.142s)\tSpeed 228.9 samples/s\tData 0.004s (0.004s)\t\n",
            "26\n",
            "===========================================================Epoch : [1/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 7.251764129337512, Training Accuracy      : 0.490484022556391, Training G-Mean      : 0.5038186457847537\n",
            "Training Kappa      : 0.17806454135969443,Training MF1     : 0.2740331663269746, Training Precision      : 0.31518757916594803, Training Sensitivity      : 0.31063474836318117, Training Specificity      : 0.833341273602687\n",
            "Validation Results : \n",
            "Validation Loss   : 2.635384436006899, Validation Accuracy : 0.27587962962962964, Validation G-Mean      : 0.4962432117132218\n",
            "Validation Kappa     : 0.042034211671606286, Validation MF1      : 0.1797394240344012, Validation Precision      : 0.2075744068732968,  Validation Sensitivity      : 0.30994996966587174, Validation Specificity      : 0.8063594868889561\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :13.397176389341\n",
            "Class wise sensitivity W: 0.07066421417726411, S1: 0.0, S2: 0.6797029177347819, S3: 0.7993827164173126, R: 0.0\n",
            "Class wise specificity W: 0.9937061225926435, S1: 1.0, S2: 0.19295308435404743, S3: 0.8451382274980899, R: 1.0\n",
            "Class wise F1  W: 0.11973886798929285, S1: 0.0, S2: 0.36349079068060275, S3: 0.4154674615021105, R: 0.0\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [2/100] ===========================================================================================================>\n",
            "Epoch: [2/100][0/190]\tTrain_Loss 9.21008 (9.21008)\tTrain_Acc 0.40625 (0.40625)\tTrain_G-Mean 0.53963(0.53963)\tTrain_Kappa 0.18170(0.18170)\tTrain_MF1 0.29212(0.29212)\tTrain_Precision 0.31992(0.31992)\tTrain_Sensitivity 0.34903(0.34903)\tTrain_Specificity 0.83433(0.83433)\tTime 0.140s (0.140s)\tSpeed 228.6 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [2/100][100/190]\tTrain_Loss 4.74649 (5.72406)\tTrain_Acc 0.59375 (0.53032)\tTrain_G-Mean 0.70169(0.55969)\tTrain_Kappa 0.46530(0.26733)\tTrain_MF1 0.51556(0.33953)\tTrain_Precision 0.52381(0.38405)\tTrain_Sensitivity 0.55167(0.37364)\tTrain_Specificity 0.89252(0.85112)\tTime 0.139s (0.148s)\tSpeed 230.5 samples/s\tData 0.003s (0.004s)\t\n",
            "26\n",
            "===========================================================Epoch : [2/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.80768143377806, Training Accuracy      : 0.5377584586466165, Training G-Mean      : 0.5654591951322497\n",
            "Training Kappa      : 0.27544197280214205,Training MF1     : 0.3483285199811586, Training Precision      : 0.4016857448769243, Training Sensitivity      : 0.38032351983220963, Training Specificity      : 0.8525345749486436\n",
            "Validation Results : \n",
            "Validation Loss   : 2.555520362324185, Validation Accuracy : 0.34435185185185185, Validation G-Mean      : 0.5037880125645517\n",
            "Validation Kappa     : 0.1000505586747012, Validation MF1      : 0.23683790045755887, Validation Precision      : 0.3286502069897122,  Validation Sensitivity      : 0.31899497277206845, Validation Specificity      : 0.820619440327088\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :12.216041211728696\n",
            "Class wise sensitivity W: 0.16396399780556006, S1: 0.044444444554823416, S2: 0.8097733568262171, S3: 0.5339506180198105, R: 0.04284244665393123\n",
            "Class wise specificity W: 0.9879894653956095, S1: 0.9920738251120956, S2: 0.21124454022005754, S3: 0.9218006156108998, R: 0.9899887552967778\n",
            "Class wise F1  W: 0.2579282008939319, S1: 0.04938271641731262, S2: 0.41914248797628617, S3: 0.3881246370297891, R: 0.06961145997047424\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [3/100] ===========================================================================================================>\n",
            "Epoch: [3/100][0/190]\tTrain_Loss 6.62735 (6.62735)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.42717(0.42717)\tTrain_Kappa 0.04120(0.04120)\tTrain_MF1 0.21333(0.21333)\tTrain_Precision 0.30714(0.30714)\tTrain_Sensitivity 0.22647(0.22647)\tTrain_Specificity 0.80573(0.80573)\tTime 0.142s (0.142s)\tSpeed 225.4 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [3/100][100/190]\tTrain_Loss 5.32457 (5.71918)\tTrain_Acc 0.46875 (0.53063)\tTrain_G-Mean 0.52102(0.57276)\tTrain_Kappa 0.18806(0.27967)\tTrain_MF1 0.28725(0.35713)\tTrain_Precision 0.29000(0.42403)\tTrain_Sensitivity 0.32444(0.38864)\tTrain_Specificity 0.83668(0.85423)\tTime 0.141s (0.141s)\tSpeed 226.8 samples/s\tData 0.004s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [3/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.912674296529669, Training Accuracy      : 0.5363486842105263, Training G-Mean      : 0.5697582269523594\n",
            "Training Kappa      : 0.2801784457132265,Training MF1     : 0.35412554759728276, Training Precision      : 0.4169585866284998, Training Sensitivity      : 0.3851537316096455, Training Specificity      : 0.8538830722868438\n",
            "Validation Results : \n",
            "Validation Loss   : 2.0796258096341735, Validation Accuracy : 0.3572222222222222, Validation G-Mean      : 0.5034954793425137\n",
            "Validation Kappa     : 0.12677832780744477, Validation MF1      : 0.23339528540770213, Validation Precision      : 0.31070356849167086,  Validation Sensitivity      : 0.3121212421743958, Validation Specificity      : 0.8264882955838134\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :9.85704250688906\n",
            "Class wise sensitivity W: 0.19172960740548592, S1: 0.0, S2: 0.8038707220995868, S3: 0.5296296311749352, R: 0.03537625019197111\n",
            "Class wise specificity W: 0.9809729501053139, S1: 1.0, S2: 0.27152481702742753, S3: 0.8976689555026867, R: 0.9822747552836383\n",
            "Class wise F1  W: 0.29255596741482065, S1: 0.0, S2: 0.44251675958986636, S3: 0.3727620177798801, R: 0.05914168225394355\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [4/100] ===========================================================================================================>\n",
            "Epoch: [4/100][0/190]\tTrain_Loss 7.62420 (7.62420)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.58340(0.58340)\tTrain_Kappa 0.34503(0.34503)\tTrain_MF1 0.38338(0.38338)\tTrain_Precision 0.57576(0.57576)\tTrain_Sensitivity 0.39295(0.39295)\tTrain_Specificity 0.86617(0.86617)\tTime 0.142s (0.142s)\tSpeed 225.5 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [4/100][100/190]\tTrain_Loss 4.15619 (5.70672)\tTrain_Acc 0.59375 (0.53929)\tTrain_G-Mean 0.67702(0.57416)\tTrain_Kappa 0.44754(0.27722)\tTrain_MF1 0.51861(0.36067)\tTrain_Precision 0.59957(0.43599)\tTrain_Sensitivity 0.51429(0.39114)\tTrain_Specificity 0.89126(0.85371)\tTime 0.137s (0.141s)\tSpeed 233.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [4/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.698369942213359, Training Accuracy      : 0.5484022556390977, Training G-Mean      : 0.5856699529160974\n",
            "Training Kappa      : 0.3009759802032852,Training MF1     : 0.3744198510521335, Training Precision      : 0.4392945119111162, Training Sensitivity      : 0.40390667181265993, Training Specificity      : 0.8581713679276013\n",
            "Validation Results : \n",
            "Validation Loss   : 2.475873028790509, Validation Accuracy : 0.36930555555555555, Validation G-Mean      : 0.4530732126468043\n",
            "Validation Kappa     : 0.11486057583981012, Validation MF1      : 0.1787580061841894, Validation Precision      : 0.28203335415433956,  Validation Sensitivity      : 0.24930113195269193, Validation Specificity      : 0.8253458872989371\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :10.905047469668919\n",
            "Class wise sensitivity W: 0.21457873781522116, S1: 0.022927690435338904, S2: 0.9881291543995893, S3: 0.0, R: 0.020870077113310497\n",
            "Class wise specificity W: 0.9887505703502231, S1: 0.9658802217907376, S2: 0.19723781060289453, S3: 0.9976851851851852, R: 0.9771756485656455\n",
            "Class wise F1  W: 0.3369272709996612, S1: 0.03333333355409128, S2: 0.4883442410716304, S3: 0.0, R: 0.03518518529556416\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [5/100] ===========================================================================================================>\n",
            "Epoch: [5/100][0/190]\tTrain_Loss 5.05460 (5.05460)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.63991(0.63991)\tTrain_Kappa 0.21657(0.21657)\tTrain_MF1 0.35111(0.35111)\tTrain_Precision 0.29259(0.29259)\tTrain_Sensitivity 0.48889(0.48889)\tTrain_Specificity 0.83757(0.83757)\tTime 0.144s (0.144s)\tSpeed 222.3 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [5/100][100/190]\tTrain_Loss 6.98179 (5.42733)\tTrain_Acc 0.62500 (0.56807)\tTrain_G-Mean 0.53232(0.57870)\tTrain_Kappa 0.26296(0.30664)\tTrain_MF1 0.32000(0.37375)\tTrain_Precision 0.33333(0.43564)\tTrain_Sensitivity 0.33333(0.39649)\tTrain_Specificity 0.85008(0.85856)\tTime 0.138s (0.140s)\tSpeed 231.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [5/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.576812311222679, Training Accuracy      : 0.5496945488721805, Training G-Mean      : 0.5768393572469999\n",
            "Training Kappa      : 0.29797740146988777,Training MF1     : 0.36601538582852017, Training Precision      : 0.4204947435385304, Training Sensitivity      : 0.39403128731407633, Training Specificity      : 0.8572947298813814\n",
            "Validation Results : \n",
            "Validation Loss   : 2.474097658086706, Validation Accuracy : 0.3360648148148148, Validation G-Mean      : 0.43089179929066657\n",
            "Validation Kappa     : 0.0648218825010397, Validation MF1      : 0.1392121300653175, Validation Precision      : 0.22512103225345967,  Validation Sensitivity      : 0.22840430206722678, Validation Specificity      : 0.8139792624723028\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :9.31978374057346\n",
            "Class wise sensitivity W: 0.1346141028183478, S1: 0.007407407517786379, S2: 1.0, S3: 0.0, R: 0.0\n",
            "Class wise specificity W: 0.9944351624559473, S1: 0.9947937991884019, S2: 0.08066735071716485, S3: 1.0, R: 1.0\n",
            "Class wise F1  W: 0.22337281483191032, S1: 0.012345679380275585, S2: 0.46034215611440166, S3: 0.0, R: 0.0\n",
            "===========================================================Training Epoch : [6/100] ===========================================================================================================>\n",
            "Epoch: [6/100][0/190]\tTrain_Loss 6.83958 (6.83958)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.55936(0.55936)\tTrain_Kappa 0.39797(0.39797)\tTrain_MF1 0.31618(0.31618)\tTrain_Precision 0.30909(0.30909)\tTrain_Sensitivity 0.35556(0.35556)\tTrain_Specificity 0.88000(0.88000)\tTime 0.141s (0.141s)\tSpeed 227.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [6/100][100/190]\tTrain_Loss 5.37348 (5.47673)\tTrain_Acc 0.56250 (0.55476)\tTrain_G-Mean 0.61805(0.59169)\tTrain_Kappa 0.33630(0.31030)\tTrain_MF1 0.40402(0.38221)\tTrain_Precision 0.39333(0.43918)\tTrain_Sensitivity 0.44462(0.41205)\tTrain_Specificity 0.85913(0.85868)\tTime 0.140s (0.140s)\tSpeed 228.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [6/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.689066624641418, Training Accuracy      : 0.5580122180451128, Training G-Mean      : 0.5885879577236635\n",
            "Training Kappa      : 0.31198589045580305,Training MF1     : 0.38261658334418336, Training Precision      : 0.4449559345684553, Training Sensitivity      : 0.40797238852632667, Training Specificity      : 0.8592309219194088\n",
            "Validation Results : \n",
            "Validation Loss   : 3.694900283107051, Validation Accuracy : 0.2861574074074074, Validation G-Mean      : 0.4679989650415681\n",
            "Validation Kappa     : 0.121271813402478, Validation MF1      : 0.19736692138292167, Validation Precision      : 0.2504712688978072,  Validation Sensitivity      : 0.2761648962895076, Validation Specificity      : 0.8297367963525986\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :18.434047840259694\n",
            "Class wise sensitivity W: 0.44894627343725274, S1: 0.0, S2: 0.2003967265288035, S3: 0.7314814814814815, R: 0.0\n",
            "Class wise specificity W: 0.9840575059254965, S1: 1.0, S2: 0.6559850319668099, S3: 0.5086414438706858, R: 1.0\n",
            "Class wise F1  W: 0.5985585870566191, S1: 0.0, S2: 0.18820826543702018, S3: 0.20006775442096922, R: 0.0\n",
            "===========================================================Training Epoch : [7/100] ===========================================================================================================>\n",
            "Epoch: [7/100][0/190]\tTrain_Loss 5.37294 (5.37294)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.66387(0.66387)\tTrain_Kappa 0.41737(0.41737)\tTrain_MF1 0.38817(0.38817)\tTrain_Precision 0.35070(0.35070)\tTrain_Sensitivity 0.49643(0.49643)\tTrain_Specificity 0.88778(0.88778)\tTime 0.141s (0.141s)\tSpeed 226.9 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [7/100][100/190]\tTrain_Loss 4.73296 (6.06770)\tTrain_Acc 0.75000 (0.51114)\tTrain_G-Mean 0.72884(0.55246)\tTrain_Kappa 0.60856(0.23927)\tTrain_MF1 0.56104(0.32605)\tTrain_Precision 0.58349(0.36639)\tTrain_Sensitivity 0.57157(0.36667)\tTrain_Specificity 0.92938(0.84537)\tTime 0.138s (0.141s)\tSpeed 232.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [7/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.733768607440748, Training Accuracy      : 0.5273496240601504, Training G-Mean      : 0.5659008346644325\n",
            "Training Kappa      : 0.2650015313147597,Training MF1     : 0.34401822959121897, Training Precision      : 0.38428780530628404, Training Sensitivity      : 0.38169938201967024, Training Specificity      : 0.8504554203152659\n",
            "Validation Results : \n",
            "Validation Loss   : 2.7846985967070967, Validation Accuracy : 0.3075, Validation G-Mean      : 0.5113059858313773\n",
            "Validation Kappa     : 0.10797632121141706, Validation MF1      : 0.22126467735679056, Validation Precision      : 0.27084787937778015,  Validation Sensitivity      : 0.3229122036033207, Validation Specificity      : 0.824190260966619\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.850149507875797\n",
            "Class wise sensitivity W: 0.3895291514970638, S1: 0.0, S2: 0.38861211196140005, S3: 0.8364197545581393, R: 0.0\n",
            "Class wise specificity W: 0.9874453058949223, S1: 0.98603962748139, S2: 0.428200700768718, S3: 0.719265670688064, R: 1.0\n",
            "Class wise F1  W: 0.5426043845989086, S1: 0.0, S2: 0.2719287905428145, S3: 0.29179021164223, R: 0.0\n",
            "===========================================================Training Epoch : [8/100] ===========================================================================================================>\n",
            "Epoch: [8/100][0/190]\tTrain_Loss 5.81261 (5.81261)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.58115(0.58115)\tTrain_Kappa 0.35657(0.35657)\tTrain_MF1 0.35332(0.35332)\tTrain_Precision 0.36111(0.36111)\tTrain_Sensitivity 0.38682(0.38682)\tTrain_Specificity 0.87312(0.87312)\tTime 0.139s (0.139s)\tSpeed 229.4 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [8/100][100/190]\tTrain_Loss 5.87569 (5.13937)\tTrain_Acc 0.40625 (0.56745)\tTrain_G-Mean 0.53021(0.60839)\tTrain_Kappa 0.14487(0.33881)\tTrain_MF1 0.30017(0.40294)\tTrain_Precision 0.31515(0.44621)\tTrain_Sensitivity 0.34048(0.43268)\tTrain_Specificity 0.82568(0.86481)\tTime 0.136s (0.140s)\tSpeed 235.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [8/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.053225608875877, Training Accuracy      : 0.5777255639097744, Training G-Mean      : 0.6161481515381373\n",
            "Training Kappa      : 0.356384095255031,Training MF1     : 0.4209741204192763, Training Precision      : 0.48078014943160524, Training Sensitivity      : 0.44159918770194045, Training Specificity      : 0.8683668737191901\n",
            "Validation Results : \n",
            "Validation Loss   : 3.122496278197677, Validation Accuracy : 0.34337962962962965, Validation G-Mean      : 0.5231885522860487\n",
            "Validation Kappa     : 0.1416392860760249, Validation MF1      : 0.23891253995674624, Validation Precision      : 0.2793858464945246,  Validation Sensitivity      : 0.33596248190712047, Validation Specificity      : 0.8311540104724746\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :14.173577414618599\n",
            "Class wise sensitivity W: 0.3860197688142459, S1: 0.0, S2: 0.5067556025805297, S3: 0.7870370381408267, R: 0.0\n",
            "Class wise specificity W: 0.9836245973904928, S1: 0.9925815683824045, S2: 0.4193934709937484, S3: 0.7617136262081288, R: 0.998456789387597\n",
            "Class wise F1  W: 0.5350498314257022, S1: 0.0, S2: 0.3392055782454985, S3: 0.32030729011253073, R: 0.0\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [9/100] ===========================================================================================================>\n",
            "Epoch: [9/100][0/190]\tTrain_Loss 4.63961 (4.63961)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.63530(0.63530)\tTrain_Kappa 0.40267(0.40267)\tTrain_MF1 0.45714(0.45714)\tTrain_Precision 0.51508(0.51508)\tTrain_Sensitivity 0.46000(0.46000)\tTrain_Specificity 0.87740(0.87740)\tTime 0.154s (0.154s)\tSpeed 207.5 samples/s\tData 0.006s (0.006s)\t\n",
            "Epoch: [9/100][100/190]\tTrain_Loss 3.98691 (4.93450)\tTrain_Acc 0.56250 (0.57550)\tTrain_G-Mean 0.57699(0.61284)\tTrain_Kappa 0.32632(0.35560)\tTrain_MF1 0.38354(0.41267)\tTrain_Precision 0.43818(0.47086)\tTrain_Sensitivity 0.38571(0.43656)\tTrain_Specificity 0.86313(0.86916)\tTime 0.139s (0.140s)\tSpeed 230.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [9/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.806357506701821, Training Accuracy      : 0.5855498120300752, Training G-Mean      : 0.6222472660667251\n",
            "Training Kappa      : 0.3700244397358035,Training MF1     : 0.4277898031473159, Training Precision      : 0.4879303971874086, Training Sensitivity      : 0.44810046421854105, Training Specificity      : 0.8722616729924553\n",
            "Validation Results : \n",
            "Validation Loss   : 3.2549138687275074, Validation Accuracy : 0.4028703703703704, Validation G-Mean      : 0.5397742853113304\n",
            "Validation Kappa     : 0.17689752056408617, Validation MF1      : 0.2751858162107291, Validation Precision      : 0.3519586421273373,  Validation Sensitivity      : 0.354091139993182, Validation Specificity      : 0.8361970376085351\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :14.116943182768646\n",
            "Class wise sensitivity W: 0.30098720074251845, S1: 0.0, S2: 0.8768759038713243, S3: 0.586419755661929, R: 0.006172839690137793\n",
            "Class wise specificity W: 0.9901014345663565, S1: 1.0, S2: 0.2533107179182547, S3: 0.9375730355580648, R: 1.0\n",
            "Class wise F1  W: 0.44522385933884867, S1: 0.0, S2: 0.45874754035914383, S3: 0.4613756703005897, R: 0.010582011055063319\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [10/100] ===========================================================================================================>\n",
            "Epoch: [10/100][0/190]\tTrain_Loss 3.93019 (3.93019)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.50345(0.50345)\tTrain_Kappa 0.28358(0.28358)\tTrain_MF1 0.31883(0.31883)\tTrain_Precision 0.53077(0.53077)\tTrain_Sensitivity 0.29746(0.29746)\tTrain_Specificity 0.85207(0.85207)\tTime 0.143s (0.143s)\tSpeed 224.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [10/100][100/190]\tTrain_Loss 6.43553 (4.62260)\tTrain_Acc 0.50000 (0.60241)\tTrain_G-Mean 0.58438(0.63114)\tTrain_Kappa 0.29863(0.38697)\tTrain_MF1 0.34946(0.44511)\tTrain_Precision 0.35429(0.50365)\tTrain_Sensitivity 0.39864(0.45912)\tTrain_Specificity 0.85668(0.87455)\tTime 0.144s (0.141s)\tSpeed 222.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [10/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.882643685842815, Training Accuracy      : 0.5906015037593985, Training G-Mean      : 0.6254109638575921\n",
            "Training Kappa      : 0.3763387276296654,Training MF1     : 0.4337281293618051, Training Precision      : 0.4986278003454206, Training Sensitivity      : 0.45177380568887054, Training Specificity      : 0.8730097710968628\n",
            "Validation Results : \n",
            "Validation Loss   : 1.9793503240302757, Validation Accuracy : 0.4002314814814815, Validation G-Mean      : 0.5593325178906527\n",
            "Validation Kappa     : 0.19554020240094122, Validation MF1      : 0.3171506039522313, Validation Precision      : 0.37210716627262247,  Validation Sensitivity      : 0.37772310243712515, Validation Specificity      : 0.8421324823741559\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :8.313088929211652\n",
            "Class wise sensitivity W: 0.37069309806382217, S1: 0.013374485627368645, S2: 0.6663177377647824, S3: 0.672839508012489, R: 0.16539068271716437\n",
            "Class wise specificity W: 0.984896962289457, S1: 0.9973544986159714, S2: 0.4701465550396178, S3: 0.8993184522346214, R: 0.8589459436911123\n",
            "Class wise F1  W: 0.515946277865657, S1: 0.019753086898061965, S2: 0.43365327627570543, S3: 0.44868259959750706, R: 0.16771777912422461\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [11/100] ===========================================================================================================>\n",
            "Epoch: [11/100][0/190]\tTrain_Loss 5.39203 (5.39203)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.61558(0.61558)\tTrain_Kappa 0.37691(0.37691)\tTrain_MF1 0.45331(0.45331)\tTrain_Precision 0.51140(0.51140)\tTrain_Sensitivity 0.43500(0.43500)\tTrain_Specificity 0.87111(0.87111)\tTime 0.141s (0.141s)\tSpeed 226.3 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [11/100][100/190]\tTrain_Loss 4.77922 (5.07946)\tTrain_Acc 0.53125 (0.58230)\tTrain_G-Mean 0.52434(0.60820)\tTrain_Kappa 0.10615(0.35990)\tTrain_MF1 0.28738(0.41674)\tTrain_Precision 0.25174(0.47887)\tTrain_Sensitivity 0.34000(0.42954)\tTrain_Specificity 0.80862(0.86990)\tTime 0.137s (0.140s)\tSpeed 233.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [11/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.95464630628887, Training Accuracy      : 0.5896146616541353, Training G-Mean      : 0.6155915491634159\n",
            "Training Kappa      : 0.36934217420600907,Training MF1     : 0.42719833447744965, Training Precision      : 0.4967885347259673, Training Sensitivity      : 0.43939407661557195, Training Specificity      : 0.8717189600436309\n",
            "Validation Results : \n",
            "Validation Loss   : 2.1489991391146623, Validation Accuracy : 0.3777314814814815, Validation G-Mean      : 0.5528691716534634\n",
            "Validation Kappa     : 0.17350399835217023, Validation MF1      : 0.2891096855755206, Validation Precision      : 0.3423992018456811,  Validation Sensitivity      : 0.37137873492859025, Validation Specificity      : 0.8367233387849948\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :9.344458597677725\n",
            "Class wise sensitivity W: 0.3223535126006162, S1: 0.0, S2: 0.6128029470090512, S3: 0.7870370370370371, R: 0.1347001779962469\n",
            "Class wise specificity W: 0.9895552705835413, S1: 0.9888322883182101, S2: 0.4229756643374761, S3: 0.8591918989464089, R: 0.9230615717393381\n",
            "Class wise F1  W: 0.4600450109552454, S1: 0.0, S2: 0.395142177188838, S3: 0.42690226821987715, R: 0.1634589715136422\n",
            "===========================================================Training Epoch : [12/100] ===========================================================================================================>\n",
            "Epoch: [12/100][0/190]\tTrain_Loss 4.53050 (4.53050)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.66765(0.66765)\tTrain_Kappa 0.49527(0.49527)\tTrain_MF1 0.50906(0.50906)\tTrain_Precision 0.53000(0.53000)\tTrain_Sensitivity 0.50060(0.50060)\tTrain_Specificity 0.89045(0.89045)\tTime 0.137s (0.137s)\tSpeed 233.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [12/100][100/190]\tTrain_Loss 4.92248 (5.04644)\tTrain_Acc 0.62500 (0.59746)\tTrain_G-Mean 0.53676(0.62999)\tTrain_Kappa 0.29412(0.39106)\tTrain_MF1 0.35814(0.44453)\tTrain_Precision 0.50833(0.51029)\tTrain_Sensitivity 0.33561(0.45743)\tTrain_Specificity 0.85847(0.87610)\tTime 0.151s (0.141s)\tSpeed 211.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [12/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.01439622828835, Training Accuracy      : 0.5949953007518797, Training G-Mean      : 0.6222950053286644\n",
            "Training Kappa      : 0.3799410944109681,Training MF1     : 0.4343681961768551, Training Precision      : 0.5041296827322558, Training Sensitivity      : 0.4472603452519364, Training Specificity      : 0.8738132977877793\n",
            "Validation Results : \n",
            "Validation Loss   : 3.2789061466852822, Validation Accuracy : 0.2499537037037037, Validation G-Mean      : 0.4637780428940668\n",
            "Validation Kappa     : 0.05734622647614169, Validation MF1      : 0.18805075784524283, Validation Precision      : 0.2852626008292039,  Validation Sensitivity      : 0.27704399746877173, Validation Specificity      : 0.8123470426709566\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :14.679946899414062\n",
            "Class wise sensitivity W: 0.27979536492515494, S1: 0.018518518518518517, S2: 0.28922826713985866, S3: 0.7623456782764859, R: 0.0353321584838408\n",
            "Class wise specificity W: 0.9934479704609623, S1: 0.996119929684533, S2: 0.42874029002807756, S3: 0.6516763810758237, R: 0.9917506421053851\n",
            "Class wise F1  W: 0.41098332018763933, S1: 0.02469135876055117, S2: 0.2034357808254383, S3: 0.2498205773256443, R: 0.05132275212694098\n",
            "===========================================================Training Epoch : [13/100] ===========================================================================================================>\n",
            "Epoch: [13/100][0/190]\tTrain_Loss 4.20984 (4.20984)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.59059(0.59059)\tTrain_Kappa 0.36391(0.36391)\tTrain_MF1 0.42943(0.42943)\tTrain_Precision 0.59638(0.59638)\tTrain_Sensitivity 0.40238(0.40238)\tTrain_Specificity 0.86684(0.86684)\tTime 0.139s (0.139s)\tSpeed 230.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [13/100][100/190]\tTrain_Loss 5.32995 (4.65013)\tTrain_Acc 0.56250 (0.60303)\tTrain_G-Mean 0.59227(0.63836)\tTrain_Kappa 0.31498(0.39855)\tTrain_MF1 0.41684(0.45442)\tTrain_Precision 0.64167(0.52162)\tTrain_Sensitivity 0.40905(0.46795)\tTrain_Specificity 0.85755(0.87830)\tTime 0.142s (0.140s)\tSpeed 224.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [13/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.588604692408913, Training Accuracy      : 0.6117716165413534, Training G-Mean      : 0.6410243539820405\n",
            "Training Kappa      : 0.4117196267922655,Training MF1     : 0.45583525960382654, Training Precision      : 0.5236071572648848, Training Sensitivity      : 0.4708933438439116, Training Specificity      : 0.8807950867477219\n",
            "Validation Results : \n",
            "Validation Loss   : 3.1765181576764143, Validation Accuracy : 0.35412037037037036, Validation G-Mean      : 0.537477702884232\n",
            "Validation Kappa     : 0.13795934669907767, Validation MF1      : 0.24622605674796635, Validation Precision      : 0.2997983308302032,  Validation Sensitivity      : 0.35221913103704094, Validation Specificity      : 0.8291714719048253\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :14.23260498046875\n",
            "Class wise sensitivity W: 0.34293263488345677, S1: 0.0, S2: 0.6570519076453315, S3: 0.7611111126564167, R: 0.0\n",
            "Class wise specificity W: 0.991923866448579, S1: 0.9987654310685617, S2: 0.32788116070959306, S3: 0.8313177139670761, R: 0.9959691873303166\n",
            "Class wise F1  W: 0.49453611230408706, S1: 0.0, S2: 0.3741656453521163, S3: 0.36242852608362836, R: 0.0\n",
            "===========================================================Training Epoch : [14/100] ===========================================================================================================>\n",
            "Epoch: [14/100][0/190]\tTrain_Loss 4.37380 (4.37380)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.63021(0.63021)\tTrain_Kappa 0.38951(0.38951)\tTrain_MF1 0.48010(0.48010)\tTrain_Precision 0.65000(0.65000)\tTrain_Sensitivity 0.45714(0.45714)\tTrain_Specificity 0.86879(0.86879)\tTime 0.139s (0.139s)\tSpeed 230.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [14/100][100/190]\tTrain_Loss 6.24468 (4.64363)\tTrain_Acc 0.53125 (0.60489)\tTrain_G-Mean 0.65747(0.63759)\tTrain_Kappa 0.36592(0.40028)\tTrain_MF1 0.43936(0.45106)\tTrain_Precision 0.57219(0.51367)\tTrain_Sensitivity 0.49524(0.46678)\tTrain_Specificity 0.87286(0.87841)\tTime 0.137s (0.141s)\tSpeed 233.2 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [14/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.754386078683954, Training Accuracy      : 0.6010808270676692, Training G-Mean      : 0.6363250890786709\n",
            "Training Kappa      : 0.39589784248999516,Training MF1     : 0.4508424481592684, Training Precision      : 0.5182121607974955, Training Sensitivity      : 0.4654642389244156, Training Specificity      : 0.8774108243616006\n",
            "Validation Results : \n",
            "Validation Loss   : 2.7357878420088024, Validation Accuracy : 0.3223611111111111, Validation G-Mean      : 0.5134763183771964\n",
            "Validation Kappa     : 0.11953260208269682, Validation MF1      : 0.23505617082118993, Validation Precision      : 0.33113991093856315,  Validation Sensitivity      : 0.3261371066172918, Validation Specificity      : 0.8261306444803874\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.341669435854312\n",
            "Class wise sensitivity W: 0.2929089480528125, S1: 0.009259259259259259, S2: 0.5046049190892113, S3: 0.7716049397433246, R: 0.052307466941851156\n",
            "Class wise specificity W: 0.9918820394410027, S1: 0.9886552890141805, S2: 0.4093678085892289, S3: 0.7564049495591058, R: 0.9843431357984189\n",
            "Class wise F1  W: 0.43159081869655186, S1: 0.012345679380275585, S2: 0.3329756370297185, S3: 0.31629934575822616, R: 0.08206937324117732\n",
            "===========================================================Training Epoch : [15/100] ===========================================================================================================>\n",
            "Epoch: [15/100][0/190]\tTrain_Loss 4.73051 (4.73051)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.57623(0.57623)\tTrain_Kappa 0.29308(0.29308)\tTrain_MF1 0.40222(0.40222)\tTrain_Precision 0.58435(0.58435)\tTrain_Sensitivity 0.38842(0.38842)\tTrain_Specificity 0.85485(0.85485)\tTime 0.139s (0.139s)\tSpeed 229.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [15/100][100/190]\tTrain_Loss 8.33449 (5.01328)\tTrain_Acc 0.46875 (0.59746)\tTrain_G-Mean 0.55721(0.62650)\tTrain_Kappa 0.23810(0.38331)\tTrain_MF1 0.30500(0.43909)\tTrain_Precision 0.28000(0.50215)\tTrain_Sensitivity 0.36667(0.45297)\tTrain_Specificity 0.84677(0.87372)\tTime 0.145s (0.141s)\tSpeed 221.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [15/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.90670999978718, Training Accuracy      : 0.5917293233082707, Training G-Mean      : 0.6229536021290565\n",
            "Training Kappa      : 0.37657186392935943,Training MF1     : 0.43380987275587884, Training Precision      : 0.4959218008267255, Training Sensitivity      : 0.4485555831068438, Training Specificity      : 0.8731347281132877\n",
            "Validation Results : \n",
            "Validation Loss   : 2.0748397332650645, Validation Accuracy : 0.42768518518518517, Validation G-Mean      : 0.5137913983959221\n",
            "Validation Kappa     : 0.19320004615686917, Validation MF1      : 0.2574881073501375, Validation Precision      : 0.3257633402391717,  Validation Sensitivity      : 0.31902102220941475, Validation Specificity      : 0.8408245810204081\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :8.435933854844835\n",
            "Class wise sensitivity W: 0.34370887334699984, S1: 0.0, S2: 0.9689888291888766, S3: 0.2777777788815675, R: 0.004629629629629629\n",
            "Class wise specificity W: 0.9829635995405691, S1: 0.9867913722991943, S2: 0.2504307978131153, S3: 0.9839371354491623, R: 1.0\n",
            "Class wise F1  W: 0.4862376771591328, S1: 0.0, S2: 0.4994979743604307, S3: 0.29347443249490524, R: 0.008230452736218771\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [16/100] ===========================================================================================================>\n",
            "Epoch: [16/100][0/190]\tTrain_Loss 5.48241 (5.48241)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.55603(0.55603)\tTrain_Kappa 0.28492(0.28492)\tTrain_MF1 0.35098(0.35098)\tTrain_Precision 0.42029(0.42029)\tTrain_Sensitivity 0.36182(0.36182)\tTrain_Specificity 0.85448(0.85448)\tTime 0.144s (0.144s)\tSpeed 222.7 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [16/100][100/190]\tTrain_Loss 4.48380 (5.23782)\tTrain_Acc 0.56250 (0.56126)\tTrain_G-Mean 0.70232(0.60094)\tTrain_Kappa 0.41053(0.32030)\tTrain_MF1 0.47914(0.39397)\tTrain_Precision 0.44402(0.44514)\tTrain_Sensitivity 0.55857(0.42407)\tTrain_Specificity 0.88307(0.86188)\tTime 0.138s (0.141s)\tSpeed 231.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [16/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.173782588306226, Training Accuracy      : 0.5627349624060151, Training G-Mean      : 0.6022626672823712\n",
            "Training Kappa      : 0.3314515041259315,Training MF1     : 0.3957761464307181, Training Precision      : 0.45014560688483085, Training Sensitivity      : 0.4239529413220127, Training Specificity      : 0.8642876124538874\n",
            "Validation Results : \n",
            "Validation Loss   : 2.99895770461471, Validation Accuracy : 0.34800925925925924, Validation G-Mean      : 0.5138775195539881\n",
            "Validation Kappa     : 0.12564975824230787, Validation MF1      : 0.2330264536318955, Validation Precision      : 0.2960795490278138,  Validation Sensitivity      : 0.32484996931420435, Validation Specificity      : 0.8261919431112433\n",
            "Validation T Acc     : 0.8535648148148148, Val_KD_Loss :13.26954514892013\n",
            "Class wise sensitivity W: 0.2700230981464739, S1: 0.0, S2: 0.7085477345519595, S3: 0.6419753101136949, R: 0.0037037037588931896\n",
            "Class wise specificity W: 0.9892840849028693, S1: 0.9947880131226999, S2: 0.2785144155776059, S3: 0.8743344037621109, R: 0.9940387981909292\n",
            "Class wise F1  W: 0.3972143155557138, S1: 0.0, S2: 0.39486733465283, S3: 0.3663166110162382, R: 0.006734006934695774\n",
            "===========================================================Training Epoch : [17/100] ===========================================================================================================>\n",
            "Epoch: [17/100][0/190]\tTrain_Loss 6.61381 (6.61381)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.49558(0.49558)\tTrain_Kappa 0.24647(0.24647)\tTrain_MF1 0.31306(0.31306)\tTrain_Precision 0.38095(0.38095)\tTrain_Sensitivity 0.29000(0.29000)\tTrain_Specificity 0.84688(0.84688)\tTime 0.140s (0.140s)\tSpeed 229.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [17/100][100/190]\tTrain_Loss 5.76260 (5.30276)\tTrain_Acc 0.62500 (0.58601)\tTrain_G-Mean 0.69143(0.60158)\tTrain_Kappa 0.41906(0.35163)\tTrain_MF1 0.50655(0.40815)\tTrain_Precision 0.49018(0.46791)\tTrain_Sensitivity 0.54250(0.42164)\tTrain_Specificity 0.88124(0.86781)\tTime 0.138s (0.140s)\tSpeed 232.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [17/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.197730941521494, Training Accuracy      : 0.5858317669172932, Training G-Mean      : 0.6104694216098517\n",
            "Training Kappa      : 0.36056121072401726,Training MF1     : 0.42221412732412916, Training Precision      : 0.4903448792978338, Training Sensitivity      : 0.4330159143632963, Training Specificity      : 0.8695806789084481\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8559602543159768, Validation Accuracy : 0.34421296296296294, Validation G-Mean      : 0.501864578227863\n",
            "Validation Kappa     : 0.11551276896301055, Validation MF1      : 0.22446007093897566, Validation Precision      : 0.2965939011838701,  Validation Sensitivity      : 0.31065154481265284, Validation Specificity      : 0.8237750956857647\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :11.810267978244358\n",
            "Class wise sensitivity W: 0.10500750980443424, S1: 0.0, S2: 0.8747052373709502, S3: 0.48950617512067157, R: 0.08403880176720796\n",
            "Class wise specificity W: 1.0, S1: 1.0, S2: 0.2936443457448924, S3: 0.9361192010067128, R: 0.8891119316772178\n",
            "Class wise F1  W: 0.17645637111531365, S1: 0.0, S2: 0.47385156375390514, S3: 0.37773770093917847, R: 0.09425471888648139\n",
            "===========================================================Training Epoch : [18/100] ===========================================================================================================>\n",
            "Epoch: [18/100][0/190]\tTrain_Loss 4.47354 (4.47354)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.58307(0.58307)\tTrain_Kappa 0.31183(0.31183)\tTrain_MF1 0.35737(0.35737)\tTrain_Precision 0.48333(0.48333)\tTrain_Sensitivity 0.39524(0.39524)\tTrain_Specificity 0.86017(0.86017)\tTime 0.137s (0.137s)\tSpeed 232.9 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [18/100][100/190]\tTrain_Loss 5.32825 (5.16440)\tTrain_Acc 0.53125 (0.57550)\tTrain_G-Mean 0.56590(0.60546)\tTrain_Kappa 0.31915(0.34667)\tTrain_MF1 0.39623(0.41497)\tTrain_Precision 0.48889(0.48546)\tTrain_Sensitivity 0.37048(0.42718)\tTrain_Specificity 0.86441(0.86674)\tTime 0.138s (0.140s)\tSpeed 231.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [18/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.1884414961463525, Training Accuracy      : 0.5836231203007519, Training G-Mean      : 0.6131483962798014\n",
            "Training Kappa      : 0.3596900987052385,Training MF1     : 0.42153916139351666, Training Precision      : 0.4899405898467494, Training Sensitivity      : 0.4369478729367254, Training Specificity      : 0.8691053285096818\n",
            "Validation Results : \n",
            "Validation Loss   : 2.0935323061766447, Validation Accuracy : 0.40273148148148147, Validation G-Mean      : 0.5185129487177431\n",
            "Validation Kappa     : 0.15527937109872625, Validation MF1      : 0.25875974551395137, Validation Precision      : 0.3476469273920413,  Validation Sensitivity      : 0.32675039635764225, Validation Specificity      : 0.8328938984208638\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :8.775113953484428\n",
            "Class wise sensitivity W: 0.2712472991810905, S1: 0.018518518518518517, S2: 0.9402824574046664, S3: 0.39753086699379814, R: 0.006172839690137793\n",
            "Class wise specificity W: 0.986618165616636, S1: 0.992178323092284, S2: 0.2110095691901666, S3: 0.9803045524491204, R: 0.9943588817561114\n",
            "Class wise F1  W: 0.39773740702205235, S1: 0.018518518518518517, S2: 0.46510893382408003, S3: 0.4018518571500425, R: 0.010582011055063319\n",
            "===========================================================Training Epoch : [19/100] ===========================================================================================================>\n",
            "Epoch: [19/100][0/190]\tTrain_Loss 6.35420 (6.35420)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.68737(0.68737)\tTrain_Kappa 0.31012(0.31012)\tTrain_MF1 0.57957(0.57957)\tTrain_Precision 0.63485(0.63485)\tTrain_Sensitivity 0.55532(0.55532)\tTrain_Specificity 0.85082(0.85082)\tTime 0.138s (0.138s)\tSpeed 232.3 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [19/100][100/190]\tTrain_Loss 6.00046 (5.09242)\tTrain_Acc 0.50000 (0.58942)\tTrain_G-Mean 0.57661(0.62238)\tTrain_Kappa 0.31915(0.36921)\tTrain_MF1 0.36479(0.42920)\tTrain_Precision 0.50564(0.50010)\tTrain_Sensitivity 0.38500(0.44771)\tTrain_Specificity 0.86357(0.87100)\tTime 0.138s (0.140s)\tSpeed 231.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [19/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.024499003510726, Training Accuracy      : 0.590718984962406, Training G-Mean      : 0.6239593670637521\n",
            "Training Kappa      : 0.3736538124837363,Training MF1     : 0.4328486289162383, Training Precision      : 0.5050582046728385, Training Sensitivity      : 0.44985320607298307, Training Specificity      : 0.8720702653966449\n",
            "Validation Results : \n",
            "Validation Loss   : 3.1744905754371926, Validation Accuracy : 0.24319444444444446, Validation G-Mean      : 0.4821609037282854\n",
            "Validation Kappa     : 0.043996486904227396, Validation MF1      : 0.17964715957641603, Validation Precision      : 0.2755852918382044,  Validation Sensitivity      : 0.292924753052217, Validation Specificity      : 0.8088392439815733\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :14.650465929949725\n",
            "Class wise sensitivity W: 0.1958809757122287, S1: 0.0, S2: 0.3784429629643758, S3: 0.8395061746791557, R: 0.05079365190532473\n",
            "Class wise specificity W: 0.9943827633504514, S1: 0.9962537222438388, S2: 0.40119434893131256, S3: 0.6840351234983515, R: 0.9683302618839122\n",
            "Class wise F1  W: 0.30894631423332075, S1: 0.0, S2: 0.2547514885663986, S3: 0.2660996417204539, R: 0.06843835336190683\n",
            "===========================================================Training Epoch : [20/100] ===========================================================================================================>\n",
            "Epoch: [20/100][0/190]\tTrain_Loss 3.66695 (3.66695)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.73061(0.73061)\tTrain_Kappa 0.48636(0.48636)\tTrain_MF1 0.52114(0.52114)\tTrain_Precision 0.48571(0.48571)\tTrain_Sensitivity 0.59123(0.59123)\tTrain_Specificity 0.90285(0.90285)\tTime 0.141s (0.141s)\tSpeed 227.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [20/100][100/190]\tTrain_Loss 6.05187 (4.80544)\tTrain_Acc 0.40625 (0.61696)\tTrain_G-Mean 0.51316(0.64073)\tTrain_Kappa 0.19895(0.41682)\tTrain_MF1 0.33370(0.46251)\tTrain_Precision 0.52970(0.54987)\tTrain_Sensitivity 0.31333(0.46945)\tTrain_Specificity 0.84043(0.88102)\tTime 0.139s (0.140s)\tSpeed 230.9 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [20/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.774344789354425, Training Accuracy      : 0.6174812030075187, Training G-Mean      : 0.6481415691213303\n",
            "Training Kappa      : 0.41976437610356,Training MF1     : 0.4662164792261627, Training Precision      : 0.5384731630118267, Training Sensitivity      : 0.47952340835019147, Training Specificity      : 0.8820371101090779\n",
            "Validation Results : \n",
            "Validation Loss   : 2.3515605838210494, Validation Accuracy : 0.3422222222222222, Validation G-Mean      : 0.5181323401126613\n",
            "Validation Kappa     : 0.13571633928441193, Validation MF1      : 0.25595112503678713, Validation Precision      : 0.3364353763284506,  Validation Sensitivity      : 0.33072960321550016, Validation Specificity      : 0.8297801021072596\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :10.07913985075774\n",
            "Class wise sensitivity W: 0.26903940130163123, S1: 0.0, S2: 0.5687355945507685, S3: 0.66358024764944, R: 0.15229277257566098\n",
            "Class wise specificity W: 0.992839844138534, S1: 1.0, S2: 0.4382271694916266, S3: 0.8331998145138776, R: 0.8846336823922617\n",
            "Class wise F1  W: 0.39986875542887934, S1: 0.0, S2: 0.3788540658575517, S3: 0.3277715974383884, R: 0.17326120645911605\n",
            "===========================================================Training Epoch : [21/100] ===========================================================================================================>\n",
            "Epoch: [21/100][0/190]\tTrain_Loss 6.38891 (6.38891)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.65455(0.65455)\tTrain_Kappa 0.35260(0.35260)\tTrain_MF1 0.45668(0.45668)\tTrain_Precision 0.44384(0.44384)\tTrain_Sensitivity 0.49286(0.49286)\tTrain_Specificity 0.86928(0.86928)\tTime 0.140s (0.140s)\tSpeed 227.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [21/100][100/190]\tTrain_Loss 5.40718 (4.48745)\tTrain_Acc 0.59375 (0.62252)\tTrain_G-Mean 0.63587(0.64978)\tTrain_Kappa 0.38824(0.42587)\tTrain_MF1 0.46564(0.47336)\tTrain_Precision 0.49444(0.53899)\tTrain_Sensitivity 0.45917(0.48187)\tTrain_Specificity 0.88057(0.88261)\tTime 0.138s (0.140s)\tSpeed 231.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [21/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.627624810369391, Training Accuracy      : 0.6179746240601504, Training G-Mean      : 0.6439523401264117\n",
            "Training Kappa      : 0.4182674724921374,Training MF1     : 0.4652296241333612, Training Precision      : 0.5408164029058656, Training Sensitivity      : 0.47437336375054584, Training Specificity      : 0.8810195941830933\n",
            "Validation Results : \n",
            "Validation Loss   : 1.987340878557276, Validation Accuracy : 0.36768518518518517, Validation G-Mean      : 0.5506654630963626\n",
            "Validation Kappa     : 0.18916616276786, Validation MF1      : 0.29499891494159347, Validation Precision      : 0.3574921526290752,  Validation Sensitivity      : 0.3679690720306502, Validation Specificity      : 0.841359865444678\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :8.522849400838217\n",
            "Class wise sensitivity W: 0.3746088332048169, S1: 0.0, S2: 0.38925180666976505, S3: 0.7580246925354004, R: 0.31796002774326887\n",
            "Class wise specificity W: 0.984705353224719, S1: 0.994744806377976, S2: 0.6208280909944464, S3: 0.7644903527365791, R: 0.8420307238896688\n",
            "Class wise F1  W: 0.5179617962351551, S1: 0.0, S2: 0.3262611503402392, S3: 0.31930066534766444, R: 0.31147096278490843\n",
            "===========================================================Training Epoch : [22/100] ===========================================================================================================>\n",
            "Epoch: [22/100][0/190]\tTrain_Loss 3.52564 (3.52564)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.73363(0.73363)\tTrain_Kappa 0.52688(0.52688)\tTrain_MF1 0.59250(0.59250)\tTrain_Precision 0.68667(0.68667)\tTrain_Sensitivity 0.59524(0.59524)\tTrain_Specificity 0.90419(0.90419)\tTime 0.138s (0.138s)\tSpeed 231.5 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [22/100][100/190]\tTrain_Loss 4.31297 (4.86714)\tTrain_Acc 0.65625 (0.60272)\tTrain_G-Mean 0.70407(0.63883)\tTrain_Kappa 0.39829(0.40000)\tTrain_MF1 0.51000(0.44977)\tTrain_Precision 0.48333(0.51124)\tTrain_Sensitivity 0.56667(0.46849)\tTrain_Specificity 0.87478(0.87746)\tTime 0.138s (0.140s)\tSpeed 231.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [22/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.907984640096363, Training Accuracy      : 0.6053336466165413, Training G-Mean      : 0.6337993905586204\n",
            "Training Kappa      : 0.39551121423216784,Training MF1     : 0.44802469982912685, Training Precision      : 0.5173522593629991, Training Sensitivity      : 0.46208153821920106, Training Specificity      : 0.8764230166532488\n",
            "Validation Results : \n",
            "Validation Loss   : 3.459598188047056, Validation Accuracy : 0.27574074074074073, Validation G-Mean      : 0.4735264767084049\n",
            "Validation Kappa     : 0.08278836212374029, Validation MF1      : 0.18980339158464365, Validation Precision      : 0.2600989643898275,  Validation Sensitivity      : 0.28203391570735864, Validation Specificity      : 0.8189355218852008\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :16.045243616457338\n",
            "Class wise sensitivity W: 0.2913754005674963, S1: 0.0, S2: 0.4089176329197707, S3: 0.709876545049526, R: 0.0\n",
            "Class wise specificity W: 0.9944351624559473, S1: 0.9910229312049018, S2: 0.45273564921485054, S3: 0.6564838665503042, R: 1.0\n",
            "Class wise F1  W: 0.43280907196027263, S1: 0.0, S2: 0.28202970877841665, S3: 0.23417817718452877, R: 0.0\n",
            "===========================================================Training Epoch : [23/100] ===========================================================================================================>\n",
            "Epoch: [23/100][0/190]\tTrain_Loss 4.02370 (4.02370)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.61128(0.61128)\tTrain_Kappa 0.46369(0.46369)\tTrain_MF1 0.43766(0.43766)\tTrain_Precision 0.48784(0.48784)\tTrain_Sensitivity 0.41676(0.41676)\tTrain_Specificity 0.89658(0.89658)\tTime 0.139s (0.139s)\tSpeed 230.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [23/100][100/190]\tTrain_Loss 5.11845 (4.93898)\tTrain_Acc 0.65625 (0.60118)\tTrain_G-Mean 0.70536(0.63063)\tTrain_Kappa 0.53254(0.39297)\tTrain_MF1 0.53872(0.44904)\tTrain_Precision 0.54667(0.52741)\tTrain_Sensitivity 0.54762(0.45820)\tTrain_Specificity 0.90854(0.87579)\tTime 0.137s (0.141s)\tSpeed 233.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [23/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.954459940759759, Training Accuracy      : 0.6043703007518797, Training G-Mean      : 0.6296977200190584\n",
            "Training Kappa      : 0.39167755453389946,Training MF1     : 0.4472442211132302, Training Precision      : 0.5230337241684136, Training Sensitivity      : 0.4568994530956997, Training Specificity      : 0.8756312925250908\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2976461516486273, Validation Accuracy : 0.34750000000000003, Validation G-Mean      : 0.5377249640679179\n",
            "Validation Kappa     : 0.14918018100990746, Validation MF1      : 0.2556198297827332, Validation Precision      : 0.31345242118393934,  Validation Sensitivity      : 0.3519897263911035, Validation Specificity      : 0.8332545512252383\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :10.15365203221639\n",
            "Class wise sensitivity W: 0.29700570167214785, S1: 0.0, S2: 0.5886043034217976, S3: 0.7802469145368647, R: 0.0940917123247076\n",
            "Class wise specificity W: 0.9914279646343656, S1: 0.9532231114528797, S2: 0.4807703020396056, S3: 0.8272904930291353, R: 0.9135608849702058\n",
            "Class wise F1  W: 0.42187285312899836, S1: 0.0, S2: 0.3886698649989234, S3: 0.34960729987533007, R: 0.11794913091041424\n",
            "===========================================================Training Epoch : [24/100] ===========================================================================================================>\n",
            "Epoch: [24/100][0/190]\tTrain_Loss 6.30582 (6.30582)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.60563(0.60563)\tTrain_Kappa 0.31818(0.31818)\tTrain_MF1 0.43259(0.43259)\tTrain_Precision 0.60476(0.60476)\tTrain_Sensitivity 0.42637(0.42637)\tTrain_Specificity 0.86025(0.86025)\tTime 0.138s (0.138s)\tSpeed 232.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [24/100][100/190]\tTrain_Loss 5.10487 (5.00812)\tTrain_Acc 0.56250 (0.60334)\tTrain_G-Mean 0.60161(0.63334)\tTrain_Kappa 0.34214(0.39839)\tTrain_MF1 0.38721(0.45201)\tTrain_Precision 0.46798(0.52557)\tTrain_Sensitivity 0.41714(0.46058)\tTrain_Specificity 0.86765(0.87717)\tTime 0.136s (0.140s)\tSpeed 234.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [24/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.0822622750934805, Training Accuracy      : 0.5953242481203007, Training G-Mean      : 0.6212333600249862\n",
            "Training Kappa      : 0.37726216380867644,Training MF1     : 0.4374352722105226, Training Precision      : 0.5099651427488581, Training Sensitivity      : 0.4464682004483127, Training Specificity      : 0.8727141835030758\n",
            "Validation Results : \n",
            "Validation Loss   : 2.4171040102287575, Validation Accuracy : 0.3914814814814815, Validation G-Mean      : 0.5191834048388414\n",
            "Validation Kappa     : 0.14979350922244555, Validation MF1      : 0.25693714210280666, Validation Precision      : 0.3299280264863262,  Validation Sensitivity      : 0.33024763520117156, Validation Specificity      : 0.8315837537248929\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :10.526884732423005\n",
            "Class wise sensitivity W: 0.2830042430648097, S1: 0.0, S2: 0.8762586205093948, S3: 0.4753086456546077, R: 0.01666666677704564\n",
            "Class wise specificity W: 0.9862732136691058, S1: 0.9987228601067154, S2: 0.23982914040486017, S3: 0.9477090526510168, R: 0.9853845017927664\n",
            "Class wise F1  W: 0.41236993118568704, S1: 0.0, S2: 0.45881301826900905, S3: 0.38551922142505646, R: 0.027983539634280734\n",
            "===========================================================Training Epoch : [25/100] ===========================================================================================================>\n",
            "Epoch: [25/100][0/190]\tTrain_Loss 5.13070 (5.13070)\tTrain_Acc 0.75000 (0.75000)\tTrain_G-Mean 0.68985(0.68985)\tTrain_Kappa 0.55633(0.55633)\tTrain_MF1 0.50896(0.50896)\tTrain_Precision 0.50333(0.50333)\tTrain_Sensitivity 0.52000(0.52000)\tTrain_Specificity 0.91519(0.91519)\tTime 0.138s (0.138s)\tSpeed 231.9 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [25/100][100/190]\tTrain_Loss 7.05200 (5.18874)\tTrain_Acc 0.46875 (0.57271)\tTrain_G-Mean 0.60989(0.60576)\tTrain_Kappa 0.23810(0.34942)\tTrain_MF1 0.35039(0.41250)\tTrain_Precision 0.42969(0.48506)\tTrain_Sensitivity 0.43968(0.42704)\tTrain_Specificity 0.84598(0.86795)\tTime 0.138s (0.140s)\tSpeed 232.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [25/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.031822963764793, Training Accuracy      : 0.5886513157894737, Training G-Mean      : 0.6198573807254766\n",
            "Training Kappa      : 0.37231806148740615,Training MF1     : 0.4320642425041452, Training Precision      : 0.5024270623608639, Training Sensitivity      : 0.44462938201270624, Training Specificity      : 0.8720399420982912\n",
            "Validation Results : \n",
            "Validation Loss   : 3.3873197944075972, Validation Accuracy : 0.24300925925925926, Validation G-Mean      : 0.4748789472402067\n",
            "Validation Kappa     : 0.04258168175450461, Validation MF1      : 0.16651194211509493, Validation Precision      : 0.25277173753689836,  Validation Sensitivity      : 0.2836077731516626, Validation Specificity      : 0.8091712029995742\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :15.326136023909957\n",
            "Class wise sensitivity W: 0.1869421769623403, S1: 0.0, S2: 0.395911502065482, S3: 0.8351851867304908, R: 0.0\n",
            "Class wise specificity W: 1.0, S1: 0.9893482813128719, S2: 0.40057667151645376, S3: 0.6758878782943443, R: 0.9800431838742009\n",
            "Class wise F1  W: 0.3020561850733227, S1: 0.0, S2: 0.27426017241345513, S3: 0.2562433530886968, R: 0.0\n",
            "===========================================================Training Epoch : [26/100] ===========================================================================================================>\n",
            "Epoch: [26/100][0/190]\tTrain_Loss 4.28598 (4.28598)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.60396(0.60396)\tTrain_Kappa 0.37950(0.37950)\tTrain_MF1 0.42607(0.42607)\tTrain_Precision 0.45098(0.45098)\tTrain_Sensitivity 0.41670(0.41670)\tTrain_Specificity 0.87535(0.87535)\tTime 0.138s (0.138s)\tSpeed 231.5 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [26/100][100/190]\tTrain_Loss 4.81167 (4.78521)\tTrain_Acc 0.62500 (0.60644)\tTrain_G-Mean 0.61186(0.63351)\tTrain_Kappa 0.33101(0.39544)\tTrain_MF1 0.43682(0.45075)\tTrain_Precision 0.44710(0.51678)\tTrain_Sensitivity 0.43667(0.46292)\tTrain_Specificity 0.85735(0.87672)\tTime 0.137s (0.140s)\tSpeed 233.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [26/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.920025779071607, Training Accuracy      : 0.6019501879699248, Training G-Mean      : 0.6308855514598537\n",
            "Training Kappa      : 0.3897325788825081,Training MF1     : 0.4478690853714942, Training Precision      : 0.515642448961735, Training Sensitivity      : 0.4596246145038228, Training Specificity      : 0.8752723499505145\n",
            "Validation Results : \n",
            "Validation Loss   : 2.9244960767251476, Validation Accuracy : 0.2550925925925926, Validation G-Mean      : 0.46594645227714465\n",
            "Validation Kappa     : 0.047466836277855515, Validation MF1      : 0.1811722368001938, Validation Precision      : 0.26744400889233305,  Validation Sensitivity      : 0.2781539269067623, Validation Specificity      : 0.8105188118086921\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :13.97316805521647\n",
            "Class wise sensitivity W: 0.28393931769662434, S1: 0.0, S2: 0.3413982187156324, S3: 0.7623456782764859, R: 0.0030864198450688963\n",
            "Class wise specificity W: 0.9935586607014691, S1: 0.9948632606753597, S2: 0.37325877834249427, S3: 0.6964555471031754, R: 0.994457812220962\n",
            "Class wise F1  W: 0.41756052772204083, S1: 0.0, S2: 0.22292668013660996, S3: 0.2596759702320452, R: 0.0056980059102729515\n",
            "===========================================================Training Epoch : [27/100] ===========================================================================================================>\n",
            "Epoch: [27/100][0/190]\tTrain_Loss 4.95776 (4.95776)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.59997(0.59997)\tTrain_Kappa 0.51913(0.51913)\tTrain_MF1 0.43082(0.43082)\tTrain_Precision 0.49048(0.49048)\tTrain_Sensitivity 0.39423(0.39423)\tTrain_Specificity 0.91309(0.91309)\tTime 0.139s (0.139s)\tSpeed 231.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [27/100][100/190]\tTrain_Loss 5.67923 (5.13437)\tTrain_Acc 0.53125 (0.58014)\tTrain_G-Mean 0.56626(0.62089)\tTrain_Kappa 0.27492(0.37039)\tTrain_MF1 0.31562(0.42508)\tTrain_Precision 0.45667(0.48827)\tTrain_Sensitivity 0.37500(0.44551)\tTrain_Specificity 0.85508(0.87223)\tTime 0.139s (0.140s)\tSpeed 230.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [27/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.081776169726723, Training Accuracy      : 0.5878289473684211, Training G-Mean      : 0.6220187162514692\n",
            "Training Kappa      : 0.37312517703784337,Training MF1     : 0.4323456023398199, Training Precision      : 0.5011060774640036, Training Sensitivity      : 0.4478351899589365, Training Specificity      : 0.872691820844223\n",
            "Validation Results : \n",
            "Validation Loss   : 2.821707804997762, Validation Accuracy : 0.2574074074074074, Validation G-Mean      : 0.47555380819558324\n",
            "Validation Kappa     : 0.05102825164582838, Validation MF1      : 0.18953349822097354, Validation Precision      : 0.2730367843475606,  Validation Sensitivity      : 0.2876989286806848, Validation Specificity      : 0.8103887917818847\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :12.947993172539604\n",
            "Class wise sensitivity W: 0.27398055019202056, S1: 0.0, S2: 0.3314292753736178, S3: 0.8117283957975882, R: 0.021356422040197585\n",
            "Class wise specificity W: 0.9914592769410875, S1: 0.9986282587051392, S2: 0.36949656738175285, S3: 0.7051446614442048, R: 0.9872151944372389\n",
            "Class wise F1  W: 0.40692949295043945, S1: 0.0, S2: 0.22185641637554876, S3: 0.2855482476728934, R: 0.033333334105986136\n",
            "===========================================================Training Epoch : [28/100] ===========================================================================================================>\n",
            "Epoch: [28/100][0/190]\tTrain_Loss 3.57911 (3.57911)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.61655(0.61655)\tTrain_Kappa 0.46095(0.46095)\tTrain_MF1 0.43273(0.43273)\tTrain_Precision 0.55333(0.55333)\tTrain_Sensitivity 0.42667(0.42667)\tTrain_Specificity 0.89095(0.89095)\tTime 0.140s (0.140s)\tSpeed 228.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [28/100][100/190]\tTrain_Loss 4.66997 (4.85532)\tTrain_Acc 0.50000 (0.59623)\tTrain_G-Mean 0.47912(0.62612)\tTrain_Kappa 0.15651(0.38163)\tTrain_MF1 0.26039(0.43517)\tTrain_Precision 0.37436(0.48925)\tTrain_Sensitivity 0.27667(0.45291)\tTrain_Specificity 0.82972(0.87414)\tTime 0.138s (0.140s)\tSpeed 232.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [28/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.820950907155087, Training Accuracy      : 0.6023731203007519, Training G-Mean      : 0.6327657844723007\n",
            "Training Kappa      : 0.39465357451032623,Training MF1     : 0.44608227823909974, Training Precision      : 0.5060077347724061, Training Sensitivity      : 0.46080298929622326, Training Specificity      : 0.8769434192149266\n",
            "Validation Results : \n",
            "Validation Loss   : 1.8407956052709509, Validation Accuracy : 0.39296296296296296, Validation G-Mean      : 0.5548411328723718\n",
            "Validation Kappa     : 0.18730058889793125, Validation MF1      : 0.3053182673675042, Validation Precision      : 0.39127980989438516,  Validation Sensitivity      : 0.3738262489989952, Validation Specificity      : 0.8390701180254971\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :7.775625193560565\n",
            "Class wise sensitivity W: 0.27613112716763105, S1: 0.01675485074520111, S2: 0.6893378286449997, S3: 0.7209876554983633, R: 0.1659197829387806\n",
            "Class wise specificity W: 0.9933391875690885, S1: 0.9650794002744887, S2: 0.3897969352978247, S3: 0.8993955497388486, R: 0.9477395172472354\n",
            "Class wise F1  W: 0.41456341909037697, S1: 0.024074074294832017, S2: 0.42448602340839525, S3: 0.46855861058941595, R: 0.1949092094545011\n",
            "===========================================================Training Epoch : [29/100] ===========================================================================================================>\n",
            "Epoch: [29/100][0/190]\tTrain_Loss 4.97070 (4.97070)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.69420(0.69420)\tTrain_Kappa 0.50920(0.50920)\tTrain_MF1 0.49004(0.49004)\tTrain_Precision 0.57123(0.57123)\tTrain_Sensitivity 0.53147(0.53147)\tTrain_Specificity 0.90676(0.90676)\tTime 0.141s (0.141s)\tSpeed 226.3 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [29/100][100/190]\tTrain_Loss 5.18051 (4.66202)\tTrain_Acc 0.50000 (0.60179)\tTrain_G-Mean 0.59967(0.63679)\tTrain_Kappa 0.30717(0.39789)\tTrain_MF1 0.43453(0.45024)\tTrain_Precision 0.51474(0.51376)\tTrain_Sensitivity 0.41864(0.46526)\tTrain_Specificity 0.85899(0.87789)\tTime 0.138s (0.141s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [29/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.797230103141383, Training Accuracy      : 0.6071193609022556, Training G-Mean      : 0.6336955118387135\n",
            "Training Kappa      : 0.39724420302775887,Training MF1     : 0.44783513742057884, Training Precision      : 0.5127811665911425, Training Sensitivity      : 0.4617106118327695, Training Specificity      : 0.8773675341041469\n",
            "Validation Results : \n",
            "Validation Loss   : 3.534455166922675, Validation Accuracy : 0.21458333333333335, Validation G-Mean      : 0.47166359781479267\n",
            "Validation Kappa     : 0.02206206221042922, Validation MF1      : 0.16676541125332864, Validation Precision      : 0.2864457435078091,  Validation Sensitivity      : 0.2806623032247578, Validation Specificity      : 0.8039843309808659\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :16.92885974601463\n",
            "Class wise sensitivity W: 0.2413796145055029, S1: 0.0, S2: 0.21933930698368284, S3: 0.9104938286322134, R: 0.03209876600239012\n",
            "Class wise specificity W: 0.998456789387597, S1: 1.0, S2: 0.4244953051761345, S3: 0.611736293192263, R: 0.9852332671483358\n",
            "Class wise F1  W: 0.3774209342620991, S1: 0.0, S2: 0.16197513043880463, S3: 0.24201797169667702, R: 0.05241301986906263\n",
            "===========================================================Training Epoch : [30/100] ===========================================================================================================>\n",
            "Epoch: [30/100][0/190]\tTrain_Loss 4.45636 (4.45636)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.66330(0.66330)\tTrain_Kappa 0.36489(0.36489)\tTrain_MF1 0.54039(0.54039)\tTrain_Precision 0.70746(0.70746)\tTrain_Sensitivity 0.50778(0.50778)\tTrain_Specificity 0.86646(0.86646)\tTime 0.138s (0.138s)\tSpeed 232.3 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [30/100][100/190]\tTrain_Loss 5.56669 (4.85555)\tTrain_Acc 0.56250 (0.59437)\tTrain_G-Mean 0.59406(0.62555)\tTrain_Kappa 0.34021(0.37749)\tTrain_MF1 0.41784(0.43971)\tTrain_Precision 0.49333(0.51378)\tTrain_Sensitivity 0.40971(0.45196)\tTrain_Specificity 0.86138(0.87321)\tTime 0.138s (0.140s)\tSpeed 231.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [30/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.904907570387188, Training Accuracy      : 0.5976268796992481, Training G-Mean      : 0.6276076199514075\n",
            "Training Kappa      : 0.3843623698212559,Training MF1     : 0.4395895519225222, Training Precision      : 0.5091247210063433, Training Sensitivity      : 0.4543543907294151, Training Specificity      : 0.874584761748188\n",
            "Validation Results : \n",
            "Validation Loss   : 2.907271941502889, Validation Accuracy : 0.31527777777777777, Validation G-Mean      : 0.5121517048131936\n",
            "Validation Kappa     : 0.10181651504180159, Validation MF1      : 0.22230054934819535, Validation Precision      : 0.29066587868664,  Validation Sensitivity      : 0.3231799190243084, Validation Specificity      : 0.8217888927018201\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.982274761906377\n",
            "Class wise sensitivity W: 0.2755678983198272, S1: 0.0, S2: 0.5545292242809579, S3: 0.767283954002239, R: 0.018518518518518517\n",
            "Class wise specificity W: 0.9918188320265876, S1: 0.996119929684533, S2: 0.3616567618317074, S3: 0.7951869302325778, R: 0.9641620097336946\n",
            "Class wise F1  W: 0.407249567133409, S1: 0.0, S2: 0.34361006485091317, S3: 0.3365690404618228, R: 0.024074074294832017\n",
            "===========================================================Training Epoch : [31/100] ===========================================================================================================>\n",
            "Epoch: [31/100][0/190]\tTrain_Loss 3.88328 (3.88328)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.62967(0.62967)\tTrain_Kappa 0.38003(0.38003)\tTrain_MF1 0.45104(0.45104)\tTrain_Precision 0.53304(0.53304)\tTrain_Sensitivity 0.45500(0.45500)\tTrain_Specificity 0.87140(0.87140)\tTime 0.139s (0.139s)\tSpeed 230.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [31/100][100/190]\tTrain_Loss 4.06908 (4.96279)\tTrain_Acc 0.65625 (0.59004)\tTrain_G-Mean 0.64942(0.62909)\tTrain_Kappa 0.38137(0.37924)\tTrain_MF1 0.47634(0.44124)\tTrain_Precision 0.48636(0.52152)\tTrain_Sensitivity 0.48789(0.45707)\tTrain_Specificity 0.86442(0.87277)\tTime 0.145s (0.141s)\tSpeed 220.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [31/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.886538764050132, Training Accuracy      : 0.594125939849624, Training G-Mean      : 0.6299768740573892\n",
            "Training Kappa      : 0.38123589634566213,Training MF1     : 0.44162886081557523, Training Precision      : 0.5142420522005935, Training Sensitivity      : 0.4574660149765642, Training Specificity      : 0.8734280077877797\n",
            "Validation Results : \n",
            "Validation Loss   : 2.610748145315382, Validation Accuracy : 0.3403703703703704, Validation G-Mean      : 0.5226645462123403\n",
            "Validation Kappa     : 0.13844828759287822, Validation MF1      : 0.2517534956887917, Validation Precision      : 0.334232315365915,  Validation Sensitivity      : 0.3385691851929381, Validation Specificity      : 0.82932751874129\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.234668819992631\n",
            "Class wise sensitivity W: 0.2777718989937394, S1: 0.0, S2: 0.5807118857348407, S3: 0.7561728402420327, R: 0.07818930099407832\n",
            "Class wise specificity W: 0.9915755192438761, S1: 0.9884346944314463, S2: 0.39618612163596684, S3: 0.8066176772117615, R: 0.9638235811833982\n",
            "Class wise F1  W: 0.4174820129518156, S1: 0.0, S2: 0.36829311704194106, S3: 0.3698855765439846, R: 0.10310677190621693\n",
            "===========================================================Training Epoch : [32/100] ===========================================================================================================>\n",
            "Epoch: [32/100][0/190]\tTrain_Loss 4.09589 (4.09589)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.76452(0.76452)\tTrain_Kappa 0.49059(0.49059)\tTrain_MF1 0.59398(0.59398)\tTrain_Precision 0.75333(0.75333)\tTrain_Sensitivity 0.65333(0.65333)\tTrain_Specificity 0.89464(0.89464)\tTime 0.138s (0.138s)\tSpeed 232.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [32/100][100/190]\tTrain_Loss 5.27681 (4.74257)\tTrain_Acc 0.43750 (0.60520)\tTrain_G-Mean 0.61392(0.63236)\tTrain_Kappa 0.20770(0.40296)\tTrain_MF1 0.33613(0.44939)\tTrain_Precision 0.32917(0.51878)\tTrain_Sensitivity 0.44857(0.45917)\tTrain_Specificity 0.84022(0.87854)\tTime 0.138s (0.141s)\tSpeed 232.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [32/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.994878047390988, Training Accuracy      : 0.600422932330827, Training G-Mean      : 0.6286180233959202\n",
            "Training Kappa      : 0.3852673793873836,Training MF1     : 0.4425712624192239, Training Precision      : 0.5208001795881673, Training Sensitivity      : 0.4561226099494258, Training Specificity      : 0.8743033595618451\n",
            "Validation Results : \n",
            "Validation Loss   : 3.020898041901765, Validation Accuracy : 0.24731481481481482, Validation G-Mean      : 0.4603653242474892\n",
            "Validation Kappa     : 0.03279921882320539, Validation MF1      : 0.16590338183773887, Validation Precision      : 0.2661067111624612,  Validation Sensitivity      : 0.2687580243304924, Validation Specificity      : 0.8068159697232423\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :14.167386372884115\n",
            "Class wise sensitivity W: 0.15400222606129116, S1: 0.0, S2: 0.4294837945037418, S3: 0.7438271619655468, R: 0.016476939121882122\n",
            "Class wise specificity W: 0.9962606827418009, S1: 1.0, S2: 0.34626244063730593, S3: 0.7057069275114272, R: 0.9858497977256775\n",
            "Class wise F1  W: 0.25184232382862654, S1: 0.0, S2: 0.2779817757783113, S3: 0.27059412995974225, R: 0.029098679622014362\n",
            "===========================================================Training Epoch : [33/100] ===========================================================================================================>\n",
            "Epoch: [33/100][0/190]\tTrain_Loss 5.59536 (5.59536)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.56255(0.56255)\tTrain_Kappa 0.28889(0.28889)\tTrain_MF1 0.35476(0.35476)\tTrain_Precision 0.34370(0.34370)\tTrain_Sensitivity 0.36845(0.36845)\tTrain_Specificity 0.85888(0.85888)\tTime 0.138s (0.138s)\tSpeed 232.1 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [33/100][100/190]\tTrain_Loss 4.64920 (5.06825)\tTrain_Acc 0.71875 (0.58787)\tTrain_G-Mean 0.72573(0.61207)\tTrain_Kappa 0.58079(0.36461)\tTrain_MF1 0.58492(0.42688)\tTrain_Precision 0.60833(0.50245)\tTrain_Sensitivity 0.57667(0.43464)\tTrain_Specificity 0.91333(0.87022)\tTime 0.138s (0.140s)\tSpeed 232.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [33/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.943546162153545, Training Accuracy      : 0.5922227443609023, Training G-Mean      : 0.6208525588240269\n",
            "Training Kappa      : 0.37782537249818854,Training MF1     : 0.4339417761877961, Training Precision      : 0.5032904142455054, Training Sensitivity      : 0.44553943738341356, Training Specificity      : 0.8732037175642811\n",
            "Validation Results : \n",
            "Validation Loss   : 2.5851040239687317, Validation Accuracy : 0.2939351851851852, Validation G-Mean      : 0.4938124815078552\n",
            "Validation Kappa     : 0.0868532752927934, Validation MF1      : 0.20678775293959517, Validation Precision      : 0.29325871942219905,  Validation Sensitivity      : 0.3062665353770609, Validation Specificity      : 0.8191922642566539\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :11.653654416402182\n",
            "Class wise sensitivity W: 0.2510171985184705, S1: 0.007407407517786379, S2: 0.4836664456460211, S3: 0.7765432121577086, R: 0.012698413045318038\n",
            "Class wise specificity W: 0.9916485084427727, S1: 0.9961685803201463, S2: 0.3889565578213445, S3: 0.7382040178334272, R: 0.9809836568655791\n",
            "Class wise F1  W: 0.38718055685361225, S1: 0.012345679380275585, S2: 0.3125833209466051, S3: 0.3011739364376775, R: 0.020655271079805162\n",
            "===========================================================Training Epoch : [34/100] ===========================================================================================================>\n",
            "Epoch: [34/100][0/190]\tTrain_Loss 4.91668 (4.91668)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.61299(0.61299)\tTrain_Kappa 0.32984(0.32984)\tTrain_MF1 0.45878(0.45878)\tTrain_Precision 0.67273(0.67273)\tTrain_Sensitivity 0.43492(0.43492)\tTrain_Specificity 0.86398(0.86398)\tTime 0.138s (0.138s)\tSpeed 231.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [34/100][100/190]\tTrain_Loss 3.94562 (5.10878)\tTrain_Acc 0.59375 (0.59344)\tTrain_G-Mean 0.63006(0.62161)\tTrain_Kappa 0.37255(0.37178)\tTrain_MF1 0.43190(0.43352)\tTrain_Precision 0.41444(0.50578)\tTrain_Sensitivity 0.45294(0.44794)\tTrain_Specificity 0.87644(0.87062)\tTime 0.137s (0.143s)\tSpeed 232.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [34/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.013094651071649, Training Accuracy      : 0.5982377819548872, Training G-Mean      : 0.6280355777073782\n",
            "Training Kappa      : 0.38485598490757456,Training MF1     : 0.4416125010032402, Training Precision      : 0.5127761371825873, Training Sensitivity      : 0.45526696879612777, Training Specificity      : 0.8741094028165473\n",
            "Validation Results : \n",
            "Validation Loss   : 2.741103927294413, Validation Accuracy : 0.3005555555555556, Validation G-Mean      : 0.5064804634698675\n",
            "Validation Kappa     : 0.10868189648755967, Validation MF1      : 0.23555521374499358, Validation Precision      : 0.33599180027290615,  Validation Sensitivity      : 0.31960934571645877, Validation Specificity      : 0.8223446945349374\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :12.556339157952202\n",
            "Class wise sensitivity W: 0.2196192741394043, S1: 0.018518518518518517, S2: 0.4829688944198467, S3: 0.6777777793230834, R: 0.19916226218144098\n",
            "Class wise specificity W: 0.9956427017847697, S1: 1.0, S2: 0.48020527208292924, S3: 0.7448827580169395, R: 0.8909927407900492\n",
            "Class wise F1  W: 0.33853497990855463, S1: 0.02469135876055117, S2: 0.33581638556939586, S3: 0.2707438706247895, R: 0.20798947386167668\n",
            "===========================================================Training Epoch : [35/100] ===========================================================================================================>\n",
            "Epoch: [35/100][0/190]\tTrain_Loss 3.81141 (3.81141)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.71149(0.71149)\tTrain_Kappa 0.50631(0.50631)\tTrain_MF1 0.51749(0.51749)\tTrain_Precision 0.57227(0.57227)\tTrain_Sensitivity 0.56429(0.56429)\tTrain_Specificity 0.89709(0.89709)\tTime 0.140s (0.140s)\tSpeed 229.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [35/100][100/190]\tTrain_Loss 4.92452 (4.82843)\tTrain_Acc 0.43750 (0.59932)\tTrain_G-Mean 0.56218(0.62250)\tTrain_Kappa 0.21526(0.38690)\tTrain_MF1 0.35098(0.43762)\tTrain_Precision 0.56333(0.51446)\tTrain_Sensitivity 0.37532(0.44689)\tTrain_Specificity 0.84206(0.87480)\tTime 0.139s (0.140s)\tSpeed 230.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [35/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.83315537352311, Training Accuracy      : 0.5973684210526315, Training G-Mean      : 0.6266309623845773\n",
            "Training Kappa      : 0.3835540565293882,Training MF1     : 0.44172998346780473, Training Precision      : 0.5087299851210492, Training Sensitivity      : 0.45333126568480536, Training Specificity      : 0.8739717994238204\n",
            "Validation Results : \n",
            "Validation Loss   : 2.525926219092475, Validation Accuracy : 0.3147685185185185, Validation G-Mean      : 0.4971901822431743\n",
            "Validation Kappa     : 0.10198782592476895, Validation MF1      : 0.22084659238656357, Validation Precision      : 0.2883304284678565,  Validation Sensitivity      : 0.3043757921015775, Validation Specificity      : 0.8227178478682482\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :11.422708334746185\n",
            "Class wise sensitivity W: 0.29122513753396495, S1: 0.0219576723045773, S2: 0.4843414894960545, S3: 0.7117283984466836, R: 0.012626262726607147\n",
            "Class wise specificity W: 0.9842352558065344, S1: 0.9846327084082144, S2: 0.381438199016783, S3: 0.7802734595757944, R: 0.9830096165339152\n",
            "Class wise F1  W: 0.41927530864874524, S1: 0.033627278826854845, S2: 0.31783527466985917, S3: 0.3129822788415132, R: 0.02051282094584571\n",
            "===========================================================Training Epoch : [36/100] ===========================================================================================================>\n",
            "Epoch: [36/100][0/190]\tTrain_Loss 2.82149 (2.82149)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.67689(0.67689)\tTrain_Kappa 0.50995(0.50995)\tTrain_MF1 0.49736(0.49736)\tTrain_Precision 0.52070(0.52070)\tTrain_Sensitivity 0.50833(0.50833)\tTrain_Specificity 0.90133(0.90133)\tTime 0.138s (0.138s)\tSpeed 231.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [36/100][100/190]\tTrain_Loss 4.10782 (4.60771)\tTrain_Acc 0.46875 (0.60891)\tTrain_G-Mean 0.56724(0.63232)\tTrain_Kappa 0.30346(0.39837)\tTrain_MF1 0.39254(0.45162)\tTrain_Precision 0.51235(0.52731)\tTrain_Sensitivity 0.37341(0.45910)\tTrain_Specificity 0.86169(0.87695)\tTime 0.137s (0.140s)\tSpeed 233.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [36/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.695055347994754, Training Accuracy      : 0.6082706766917293, Training G-Mean      : 0.634989892113341\n",
            "Training Kappa      : 0.4005106061949945,Training MF1     : 0.45087888008669824, Training Precision      : 0.521675078555157, Training Sensitivity      : 0.4631456344770755, Training Specificity      : 0.8776448127156807\n",
            "Validation Results : \n",
            "Validation Loss   : 3.3799593448638916, Validation Accuracy : 0.22351851851851853, Validation G-Mean      : 0.4659313557965095\n",
            "Validation Kappa     : 0.033835663380613765, Validation MF1      : 0.1637363996218752, Validation Precision      : 0.2765631077466187,  Validation Sensitivity      : 0.27433463864856295, Validation Specificity      : 0.8067307109082186\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :16.17666827307807\n",
            "Class wise sensitivity W: 0.1938400114024127, S1: 0.0, S2: 0.2942499937834563, S3: 0.850617285127993, R: 0.032965902928952816\n",
            "Class wise specificity W: 0.9946767091751099, S1: 1.0, S2: 0.4485779034870642, S3: 0.6115322918803604, R: 0.9788666499985589\n",
            "Class wise F1  W: 0.30758204835432545, S1: 0.0, S2: 0.2112840505109893, S3: 0.2491050864811297, R: 0.05071081276293154\n",
            "===========================================================Training Epoch : [37/100] ===========================================================================================================>\n",
            "Epoch: [37/100][0/190]\tTrain_Loss 5.03387 (5.03387)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.61673(0.61673)\tTrain_Kappa 0.38643(0.38643)\tTrain_MF1 0.40577(0.40577)\tTrain_Precision 0.43333(0.43333)\tTrain_Sensitivity 0.43333(0.43333)\tTrain_Specificity 0.87775(0.87775)\tTime 0.139s (0.139s)\tSpeed 229.7 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [37/100][100/190]\tTrain_Loss 3.98914 (4.68635)\tTrain_Acc 0.65625 (0.61572)\tTrain_G-Mean 0.63409(0.63671)\tTrain_Kappa 0.42010(0.41136)\tTrain_MF1 0.47737(0.45676)\tTrain_Precision 0.51000(0.52168)\tTrain_Sensitivity 0.45794(0.46421)\tTrain_Specificity 0.87800(0.87962)\tTime 0.140s (0.140s)\tSpeed 228.9 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [37/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.844423059413308, Training Accuracy      : 0.6006578947368421, Training G-Mean      : 0.6297510856181381\n",
            "Training Kappa      : 0.39161708968061887,Training MF1     : 0.44285198232060996, Training Precision      : 0.511272837237308, Training Sensitivity      : 0.4561078803790245, Training Specificity      : 0.8756358265563051\n",
            "Validation Results : \n",
            "Validation Loss   : 3.3316594583016856, Validation Accuracy : 0.23689814814814816, Validation G-Mean      : 0.470025673449335\n",
            "Validation Kappa     : 0.05556910231491702, Validation MF1      : 0.16941064132584463, Validation Precision      : 0.25691697956235326,  Validation Sensitivity      : 0.2771638346491037, Validation Specificity      : 0.8128461089399126\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :16.01085786466245\n",
            "Class wise sensitivity W: 0.25983856508025416, S1: 0.0, S2: 0.26538390031567327, S3: 0.8425925925925926, R: 0.018004115256998274\n",
            "Class wise specificity W: 0.9881475567817688, S1: 0.9973936897737009, S2: 0.5203175655117741, S3: 0.5654536916149987, R: 0.9929180410173204\n",
            "Class wise F1  W: 0.39187312346917613, S1: 0.0, S2: 0.2072645765211847, S3: 0.21746283134928457, R: 0.03045267528957791\n",
            "===========================================================Training Epoch : [38/100] ===========================================================================================================>\n",
            "Epoch: [38/100][0/190]\tTrain_Loss 5.81706 (5.81706)\tTrain_Acc 0.40625 (0.40625)\tTrain_G-Mean 0.57562(0.57562)\tTrain_Kappa 0.25031(0.25031)\tTrain_MF1 0.34384(0.34384)\tTrain_Precision 0.61087(0.61087)\tTrain_Sensitivity 0.38944(0.38944)\tTrain_Specificity 0.85080(0.85080)\tTime 0.141s (0.141s)\tSpeed 227.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [38/100][100/190]\tTrain_Loss 5.53772 (5.26450)\tTrain_Acc 0.59375 (0.56962)\tTrain_G-Mean 0.69021(0.60749)\tTrain_Kappa 0.48065(0.33998)\tTrain_MF1 0.52654(0.41668)\tTrain_Precision 0.62500(0.49045)\tTrain_Sensitivity 0.53056(0.43095)\tTrain_Specificity 0.89790(0.86487)\tTime 0.137s (0.140s)\tSpeed 233.2 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [38/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.172609780964098, Training Accuracy      : 0.5866071428571429, Training G-Mean      : 0.6163265917356029\n",
            "Training Kappa      : 0.3618938324111476,Training MF1     : 0.4293739094545967, Training Precision      : 0.5002563154540561, Training Sensitivity      : 0.4415780243120697, Training Specificity      : 0.8696138447993688\n",
            "Validation Results : \n",
            "Validation Loss   : 2.34885863021568, Validation Accuracy : 0.2828240740740741, Validation G-Mean      : 0.5070294924722705\n",
            "Validation Kappa     : 0.08648731016868111, Validation MF1      : 0.2229358436332808, Validation Precision      : 0.33611218487774885,  Validation Sensitivity      : 0.32150921374559405, Validation Specificity      : 0.8179753707514869\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :10.598560615822121\n",
            "Class wise sensitivity W: 0.2828119471669197, S1: 0.009259259259259259, S2: 0.379187381653874, S3: 0.8555555564385874, R: 0.08073192420932981\n",
            "Class wise specificity W: 0.9946035124637462, S1: 0.962583358641024, S2: 0.41820226664896365, S3: 0.7462320504365144, R: 0.9682556655671861\n",
            "Class wise F1  W: 0.415614554727519, S1: 0.012345679380275585, S2: 0.2572332573709665, S3: 0.3111649503310521, R: 0.11832077635659112\n",
            "===========================================================Training Epoch : [39/100] ===========================================================================================================>\n",
            "Epoch: [39/100][0/190]\tTrain_Loss 4.24131 (4.24131)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.55246(0.55246)\tTrain_Kappa 0.30029(0.30029)\tTrain_MF1 0.37063(0.37063)\tTrain_Precision 0.60571(0.60571)\tTrain_Sensitivity 0.35476(0.35476)\tTrain_Specificity 0.86034(0.86034)\tTime 0.142s (0.142s)\tSpeed 225.7 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [39/100][100/190]\tTrain_Loss 4.14876 (4.84954)\tTrain_Acc 0.68750 (0.60179)\tTrain_G-Mean 0.69253(0.62143)\tTrain_Kappa 0.54481(0.38835)\tTrain_MF1 0.53051(0.43911)\tTrain_Precision 0.57941(0.51294)\tTrain_Sensitivity 0.53048(0.44427)\tTrain_Specificity 0.90409(0.87479)\tTime 0.138s (0.140s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [39/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.862099277345758, Training Accuracy      : 0.5972039473684211, Training G-Mean      : 0.6223518066212638\n",
            "Training Kappa      : 0.3808164693410203,Training MF1     : 0.4379146655452878, Training Precision      : 0.5081859107393967, Training Sensitivity      : 0.44685743556995133, Training Specificity      : 0.8736087930986753\n",
            "Validation Results : \n",
            "Validation Loss   : 2.7329488860236273, Validation Accuracy : 0.24287037037037038, Validation G-Mean      : 0.4782748352736905\n",
            "Validation Kappa     : 0.04762317403999052, Validation MF1      : 0.1967124868322302, Validation Precision      : 0.2946048534026853,  Validation Sensitivity      : 0.2890004158571915, Validation Specificity      : 0.8099153439203897\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.714970023543746\n",
            "Class wise sensitivity W: 0.22418564447650202, S1: 0.018518518518518517, S2: 0.29187757135541353, S3: 0.805555557763135, R: 0.10486478717238815\n",
            "Class wise specificity W: 0.9963844793814199, S1: 0.992081966665056, S2: 0.45755858222643536, S3: 0.6706366903252072, R: 0.9329150010038305\n",
            "Class wise F1  W: 0.34759730762905544, S1: 0.018518518518518517, S2: 0.20790958459730502, S3: 0.27499584080996337, R: 0.13454118260630854\n",
            "===========================================================Training Epoch : [40/100] ===========================================================================================================>\n",
            "Epoch: [40/100][0/190]\tTrain_Loss 6.09922 (6.09922)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.64509(0.64509)\tTrain_Kappa 0.42256(0.42256)\tTrain_MF1 0.50048(0.50048)\tTrain_Precision 0.63000(0.63000)\tTrain_Sensitivity 0.47429(0.47429)\tTrain_Specificity 0.87741(0.87741)\tTime 0.139s (0.139s)\tSpeed 231.0 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [40/100][100/190]\tTrain_Loss 5.03394 (4.85762)\tTrain_Acc 0.65625 (0.61046)\tTrain_G-Mean 0.70703(0.62401)\tTrain_Kappa 0.49206(0.39683)\tTrain_MF1 0.53571(0.44319)\tTrain_Precision 0.53546(0.51856)\tTrain_Sensitivity 0.55762(0.44806)\tTrain_Specificity 0.89649(0.87590)\tTime 0.139s (0.140s)\tSpeed 230.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [40/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.912754646100496, Training Accuracy      : 0.6042293233082707, Training G-Mean      : 0.6226140779757289\n",
            "Training Kappa      : 0.3898100281772348,Training MF1     : 0.4405487001255935, Training Precision      : 0.5118377942787976, Training Sensitivity      : 0.44663987072674843, Training Specificity      : 0.874844955268659\n",
            "Validation Results : \n",
            "Validation Loss   : 3.0187613434261746, Validation Accuracy : 0.2114351851851852, Validation G-Mean      : 0.4431782201943637\n",
            "Validation Kappa     : 0.010428636669330825, Validation MF1      : 0.15658065969193424, Validation Precision      : 0.24843273331169724,  Validation Sensitivity      : 0.2578048402512515, Validation Specificity      : 0.8018646878224833\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :14.282133279023347\n",
            "Class wise sensitivity W: 0.2353279941059925, S1: 0.0, S2: 0.2215974399888957, S3: 0.824691359643583, R: 0.007407407517786379\n",
            "Class wise specificity W: 0.9937296951258624, S1: 0.9986772493079856, S2: 0.4086407345754129, S3: 0.6113621813279612, R: 0.9969135787751939\n",
            "Class wise F1  W: 0.3634981910387675, S1: 0.0, S2: 0.16150343914826712, S3: 0.24555598889236097, R: 0.012345679380275585\n",
            "===========================================================Training Epoch : [41/100] ===========================================================================================================>\n",
            "Epoch: [41/100][0/190]\tTrain_Loss 5.32693 (5.32693)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.51607(0.51607)\tTrain_Kappa 0.11401(0.11401)\tTrain_MF1 0.29803(0.29803)\tTrain_Precision 0.54231(0.54231)\tTrain_Sensitivity 0.32667(0.32667)\tTrain_Specificity 0.81529(0.81529)\tTime 0.144s (0.144s)\tSpeed 222.8 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [41/100][100/190]\tTrain_Loss 6.98296 (4.86503)\tTrain_Acc 0.50000 (0.60149)\tTrain_G-Mean 0.46010(0.62253)\tTrain_Kappa 0.13073(0.38753)\tTrain_MF1 0.25368(0.43794)\tTrain_Precision 0.39630(0.51077)\tTrain_Sensitivity 0.25778(0.44706)\tTrain_Specificity 0.82122(0.87464)\tTime 0.138s (0.140s)\tSpeed 232.2 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [41/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.876415390717356, Training Accuracy      : 0.6028195488721805, Training G-Mean      : 0.6311338388045287\n",
            "Training Kappa      : 0.3937227055603453,Training MF1     : 0.4472870134993604, Training Precision      : 0.5173555305757016, Training Sensitivity      : 0.4587191278448229, Training Specificity      : 0.8758676886872244\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8853688681567156, Validation Accuracy : 0.25407407407407406, Validation G-Mean      : 0.4697865783037529\n",
            "Validation Kappa     : 0.04415877927211538, Validation MF1      : 0.18231480728696892, Validation Precision      : 0.2642148592681796,  Validation Sensitivity      : 0.27942383007870775, Validation Specificity      : 0.8091789402343609\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :13.318602032131619\n",
            "Class wise sensitivity W: 0.24813859468256985, S1: 0.007407407517786379, S2: 0.36811635615649047, S3: 0.7734567920366923, R: 0.0\n",
            "Class wise specificity W: 0.9877866197515417, S1: 0.9886552890141805, S2: 0.34913051901040254, S3: 0.7217467758390639, R: 0.9985754975566158\n",
            "Class wise F1  W: 0.3727226930635947, S1: 0.012345679380275585, S2: 0.23968003083158423, S3: 0.2868256331593902, R: 0.0\n",
            "===========================================================Training Epoch : [42/100] ===========================================================================================================>\n",
            "Epoch: [42/100][0/190]\tTrain_Loss 6.16278 (6.16278)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.53766(0.53766)\tTrain_Kappa 0.25172(0.25172)\tTrain_MF1 0.35354(0.35354)\tTrain_Precision 0.60182(0.60182)\tTrain_Sensitivity 0.34102(0.34102)\tTrain_Specificity 0.84769(0.84769)\tTime 0.139s (0.139s)\tSpeed 230.2 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [42/100][100/190]\tTrain_Loss 5.23268 (4.89655)\tTrain_Acc 0.50000 (0.58230)\tTrain_G-Mean 0.50811(0.60920)\tTrain_Kappa 0.21231(0.35543)\tTrain_MF1 0.30351(0.41953)\tTrain_Precision 0.37971(0.49114)\tTrain_Sensitivity 0.30667(0.43253)\tTrain_Specificity 0.84189(0.86842)\tTime 0.138s (0.140s)\tSpeed 232.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [42/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.86787423711074, Training Accuracy      : 0.5909304511278195, Training G-Mean      : 0.6177009741485083\n",
            "Training Kappa      : 0.3735728094228052,Training MF1     : 0.43081310375740667, Training Precision      : 0.5008947460196523, Training Sensitivity      : 0.44225176127333404, Training Specificity      : 0.8723008192212959\n",
            "Validation Results : \n",
            "Validation Loss   : 3.060879910433734, Validation Accuracy : 0.2461574074074074, Validation G-Mean      : 0.4779299573762203\n",
            "Validation Kappa     : 0.060857010600165885, Validation MF1      : 0.19507341053750782, Validation Precision      : 0.29689374240460215,  Validation Sensitivity      : 0.2918431704243024, Validation Specificity      : 0.8136901355452006\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :14.752160001684118\n",
            "Class wise sensitivity W: 0.25293714304765064, S1: 0.018518518518518517, S2: 0.275971668737906, S3: 0.8487654328346252, R: 0.06302308898281168\n",
            "Class wise specificity W: 0.9936583130447952, S1: 0.971065119460777, S2: 0.5006712647499861, S3: 0.6524217702724315, R: 0.9506342101980139\n",
            "Class wise F1  W: 0.3737937123687179, S1: 0.02469135876055117, S2: 0.20979435686711911, S3: 0.26923074656062657, R: 0.09785687813052425\n",
            "===========================================================Training Epoch : [43/100] ===========================================================================================================>\n",
            "Epoch: [43/100][0/190]\tTrain_Loss 3.35602 (3.35602)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.61451(0.61451)\tTrain_Kappa 0.35657(0.35657)\tTrain_MF1 0.42675(0.42675)\tTrain_Precision 0.44583(0.44583)\tTrain_Sensitivity 0.43381(0.43381)\tTrain_Specificity 0.87047(0.87047)\tTime 0.141s (0.141s)\tSpeed 227.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [43/100][100/190]\tTrain_Loss 4.46200 (4.76381)\tTrain_Acc 0.62500 (0.60272)\tTrain_G-Mean 0.73874(0.62952)\tTrain_Kappa 0.47253(0.39815)\tTrain_MF1 0.51736(0.44848)\tTrain_Precision 0.48179(0.52194)\tTrain_Sensitivity 0.61048(0.45573)\tTrain_Specificity 0.89396(0.87722)\tTime 0.139s (0.140s)\tSpeed 231.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [43/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.816378693831594, Training Accuracy      : 0.5988956766917293, Training G-Mean      : 0.6262231066261806\n",
            "Training Kappa      : 0.382923695716426,Training MF1     : 0.44657329096605913, Training Precision      : 0.5227686344636115, Training Sensitivity      : 0.4532928058583484, Training Specificity      : 0.873529796678769\n",
            "Validation Results : \n",
            "Validation Loss   : 2.3593442925700434, Validation Accuracy : 0.35444444444444445, Validation G-Mean      : 0.542170781464624\n",
            "Validation Kappa     : 0.1399335185540162, Validation MF1      : 0.26330963119312567, Validation Precision      : 0.32916794215087536,  Validation Sensitivity      : 0.35976775691465096, Validation Specificity      : 0.8291061437792251\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :10.231255390025952\n",
            "Class wise sensitivity W: 0.2972955607153751, S1: 0.07530864245361751, S2: 0.6527777877118852, S3: 0.7537037067943149, R: 0.019753086898061965\n",
            "Class wise specificity W: 0.9804924660258822, S1: 0.9723050483950862, S2: 0.333805239310971, S3: 0.8671306879432114, R: 0.9917972772209732\n",
            "Class wise F1  W: 0.4350971872055972, S1: 0.07169312349072209, S2: 0.38449667670108656, S3: 0.39616063899464077, R: 0.029100529573581838\n",
            "===========================================================Training Epoch : [44/100] ===========================================================================================================>\n",
            "Epoch: [44/100][0/190]\tTrain_Loss 5.79517 (5.79517)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.53762(0.53762)\tTrain_Kappa 0.23596(0.23596)\tTrain_MF1 0.30121(0.30121)\tTrain_Precision 0.31190(0.31190)\tTrain_Sensitivity 0.34167(0.34167)\tTrain_Specificity 0.84595(0.84595)\tTime 0.138s (0.138s)\tSpeed 231.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [44/100][100/190]\tTrain_Loss 5.07722 (4.99475)\tTrain_Acc 0.56250 (0.58292)\tTrain_G-Mean 0.60148(0.61021)\tTrain_Kappa 0.34884(0.35757)\tTrain_MF1 0.40317(0.41488)\tTrain_Precision 0.53909(0.48631)\tTrain_Sensitivity 0.41795(0.43243)\tTrain_Specificity 0.86562(0.86917)\tTime 0.137s (0.140s)\tSpeed 233.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [44/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.097558045387268, Training Accuracy      : 0.587453007518797, Training G-Mean      : 0.6113333335115471\n",
            "Training Kappa      : 0.3628356802925419,Training MF1     : 0.420362032855812, Training Precision      : 0.49485393850426884, Training Sensitivity      : 0.43407429861394986, Training Specificity      : 0.8699900520161579\n",
            "Validation Results : \n",
            "Validation Loss   : 3.9232178617406777, Validation Accuracy : 0.1714351851851852, Validation G-Mean      : 0.4199676856638979\n",
            "Validation Kappa     : -0.00423055476716369, Validation MF1      : 0.120864684217506, Validation Precision      : 0.23343651402327753,  Validation Sensitivity      : 0.22986869723708545, Validation Specificity      : 0.7988112842595136\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :18.445558371367277\n",
            "Class wise sensitivity W: 0.17710481584072113, S1: 0.0, S2: 0.13828805226970603, S3: 0.8302469143161068, R: 0.0037037037588931896\n",
            "Class wise specificity W: 0.9926513808744925, S1: 0.995016407083582, S2: 0.5244573884540134, S3: 0.4905036555396186, R: 0.9914275893458614\n",
            "Class wise F1  W: 0.28501435065710984, S1: 0.0, S2: 0.11827817448863277, S3: 0.19429688900709152, R: 0.006734006934695774\n",
            "===========================================================Training Epoch : [45/100] ===========================================================================================================>\n",
            "Epoch: [45/100][0/190]\tTrain_Loss 7.52747 (7.52747)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.58708(0.58708)\tTrain_Kappa 0.34311(0.34311)\tTrain_MF1 0.42828(0.42828)\tTrain_Precision 0.62500(0.62500)\tTrain_Sensitivity 0.40000(0.40000)\tTrain_Specificity 0.86167(0.86167)\tTime 0.138s (0.138s)\tSpeed 231.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [45/100][100/190]\tTrain_Loss 5.12258 (4.87081)\tTrain_Acc 0.56250 (0.59746)\tTrain_G-Mean 0.60409(0.63346)\tTrain_Kappa 0.26316(0.39058)\tTrain_MF1 0.36857(0.45254)\tTrain_Precision 0.32174(0.51617)\tTrain_Sensitivity 0.43137(0.46252)\tTrain_Specificity 0.84596(0.87602)\tTime 0.144s (0.141s)\tSpeed 222.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [45/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.781913812536942, Training Accuracy      : 0.6022321428571429, Training G-Mean      : 0.6345466207778796\n",
            "Training Kappa      : 0.3942585420937454,Training MF1     : 0.45427615230020707, Training Precision      : 0.5213787344568658, Training Sensitivity      : 0.46415604820376966, Training Specificity      : 0.8764677773572903\n",
            "Validation Results : \n",
            "Validation Loss   : 4.0262224585921675, Validation Accuracy : 0.21439814814814817, Validation G-Mean      : 0.45421335230678506\n",
            "Validation Kappa     : 0.031569175114671416, Validation MF1      : 0.15153863430023196, Validation Precision      : 0.2392411587138971,  Validation Sensitivity      : 0.2666954564275565, Validation Specificity      : 0.8065802265096593\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :19.415125175758643\n",
            "Class wise sensitivity W: 0.22521969317286103, S1: 0.0, S2: 0.27029462600195847, S3: 0.8333333333333334, R: 0.004629629629629629\n",
            "Class wise specificity W: 0.9908429516686333, S1: 0.9972527468646014, S2: 0.47595111197895473, S3: 0.5746326126434185, R: 0.994221709392689\n",
            "Class wise F1  W: 0.34367674313209673, S1: 0.0, S2: 0.18506213084415155, S3: 0.2207238447886926, R: 0.008230452736218771\n",
            "===========================================================Training Epoch : [46/100] ===========================================================================================================>\n",
            "Epoch: [46/100][0/190]\tTrain_Loss 4.82858 (4.82858)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.53652(0.53652)\tTrain_Kappa 0.26753(0.26753)\tTrain_MF1 0.36012(0.36012)\tTrain_Precision 0.42807(0.42807)\tTrain_Sensitivity 0.33846(0.33846)\tTrain_Specificity 0.85048(0.85048)\tTime 0.137s (0.137s)\tSpeed 233.2 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [46/100][100/190]\tTrain_Loss 4.23245 (4.86061)\tTrain_Acc 0.56250 (0.59994)\tTrain_G-Mean 0.60127(0.63397)\tTrain_Kappa 0.36544(0.39367)\tTrain_MF1 0.41455(0.44870)\tTrain_Precision 0.50909(0.51463)\tTrain_Sensitivity 0.41500(0.46218)\tTrain_Specificity 0.87115(0.87564)\tTime 0.145s (0.140s)\tSpeed 220.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [46/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.046667602187709, Training Accuracy      : 0.5972039473684211, Training G-Mean      : 0.6250729445356217\n",
            "Training Kappa      : 0.38060267238179474,Training MF1     : 0.4372459568632277, Training Precision      : 0.5053646409982129, Training Sensitivity      : 0.45131770875893135, Training Specificity      : 0.8730755151965112\n",
            "Validation Results : \n",
            "Validation Loss   : 3.588279300265842, Validation Accuracy : 0.2683333333333333, Validation G-Mean      : 0.47766664824983035\n",
            "Validation Kappa     : 0.10487260392569178, Validation MF1      : 0.20346109905728585, Validation Precision      : 0.30179526204312296,  Validation Sensitivity      : 0.28637634267409634, Validation Specificity      : 0.825127038470021\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :16.983768851668746\n",
            "Class wise sensitivity W: 0.4053331533515895, S1: 0.0, S2: 0.1717131667666965, S3: 0.7879188723034329, R: 0.06691652094876324\n",
            "Class wise specificity W: 0.9809137450324165, S1: 0.9914148538200943, S2: 0.6494117180506388, S3: 0.5136174371948948, R: 0.9902774382520605\n",
            "Class wise F1  W: 0.54857997154748, S1: 0.0, S2: 0.16165512275916558, S3: 0.19692396069014514, R: 0.11014644028963866\n",
            "===========================================================Training Epoch : [47/100] ===========================================================================================================>\n",
            "Epoch: [47/100][0/190]\tTrain_Loss 3.90334 (3.90334)\tTrain_Acc 0.75000 (0.75000)\tTrain_G-Mean 0.69703(0.69703)\tTrain_Kappa 0.60062(0.60062)\tTrain_MF1 0.53879(0.53879)\tTrain_Precision 0.57000(0.57000)\tTrain_Sensitivity 0.53083(0.53083)\tTrain_Specificity 0.91526(0.91526)\tTime 0.143s (0.143s)\tSpeed 224.0 samples/s\tData 0.006s (0.006s)\t\n",
            "Epoch: [47/100][100/190]\tTrain_Loss 4.38619 (4.95066)\tTrain_Acc 0.75000 (0.58447)\tTrain_G-Mean 0.75967(0.61274)\tTrain_Kappa 0.54610(0.36279)\tTrain_MF1 0.65919(0.42000)\tTrain_Precision 0.69524(0.48453)\tTrain_Sensitivity 0.63667(0.43564)\tTrain_Specificity 0.90644(0.87131)\tTime 0.139s (0.141s)\tSpeed 230.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [47/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.973292421039782, Training Accuracy      : 0.5887687969924812, Training G-Mean      : 0.617238049260127\n",
            "Training Kappa      : 0.37159078577258026,Training MF1     : 0.4290060847683957, Training Precision      : 0.4946329548641257, Training Sensitivity      : 0.44055625658286274, Training Specificity      : 0.8724061425892928\n",
            "Validation Results : \n",
            "Validation Loss   : 3.3398681305072926, Validation Accuracy : 0.21310185185185185, Validation G-Mean      : 0.4530283373158985\n",
            "Validation Kappa     : 0.021313750438237872, Validation MF1      : 0.1584864525883286, Validation Precision      : 0.25356165245175355,  Validation Sensitivity      : 0.2636741378241115, Validation Specificity      : 0.8037082321114012\n",
            "Validation T Acc     : 0.8538888888888889, Val_KD_Loss :15.701964343035662\n",
            "Class wise sensitivity W: 0.2045160170506548, S1: 0.006172839690137793, S2: 0.2928670159092656, S3: 0.8086419767803616, R: 0.006172839690137793\n",
            "Class wise specificity W: 0.9923050381519176, S1: 0.9921665633166278, S2: 0.4185873093428435, S3: 0.6202093462149302, R: 0.995272903530686\n",
            "Class wise F1  W: 0.320786221159829, S1: 0.010582011055063319, S2: 0.19661307141736703, S3: 0.2538689482543204, R: 0.010582011055063319\n",
            "===========================================================Training Epoch : [48/100] ===========================================================================================================>\n",
            "Epoch: [48/100][0/190]\tTrain_Loss 2.83858 (2.83858)\tTrain_Acc 0.84375 (0.84375)\tTrain_G-Mean 0.74119(0.74119)\tTrain_Kappa 0.72603(0.72603)\tTrain_MF1 0.63556(0.63556)\tTrain_Precision 0.76364(0.76364)\tTrain_Sensitivity 0.58667(0.58667)\tTrain_Specificity 0.93641(0.93641)\tTime 0.139s (0.139s)\tSpeed 230.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [48/100][100/190]\tTrain_Loss 4.72265 (5.01745)\tTrain_Acc 0.37500 (0.59530)\tTrain_G-Mean 0.45386(0.61999)\tTrain_Kappa 0.06433(0.38246)\tTrain_MF1 0.19111(0.43248)\tTrain_Precision 0.15362(0.50034)\tTrain_Sensitivity 0.25385(0.44321)\tTrain_Specificity 0.81147(0.87385)\tTime 0.138s (0.140s)\tSpeed 231.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [48/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.024881896219756, Training Accuracy      : 0.5955357142857143, Training G-Mean      : 0.6191141881235628\n",
            "Training Kappa      : 0.3774095359362607,Training MF1     : 0.43257074829779185, Training Precision      : 0.4974304108870658, Training Sensitivity      : 0.4435829946634018, Training Specificity      : 0.8724819218484979\n",
            "Validation Results : \n",
            "Validation Loss   : 3.576709341119837, Validation Accuracy : 0.24532407407407408, Validation G-Mean      : 0.45415316022796604\n",
            "Validation Kappa     : 0.0578643280889831, Validation MF1      : 0.17317970615846137, Validation Precision      : 0.24723415148478967,  Validation Sensitivity      : 0.26508289834967363, Validation Specificity      : 0.8134051579016227\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :17.114847077263725\n",
            "Class wise sensitivity W: 0.30980201175919286, S1: 0.0, S2: 0.2625260587643694, S3: 0.7530864212248061, R: 0.0\n",
            "Class wise specificity W: 0.9860709662790652, S1: 1.0, S2: 0.48938267208911757, S3: 0.5943683948781755, R: 0.997203756261755\n",
            "Class wise F1  W: 0.4428152745520627, S1: 0.0, S2: 0.18681667579544914, S3: 0.23626658044479512, R: 0.0\n",
            "===========================================================Training Epoch : [49/100] ===========================================================================================================>\n",
            "Epoch: [49/100][0/190]\tTrain_Loss 3.64823 (3.64823)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.66213(0.66213)\tTrain_Kappa 0.35908(0.35908)\tTrain_MF1 0.44306(0.44306)\tTrain_Precision 0.40941(0.40941)\tTrain_Sensitivity 0.50381(0.50381)\tTrain_Specificity 0.87019(0.87019)\tTime 0.138s (0.138s)\tSpeed 231.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [49/100][100/190]\tTrain_Loss 3.00427 (4.89543)\tTrain_Acc 0.59375 (0.60087)\tTrain_G-Mean 0.67180(0.62554)\tTrain_Kappa 0.34281(0.38879)\tTrain_MF1 0.43626(0.44052)\tTrain_Precision 0.49394(0.50621)\tTrain_Sensitivity 0.52167(0.45067)\tTrain_Specificity 0.86514(0.87504)\tTime 0.138s (0.140s)\tSpeed 231.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [49/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.864137530326843, Training Accuracy      : 0.5977678571428572, Training G-Mean      : 0.6256649003705199\n",
            "Training Kappa      : 0.3861432339159312,Training MF1     : 0.43849198788404453, Training Precision      : 0.5095156800433212, Training Sensitivity      : 0.45143690212776777, Training Specificity      : 0.8749769983009289\n",
            "Validation Results : \n",
            "Validation Loss   : 4.091514128225821, Validation Accuracy : 0.21458333333333335, Validation G-Mean      : 0.4583469265632833\n",
            "Validation Kappa     : 0.0202098953735069, Validation MF1      : 0.14431035391710423, Validation Precision      : 0.2266073991027143,  Validation Sensitivity      : 0.26555480156783706, Validation Specificity      : 0.8046501497427622\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :19.777067961516202\n",
            "Class wise sensitivity W: 0.15999157809548908, S1: 0.0, S2: 0.32827625616833017, S3: 0.8395061735753659, R: 0.0\n",
            "Class wise specificity W: 0.9963320802759241, S1: 1.0, S2: 0.4240507008852782, S3: 0.604349449828819, R: 0.99851851772379\n",
            "Class wise F1  W: 0.2594889716969596, S1: 0.0, S2: 0.23652958207660252, S3: 0.225533215811959, R: 0.0\n",
            "===========================================================Training Epoch : [50/100] ===========================================================================================================>\n",
            "Epoch: [50/100][0/190]\tTrain_Loss 4.61707 (4.61707)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.64736(0.64736)\tTrain_Kappa 0.45763(0.45763)\tTrain_MF1 0.47143(0.47143)\tTrain_Precision 0.47970(0.47970)\tTrain_Sensitivity 0.47202(0.47202)\tTrain_Specificity 0.88782(0.88782)\tTime 0.139s (0.139s)\tSpeed 230.6 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [50/100][100/190]\tTrain_Loss 6.51471 (4.73146)\tTrain_Acc 0.56250 (0.60613)\tTrain_G-Mean 0.58767(0.63410)\tTrain_Kappa 0.33630(0.39996)\tTrain_MF1 0.34825(0.45492)\tTrain_Precision 0.35714(0.52151)\tTrain_Sensitivity 0.40000(0.46269)\tTrain_Specificity 0.86338(0.87719)\tTime 0.137s (0.140s)\tSpeed 232.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [50/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.710200878193504, Training Accuracy      : 0.6079652255639098, Training G-Mean      : 0.6345776864307564\n",
            "Training Kappa      : 0.4017430811122193,Training MF1     : 0.45127637820808486, Training Precision      : 0.5179610125485217, Training Sensitivity      : 0.46323187233586066, Training Specificity      : 0.8779601304938918\n",
            "Validation Results : \n",
            "Validation Loss   : 3.3326362857112177, Validation Accuracy : 0.29097222222222224, Validation G-Mean      : 0.4979004151479064\n",
            "Validation Kappa     : 0.08948923015899785, Validation MF1      : 0.19705804095224094, Validation Precision      : 0.26679318434110394,  Validation Sensitivity      : 0.30593010192668, Validation Specificity      : 0.8203685839970908\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :15.418388613948116\n",
            "Class wise sensitivity W: 0.2530995797779825, S1: 0.0, S2: 0.4542846116754744, S3: 0.8061728433326438, R: 0.01609347484729908\n",
            "Class wise specificity W: 0.9950401429776792, S1: 1.0, S2: 0.4148063593440586, S3: 0.7052639568293536, R: 0.9867324608343618\n",
            "Class wise F1  W: 0.3757656028977147, S1: 0.0, S2: 0.31455539139332594, S3: 0.2690432844338594, R: 0.0259259260363049\n",
            "===========================================================Training Epoch : [51/100] ===========================================================================================================>\n",
            "Epoch: [51/100][0/190]\tTrain_Loss 4.25340 (4.25340)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.65923(0.65923)\tTrain_Kappa 0.34281(0.34281)\tTrain_MF1 0.42692(0.42692)\tTrain_Precision 0.56200(0.56200)\tTrain_Sensitivity 0.50357(0.50357)\tTrain_Specificity 0.86299(0.86299)\tTime 0.138s (0.138s)\tSpeed 231.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [51/100][100/190]\tTrain_Loss 4.42146 (4.66388)\tTrain_Acc 0.53125 (0.61757)\tTrain_G-Mean 0.65740(0.65049)\tTrain_Kappa 0.39925(0.42412)\tTrain_MF1 0.44000(0.46994)\tTrain_Precision 0.50667(0.53024)\tTrain_Sensitivity 0.49000(0.48314)\tTrain_Specificity 0.88199(0.88301)\tTime 0.138s (0.140s)\tSpeed 231.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [51/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.674840752702011, Training Accuracy      : 0.615765977443609, Training G-Mean      : 0.6461825597123919\n",
            "Training Kappa      : 0.41301772066432124,Training MF1     : 0.46349445967297787, Training Precision      : 0.5262133871724729, Training Sensitivity      : 0.4781176043419462, Training Specificity      : 0.8801039790322903\n",
            "Validation Results : \n",
            "Validation Loss   : 3.447462346818712, Validation Accuracy : 0.3549074074074074, Validation G-Mean      : 0.5244298315156235\n",
            "Validation Kappa     : 0.1464441724878352, Validation MF1      : 0.2357639891129953, Validation Precision      : 0.28407106283638217,  Validation Sensitivity      : 0.33605552205332995, Validation Specificity      : 0.8317138517344439\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :16.7204177291305\n",
            "Class wise sensitivity W: 0.33772622214423287, S1: 0.0, S2: 0.646255088073236, S3: 0.6962963000491813, R: 0.0\n",
            "Class wise specificity W: 0.9887454443507724, S1: 1.0, S2: 0.37226282004956845, S3: 0.7975609942718789, R: 1.0\n",
            "Class wise F1  W: 0.481727569743439, S1: 0.0, S2: 0.39597589163868513, S3: 0.3011164841828523, R: 0.0\n",
            "===========================================================Training Epoch : [52/100] ===========================================================================================================>\n",
            "Epoch: [52/100][0/190]\tTrain_Loss 5.04850 (5.04850)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.62251(0.62251)\tTrain_Kappa 0.38995(0.38995)\tTrain_MF1 0.43610(0.43610)\tTrain_Precision 0.52580(0.52580)\tTrain_Sensitivity 0.44444(0.44444)\tTrain_Specificity 0.87192(0.87192)\tTime 0.140s (0.140s)\tSpeed 229.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [52/100][100/190]\tTrain_Loss 5.63389 (4.89980)\tTrain_Acc 0.56250 (0.60798)\tTrain_G-Mean 0.60211(0.64164)\tTrain_Kappa 0.34789(0.40706)\tTrain_MF1 0.43667(0.45427)\tTrain_Precision 0.49111(0.51892)\tTrain_Sensitivity 0.42000(0.47244)\tTrain_Specificity 0.86318(0.87932)\tTime 0.138s (0.140s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [52/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.9269872050536305, Training Accuracy      : 0.6086231203007519, Training G-Mean      : 0.6327370615561728\n",
            "Training Kappa      : 0.4015485658108001,Training MF1     : 0.4466149010313184, Training Precision      : 0.5105920065704146, Training Sensitivity      : 0.4603219720250679, Training Specificity      : 0.8778198000004422\n",
            "Validation Results : \n",
            "Validation Loss   : 2.550880471865336, Validation Accuracy : 0.33574074074074073, Validation G-Mean      : 0.5075629884127019\n",
            "Validation Kappa     : 0.11485174917241515, Validation MF1      : 0.2391312024107686, Validation Precision      : 0.3246661700584271,  Validation Sensitivity      : 0.3205536673466365, Validation Specificity      : 0.8243754225748559\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :11.59459368387858\n",
            "Class wise sensitivity W: 0.30148596730497146, S1: 0.0, S2: 0.5938749567226127, S3: 0.6629629675988797, R: 0.04444444510671827\n",
            "Class wise specificity W: 0.9871187695750484, S1: 0.9919569867628591, S2: 0.3183037473095788, S3: 0.8327355980873108, R: 0.9917620111394811\n",
            "Class wise F1  W: 0.438324229032905, S1: 0.0, S2: 0.35384082131915623, S3: 0.3352957809412921, R: 0.06819518076048957\n",
            "===========================================================Training Epoch : [53/100] ===========================================================================================================>\n",
            "Epoch: [53/100][0/190]\tTrain_Loss 4.96656 (4.96656)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.63478(0.63478)\tTrain_Kappa 0.41980(0.41980)\tTrain_MF1 0.47946(0.47946)\tTrain_Precision 0.63286(0.63286)\tTrain_Sensitivity 0.45864(0.45864)\tTrain_Specificity 0.87857(0.87857)\tTime 0.138s (0.138s)\tSpeed 231.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [53/100][100/190]\tTrain_Loss 5.56578 (4.82894)\tTrain_Acc 0.53125 (0.61696)\tTrain_G-Mean 0.54816(0.63835)\tTrain_Kappa 0.25117(0.41480)\tTrain_MF1 0.37762(0.45284)\tTrain_Precision 0.57067(0.51520)\tTrain_Sensitivity 0.35571(0.46534)\tTrain_Specificity 0.84471(0.88114)\tTime 0.138s (0.140s)\tSpeed 231.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [53/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.906570572602122, Training Accuracy      : 0.6148026315789473, Training G-Mean      : 0.6401396063024827\n",
            "Training Kappa      : 0.4132495147218766,Training MF1     : 0.46049880684990613, Training Precision      : 0.5289515879750255, Training Sensitivity      : 0.46908967465946566, Training Specificity      : 0.8802132635994965\n",
            "Validation Results : \n",
            "Validation Loss   : 2.69519865071332, Validation Accuracy : 0.3321296296296296, Validation G-Mean      : 0.48330788593196616\n",
            "Validation Kappa     : 0.09278730866047427, Validation MF1      : 0.2182097632575918, Validation Precision      : 0.2881574010407483,  Validation Sensitivity      : 0.2935801566750915, Validation Specificity      : 0.8188798325757184\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :12.041443330270273\n",
            "Class wise sensitivity W: 0.2115662647066293, S1: 0.0, S2: 0.7359641459253099, S3: 0.5092592614668386, R: 0.011111111276679568\n",
            "Class wise specificity W: 0.990616692437066, S1: 1.0, S2: 0.21934114989859085, S3: 0.8943802736423634, R: 0.9900610469005726\n",
            "Class wise F1  W: 0.325238025298825, S1: 0.0, S2: 0.3928964568508996, S3: 0.35383464782326307, R: 0.01907968631497136\n",
            "===========================================================Training Epoch : [54/100] ===========================================================================================================>\n",
            "Epoch: [54/100][0/190]\tTrain_Loss 4.87170 (4.87170)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.50071(0.50071)\tTrain_Kappa 0.19288(0.19288)\tTrain_MF1 0.25641(0.25641)\tTrain_Precision 0.28889(0.28889)\tTrain_Sensitivity 0.30000(0.30000)\tTrain_Specificity 0.83571(0.83571)\tTime 0.140s (0.140s)\tSpeed 228.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [54/100][100/190]\tTrain_Loss 5.34818 (4.80320)\tTrain_Acc 0.62500 (0.60396)\tTrain_G-Mean 0.59730(0.63170)\tTrain_Kappa 0.39144(0.39200)\tTrain_MF1 0.43135(0.44816)\tTrain_Precision 0.48667(0.52220)\tTrain_Sensitivity 0.40738(0.45892)\tTrain_Specificity 0.87574(0.87577)\tTime 0.143s (0.140s)\tSpeed 224.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [54/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.793029412470366, Training Accuracy      : 0.6068139097744362, Training G-Mean      : 0.6322877017994151\n",
            "Training Kappa      : 0.3991608192687784,Training MF1     : 0.44959036209081343, Training Precision      : 0.5200605717144514, Training Sensitivity      : 0.4593800133858859, Training Specificity      : 0.8776261913776394\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8994993721997298, Validation Accuracy : 0.2851851851851852, Validation G-Mean      : 0.4827007732527113\n",
            "Validation Kappa     : 0.07342876292973327, Validation MF1      : 0.1995366874668333, Validation Precision      : 0.27125393546841764,  Validation Sensitivity      : 0.2918575487203068, Validation Specificity      : 0.8154926619044057\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :13.335217052035862\n",
            "Class wise sensitivity W: 0.2511677673017537, S1: 0.0, S2: 0.4717002213001251, S3: 0.727160495740396, R: 0.009259259259259259\n",
            "Class wise specificity W: 0.9878692869786863, S1: 0.9849996456393489, S2: 0.3563141585500152, S3: 0.7588364194940638, R: 0.9894437988599142\n",
            "Class wise F1  W: 0.381330794206372, S1: 0.0, S2: 0.29512148212503503, S3: 0.3064163459671868, R: 0.014814815035572759\n",
            "===========================================================Training Epoch : [55/100] ===========================================================================================================>\n",
            "Epoch: [55/100][0/190]\tTrain_Loss 4.54329 (4.54329)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.65120(0.65120)\tTrain_Kappa 0.45763(0.45763)\tTrain_MF1 0.47000(0.47000)\tTrain_Precision 0.56212(0.56212)\tTrain_Sensitivity 0.47778(0.47778)\tTrain_Specificity 0.88756(0.88756)\tTime 0.140s (0.140s)\tSpeed 228.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [55/100][100/190]\tTrain_Loss 5.68018 (4.63676)\tTrain_Acc 0.56250 (0.60365)\tTrain_G-Mean 0.62923(0.63492)\tTrain_Kappa 0.39378(0.40239)\tTrain_MF1 0.45176(0.45320)\tTrain_Precision 0.54524(0.52403)\tTrain_Sensitivity 0.45214(0.46259)\tTrain_Specificity 0.87567(0.87797)\tTime 0.140s (0.140s)\tSpeed 228.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [55/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.641676046973781, Training Accuracy      : 0.6202067669172932, Training G-Mean      : 0.6437134554033335\n",
            "Training Kappa      : 0.41952104017481184,Training MF1     : 0.4639112491827261, Training Precision      : 0.5336912087860861, Training Sensitivity      : 0.4739346294105054, Training Specificity      : 0.8813187273081979\n",
            "Validation Results : \n",
            "Validation Loss   : 2.849386700877437, Validation Accuracy : 0.28782407407407407, Validation G-Mean      : 0.5084678971795447\n",
            "Validation Kappa     : 0.10342242894377517, Validation MF1      : 0.22111232667057604, Validation Precision      : 0.30033460562979747,  Validation Sensitivity      : 0.3176288965675566, Validation Specificity      : 0.8234760487521134\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :12.685577639827022\n",
            "Class wise sensitivity W: 0.3218732964661386, S1: 0.006172839690137793, S2: 0.3140665988127391, S3: 0.8580246920938845, R: 0.08800705577488299\n",
            "Class wise specificity W: 0.9892975202313176, S1: 0.9924287111670883, S2: 0.5453973567044293, S3: 0.6625379147352995, R: 0.9277187409224333\n",
            "Class wise F1  W: 0.4612562043799294, S1: 0.009259259259259259, S2: 0.2544288436571757, S3: 0.26714663759425833, R: 0.11347068846225739\n",
            "===========================================================Training Epoch : [56/100] ===========================================================================================================>\n",
            "Epoch: [56/100][0/190]\tTrain_Loss 3.77691 (3.77691)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.74245(0.74245)\tTrain_Kappa 0.50000(0.50000)\tTrain_MF1 0.56802(0.56802)\tTrain_Precision 0.54333(0.54333)\tTrain_Sensitivity 0.61804(0.61804)\tTrain_Specificity 0.89190(0.89190)\tTime 0.139s (0.139s)\tSpeed 229.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [56/100][100/190]\tTrain_Loss 5.20146 (4.65246)\tTrain_Acc 0.43750 (0.61757)\tTrain_G-Mean 0.44037(0.64537)\tTrain_Kappa 0.08134(0.41544)\tTrain_MF1 0.20889(0.46642)\tTrain_Precision 0.22564(0.52838)\tTrain_Sensitivity 0.23810(0.47668)\tTrain_Specificity 0.81447(0.88001)\tTime 0.138s (0.140s)\tSpeed 231.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [56/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.639172254110638, Training Accuracy      : 0.6207706766917294, Training G-Mean      : 0.6460784931224781\n",
            "Training Kappa      : 0.42190628127410823,Training MF1     : 0.46765731450758463, Training Precision      : 0.5285602463860262, Training Sensitivity      : 0.4768649729694191, Training Specificity      : 0.8818121408945641\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8666221918883146, Validation Accuracy : 0.28055555555555556, Validation G-Mean      : 0.48661026808638247\n",
            "Validation Kappa     : 0.06690963499746483, Validation MF1      : 0.20116380436552894, Validation Precision      : 0.28598954829352874,  Validation Sensitivity      : 0.297451666659779, Validation Specificity      : 0.8145250216678336\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :12.772725564462167\n",
            "Class wise sensitivity W: 0.2440997326263675, S1: 0.0, S2: 0.4298869945384838, S3: 0.7839506180198105, R: 0.02932098811423337\n",
            "Class wise specificity W: 0.9895310490219681, S1: 0.9961655404832628, S2: 0.3639484765353026, S3: 0.7475392553541396, R: 0.9754407869444953\n",
            "Class wise F1  W: 0.36854353491906766, S1: 0.0, S2: 0.2748569815799042, S3: 0.3170085648695628, R: 0.04540994045911012\n",
            "===========================================================Training Epoch : [57/100] ===========================================================================================================>\n",
            "Epoch: [57/100][0/190]\tTrain_Loss 4.41124 (4.41124)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.62499(0.62499)\tTrain_Kappa 0.47305(0.47305)\tTrain_MF1 0.47048(0.47048)\tTrain_Precision 0.55667(0.55667)\tTrain_Sensitivity 0.43667(0.43667)\tTrain_Specificity 0.89453(0.89453)\tTime 0.140s (0.140s)\tSpeed 228.6 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [57/100][100/190]\tTrain_Loss 5.27032 (4.86778)\tTrain_Acc 0.56250 (0.60489)\tTrain_G-Mean 0.57628(0.62873)\tTrain_Kappa 0.32326(0.39437)\tTrain_MF1 0.40113(0.44900)\tTrain_Precision 0.57067(0.52391)\tTrain_Sensitivity 0.38667(0.45375)\tTrain_Specificity 0.85887(0.87637)\tTime 0.139s (0.140s)\tSpeed 230.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [57/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.8124635395250825, Training Accuracy      : 0.6084116541353384, Training G-Mean      : 0.6312719059626097\n",
            "Training Kappa      : 0.39837010017766233,Training MF1     : 0.45133467534655003, Training Precision      : 0.5264688773217953, Training Sensitivity      : 0.45812293037772195, Training Specificity      : 0.8769568107786933\n",
            "Validation Results : \n",
            "Validation Loss   : 2.271148959795634, Validation Accuracy : 0.3513425925925926, Validation G-Mean      : 0.5400129078971007\n",
            "Validation Kappa     : 0.1644283342321026, Validation MF1      : 0.27642929377379244, Validation Precision      : 0.3512116659570623,  Validation Sensitivity      : 0.35648634731769563, Validation Specificity      : 0.8358137313966396\n",
            "Validation T Acc     : 0.8542129629629629, Val_KD_Loss :10.127543414080584\n",
            "Class wise sensitivity W: 0.3715696409344673, S1: 0.0, S2: 0.44789912744804666, S3: 0.7314814825852712, R: 0.23148148562069293\n",
            "Class wise specificity W: 0.9875471989313761, S1: 0.9936789627428408, S2: 0.5193565653430091, S3: 0.7595205925129078, R: 0.9189653374530651\n",
            "Class wise F1  W: 0.5152634481588999, S1: 0.0, S2: 0.32672686874866486, S3: 0.29406293840320025, R: 0.24609321355819702\n",
            "===========================================================Training Epoch : [58/100] ===========================================================================================================>\n",
            "Epoch: [58/100][0/190]\tTrain_Loss 5.07534 (5.07534)\tTrain_Acc 0.71875 (0.71875)\tTrain_G-Mean 0.72332(0.72332)\tTrain_Kappa 0.53247(0.53247)\tTrain_MF1 0.55032(0.55032)\tTrain_Precision 0.59286(0.59286)\tTrain_Sensitivity 0.58218(0.58218)\tTrain_Specificity 0.89867(0.89867)\tTime 0.138s (0.138s)\tSpeed 231.5 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [58/100][100/190]\tTrain_Loss 3.80020 (4.67357)\tTrain_Acc 0.62500 (0.61262)\tTrain_G-Mean 0.63509(0.63982)\tTrain_Kappa 0.43027(0.41023)\tTrain_MF1 0.48693(0.45708)\tTrain_Precision 0.58070(0.52510)\tTrain_Sensitivity 0.45333(0.46903)\tTrain_Specificity 0.88972(0.87988)\tTime 0.138s (0.140s)\tSpeed 232.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [58/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.648101750173066, Training Accuracy      : 0.618609022556391, Training G-Mean      : 0.6479977132858374\n",
            "Training Kappa      : 0.42002643838950304,Training MF1     : 0.4679257930266232, Training Precision      : 0.5342712887337329, Training Sensitivity      : 0.4801865239911957, Training Specificity      : 0.8818066211593781\n",
            "Validation Results : \n",
            "Validation Loss   : 1.9670989204336096, Validation Accuracy : 0.4010648148148148, Validation G-Mean      : 0.5657342695009597\n",
            "Validation Kappa     : 0.20111031343705094, Validation MF1      : 0.31433828638659583, Validation Precision      : 0.39032182450647707,  Validation Sensitivity      : 0.38553806504717586, Validation Specificity      : 0.8424504015180799\n",
            "Validation T Acc     : 0.8538888888888889, Val_KD_Loss :8.019561502668592\n",
            "Class wise sensitivity W: 0.28286440422137576, S1: 0.0052910055275316595, S2: 0.6752638905136673, S3: 0.6777777804268731, R: 0.2864932445464311\n",
            "Class wise specificity W: 0.9900298825016728, S1: 0.9987654310685617, S2: 0.46787213616900974, S3: 0.892980017043926, R: 0.8626045408072295\n",
            "Class wise F1  W: 0.42114021711879307, S1: 0.009259259259259259, S2: 0.43157193395826554, S3: 0.4130358574567018, R: 0.29668416413995957\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [59/100] ===========================================================================================================>\n",
            "Epoch: [59/100][0/190]\tTrain_Loss 3.41819 (3.41819)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.74581(0.74581)\tTrain_Kappa 0.53129(0.53129)\tTrain_MF1 0.62513(0.62513)\tTrain_Precision 0.64143(0.64143)\tTrain_Sensitivity 0.61500(0.61500)\tTrain_Specificity 0.90444(0.90444)\tTime 0.142s (0.142s)\tSpeed 225.1 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [59/100][100/190]\tTrain_Loss 4.25008 (4.81767)\tTrain_Acc 0.68750 (0.61231)\tTrain_G-Mean 0.70434(0.64066)\tTrain_Kappa 0.47541(0.40313)\tTrain_MF1 0.56842(0.46006)\tTrain_Precision 0.59842(0.52634)\tTrain_Sensitivity 0.54842(0.47169)\tTrain_Specificity 0.90458(0.87800)\tTime 0.138s (0.141s)\tSpeed 232.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [59/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.950266883247777, Training Accuracy      : 0.6096099624060151, Training G-Mean      : 0.6375319530375466\n",
            "Training Kappa      : 0.40230647277873777,Training MF1     : 0.4576726194118196, Training Precision      : 0.5303637624571197, Training Sensitivity      : 0.46706033429032856, Training Specificity      : 0.8779743539032178\n",
            "Validation Results : \n",
            "Validation Loss   : 2.241476549042596, Validation Accuracy : 0.3263425925925926, Validation G-Mean      : 0.5166385355312892\n",
            "Validation Kappa     : 0.09711221772183229, Validation MF1      : 0.24087038106388514, Validation Precision      : 0.31905962891048856,  Validation Sensitivity      : 0.33387292998808393, Validation Specificity      : 0.8197003662862159\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :9.439294196941235\n",
            "Class wise sensitivity W: 0.20215551224019793, S1: 0.0, S2: 0.6803925556165201, S3: 0.7148148174639102, R: 0.07200176461979195\n",
            "Class wise specificity W: 0.9933322182408085, S1: 0.9982363316747878, S2: 0.2625644771313226, S3: 0.8715028718665794, R: 0.9728659325175815\n",
            "Class wise F1  W: 0.3104309340318044, S1: 0.0, S2: 0.37655951689790795, S3: 0.42229972448613906, R: 0.09506172990357434\n",
            "===========================================================Training Epoch : [60/100] ===========================================================================================================>\n",
            "Epoch: [60/100][0/190]\tTrain_Loss 4.36808 (4.36808)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.71029(0.71029)\tTrain_Kappa 0.51448(0.51448)\tTrain_MF1 0.56506(0.56506)\tTrain_Precision 0.65298(0.65298)\tTrain_Sensitivity 0.56000(0.56000)\tTrain_Specificity 0.90093(0.90093)\tTime 0.140s (0.140s)\tSpeed 229.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [60/100][100/190]\tTrain_Loss 5.94663 (5.14784)\tTrain_Acc 0.53125 (0.59251)\tTrain_G-Mean 0.53907(0.62320)\tTrain_Kappa 0.28678(0.38028)\tTrain_MF1 0.34390(0.43298)\tTrain_Precision 0.39810(0.49019)\tTrain_Sensitivity 0.34048(0.44922)\tTrain_Specificity 0.85351(0.87336)\tTime 0.138s (0.140s)\tSpeed 232.2 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [60/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.100208909888017, Training Accuracy      : 0.5987546992481203, Training G-Mean      : 0.62609238163944\n",
            "Training Kappa      : 0.3864758940103315,Training MF1     : 0.4406446016775935, Training Precision      : 0.5056623725357806, Training Sensitivity      : 0.4523921226670868, Training Specificity      : 0.8746232052931657\n",
            "Validation Results : \n",
            "Validation Loss   : 2.1504796390180236, Validation Accuracy : 0.41662037037037036, Validation G-Mean      : 0.5230394717614817\n",
            "Validation Kappa     : 0.19443015695530808, Validation MF1      : 0.28795927542227284, Validation Precision      : 0.41469195446482393,  Validation Sensitivity      : 0.33015844579096193, Validation Specificity      : 0.8402898695181917\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :9.02494047306202\n",
            "Class wise sensitivity W: 0.2717494495488979, S1: 0.0, S2: 0.9097599983215332, S3: 0.2728395097785526, R: 0.19644327130582598\n",
            "Class wise specificity W: 0.9907062605575279, S1: 1.0, S2: 0.3002741731427334, S3: 0.9790080322159661, R: 0.9314608816747312\n",
            "Class wise F1  W: 0.40601649990788213, S1: 0.0, S2: 0.4895919550348211, S3: 0.3020282233202899, R: 0.24215969884837116\n",
            "===========================================================Training Epoch : [61/100] ===========================================================================================================>\n",
            "Epoch: [61/100][0/190]\tTrain_Loss 5.31423 (5.31423)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.52652(0.52652)\tTrain_Kappa 0.28152(0.28152)\tTrain_MF1 0.31162(0.31162)\tTrain_Precision 0.31636(0.31636)\tTrain_Sensitivity 0.32381(0.32381)\tTrain_Specificity 0.85612(0.85612)\tTime 0.143s (0.143s)\tSpeed 223.6 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [61/100][100/190]\tTrain_Loss 3.98703 (5.22481)\tTrain_Acc 0.68750 (0.59220)\tTrain_G-Mean 0.67002(0.62479)\tTrain_Kappa 0.51220(0.38276)\tTrain_MF1 0.53651(0.44245)\tTrain_Precision 0.66667(0.51597)\tTrain_Sensitivity 0.50000(0.45119)\tTrain_Specificity 0.89785(0.87422)\tTime 0.138s (0.140s)\tSpeed 231.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [61/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.148979853328906, Training Accuracy      : 0.5963580827067669, Training G-Mean      : 0.6245475335819113\n",
            "Training Kappa      : 0.3797161213013486,Training MF1     : 0.4398478419529763, Training Precision      : 0.5152946416170973, Training Sensitivity      : 0.45131960189656195, Training Specificity      : 0.8731883824655888\n",
            "Validation Results : \n",
            "Validation Loss   : 2.5309041385297424, Validation Accuracy : 0.30814814814814817, Validation G-Mean      : 0.49787867511128736\n",
            "Validation Kappa     : 0.08762363188715698, Validation MF1      : 0.22160049240898202, Validation Precision      : 0.32315928102643404,  Validation Sensitivity      : 0.31138402295333373, Validation Specificity      : 0.8185908892640362\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :11.199449291935673\n",
            "Class wise sensitivity W: 0.24979933930767906, S1: 0.007407407517786379, S2: 0.5203482861872073, S3: 0.7370370383615847, R: 0.04232804339241098\n",
            "Class wise specificity W: 0.9899371553350378, S1: 0.9897908546306469, S2: 0.32066478387073233, S3: 0.8056025151853208, R: 0.9869591372984426\n",
            "Class wise F1  W: 0.37013758774156924, S1: 0.012345679380275585, S2: 0.3146535067094697, S3: 0.3407598656636697, R: 0.07010582254992591\n",
            "===========================================================Training Epoch : [62/100] ===========================================================================================================>\n",
            "Epoch: [62/100][0/190]\tTrain_Loss 4.29448 (4.29448)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.62358(0.62358)\tTrain_Kappa 0.41076(0.41076)\tTrain_MF1 0.46434(0.46434)\tTrain_Precision 0.60000(0.60000)\tTrain_Sensitivity 0.44176(0.44176)\tTrain_Specificity 0.88023(0.88023)\tTime 0.142s (0.142s)\tSpeed 224.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [62/100][100/190]\tTrain_Loss 5.18581 (4.92610)\tTrain_Acc 0.65625 (0.60860)\tTrain_G-Mean 0.62368(0.62807)\tTrain_Kappa 0.41818(0.39775)\tTrain_MF1 0.46385(0.44672)\tTrain_Precision 0.56303(0.51269)\tTrain_Sensitivity 0.44314(0.45337)\tTrain_Specificity 0.87778(0.87776)\tTime 0.140s (0.140s)\tSpeed 229.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [62/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.0515979904877515, Training Accuracy      : 0.5967105263157895, Training G-Mean      : 0.6224067009878715\n",
            "Training Kappa      : 0.38384117883504276,Training MF1     : 0.4365058336446162, Training Precision      : 0.5024978321790694, Training Sensitivity      : 0.4471114963606787, Training Specificity      : 0.8746856049487465\n",
            "Validation Results : \n",
            "Validation Loss   : 2.6860635457215487, Validation Accuracy : 0.30814814814814817, Validation G-Mean      : 0.5125817428975731\n",
            "Validation Kappa     : 0.09496373915976368, Validation MF1      : 0.23011927775762697, Validation Precision      : 0.3713148364866221,  Validation Sensitivity      : 0.3256904519266552, Validation Specificity      : 0.8197046361587664\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :12.083438590720847\n",
            "Class wise sensitivity W: 0.2099341419008043, S1: 0.0, S2: 0.572795010275311, S3: 0.7487654343799308, R: 0.0969576730772301\n",
            "Class wise specificity W: 0.9968350154382212, S1: 0.996208111445109, S2: 0.32553204894065857, S3: 0.8100498627733301, R: 0.9698981421965139\n",
            "Class wise F1  W: 0.32743108796852605, S1: 0.0, S2: 0.3366300143577434, S3: 0.3423553887340758, R: 0.1441798977277897\n",
            "===========================================================Training Epoch : [63/100] ===========================================================================================================>\n",
            "Epoch: [63/100][0/190]\tTrain_Loss 4.40807 (4.40807)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.49602(0.49602)\tTrain_Kappa 0.12540(0.12540)\tTrain_MF1 0.27308(0.27308)\tTrain_Precision 0.25435(0.25435)\tTrain_Sensitivity 0.30000(0.30000)\tTrain_Specificity 0.82012(0.82012)\tTime 0.138s (0.138s)\tSpeed 231.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [63/100][100/190]\tTrain_Loss 5.37170 (5.02798)\tTrain_Acc 0.62500 (0.58663)\tTrain_G-Mean 0.71993(0.61670)\tTrain_Kappa 0.47180(0.37659)\tTrain_MF1 0.53295(0.42883)\tTrain_Precision 0.59000(0.49460)\tTrain_Sensitivity 0.58333(0.43899)\tTrain_Specificity 0.88851(0.87337)\tTime 0.138s (0.140s)\tSpeed 232.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [63/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.194796314992403, Training Accuracy      : 0.5818609022556391, Training G-Mean      : 0.607983174015768\n",
            "Training Kappa      : 0.35729195816885606,Training MF1     : 0.4158991846442225, Training Precision      : 0.4848481125659068, Training Sensitivity      : 0.42898706972599016, Training Specificity      : 0.8690567240942471\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2100026916574547, Validation Accuracy : 0.31773148148148145, Validation G-Mean      : 0.4942130574571526\n",
            "Validation Kappa     : 0.08247656769393633, Validation MF1      : 0.21822209380291124, Validation Precision      : 0.3040070104378241,  Validation Sensitivity      : 0.30482414999493845, Validation Specificity      : 0.8168345959650146\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :9.567481659076831\n",
            "Class wise sensitivity W: 0.1918717128811059, S1: 0.012345679380275585, S2: 0.6738716087959431, S3: 0.6407407433898361, R: 0.0052910055275316595\n",
            "Class wise specificity W: 0.9911290296801815, S1: 0.9873474902576871, S2: 0.2218234872928372, S3: 0.886726196165438, R: 0.9971467764289291\n",
            "Class wise F1  W: 0.30746454607557366, S1: 0.018518518518518517, S2: 0.3652222603559494, S3: 0.39064588480525547, R: 0.009259259259259259\n",
            "===========================================================Training Epoch : [64/100] ===========================================================================================================>\n",
            "Epoch: [64/100][0/190]\tTrain_Loss 4.77940 (4.77940)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.73948(0.73948)\tTrain_Kappa 0.47208(0.47208)\tTrain_MF1 0.59932(0.59932)\tTrain_Precision 0.75854(0.75854)\tTrain_Sensitivity 0.61389(0.61389)\tTrain_Specificity 0.89076(0.89076)\tTime 0.139s (0.139s)\tSpeed 229.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [64/100][100/190]\tTrain_Loss 7.52734 (5.04820)\tTrain_Acc 0.46875 (0.58818)\tTrain_G-Mean 0.51141(0.62564)\tTrain_Kappa 0.20351(0.37910)\tTrain_MF1 0.30162(0.44086)\tTrain_Precision 0.39167(0.51186)\tTrain_Sensitivity 0.31190(0.45147)\tTrain_Specificity 0.83852(0.87319)\tTime 0.139s (0.140s)\tSpeed 230.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [64/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.9875345029329, Training Accuracy      : 0.5960761278195489, Training G-Mean      : 0.6243160303932936\n",
            "Training Kappa      : 0.38426509336886744,Training MF1     : 0.4405660999762385, Training Precision      : 0.510013424879626, Training Sensitivity      : 0.44897352649977335, Training Specificity      : 0.8744421734935353\n",
            "Validation Results : \n",
            "Validation Loss   : 1.733563122925935, Validation Accuracy : 0.4610648148148148, Validation G-Mean      : 0.6168437040590559\n",
            "Validation Kappa     : 0.27055493750522386, Validation MF1      : 0.39173149643120936, Validation Precision      : 0.45481050257329586,  Validation Sensitivity      : 0.44939543257157005, Validation Specificity      : 0.8556153372482016\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :7.1505061961986405\n",
            "Class wise sensitivity W: 0.3161640390753746, S1: 0.0052910055275316595, S2: 0.7761834892961714, S3: 0.7129629640667526, R: 0.43637566489202007\n",
            "Class wise specificity W: 0.9905029822278906, S1: 0.9924200883618107, S2: 0.5063545946721677, S3: 0.9646086869416414, R: 0.8241903340374982\n",
            "Class wise F1  W: 0.4598990982329404, S1: 0.008230452736218771, S2: 0.50552172406956, S3: 0.6152263438260114, R: 0.36977986329131657\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [65/100] ===========================================================================================================>\n",
            "Epoch: [65/100][0/190]\tTrain_Loss 5.06696 (5.06696)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.54054(0.54054)\tTrain_Kappa 0.30757(0.30757)\tTrain_MF1 0.36022(0.36022)\tTrain_Precision 0.50833(0.50833)\tTrain_Sensitivity 0.34071(0.34071)\tTrain_Specificity 0.85755(0.85755)\tTime 0.150s (0.150s)\tSpeed 213.9 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [65/100][100/190]\tTrain_Loss 4.99292 (5.22722)\tTrain_Acc 0.71875 (0.58663)\tTrain_G-Mean 0.67354(0.61506)\tTrain_Kappa 0.54646(0.36527)\tTrain_MF1 0.54692(0.42903)\tTrain_Precision 0.70286(0.50841)\tTrain_Sensitivity 0.50179(0.43949)\tTrain_Specificity 0.90410(0.86966)\tTime 0.159s (0.141s)\tSpeed 200.8 samples/s\tData 0.004s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [65/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.389228179580287, Training Accuracy      : 0.5847744360902256, Training G-Mean      : 0.6093072639986824\n",
            "Training Kappa      : 0.35845371542293686,Training MF1     : 0.42171185912270315, Training Precision      : 0.5040096335426759, Training Sensitivity      : 0.43197734359063605, Training Specificity      : 0.8684786126801842\n",
            "Validation Results : \n",
            "Validation Loss   : 1.639809705592968, Validation Accuracy : 0.4625462962962963, Validation G-Mean      : 0.5965300361987905\n",
            "Validation Kappa     : 0.2762886324198241, Validation MF1      : 0.36725741083975194, Validation Precision      : 0.4379348421538318,  Validation Sensitivity      : 0.4230267489949862, Validation Specificity      : 0.8568179121723881\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :7.149168650309245\n",
            "Class wise sensitivity W: 0.32806062118874657, S1: 0.021604938639534846, S2: 0.8240219707842227, S3: 0.5092592592592593, R: 0.4321869551031678\n",
            "Class wise specificity W: 0.9878751083656594, S1: 0.9946791132291158, S2: 0.4959498842557271, S3: 0.9488473402129279, R: 0.8567381147985105\n",
            "Class wise F1  W: 0.46109999385144973, S1: 0.03333333355409128, S2: 0.5260366046870196, S3: 0.4118005534013112, R: 0.40401656870488767\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [66/100] ===========================================================================================================>\n",
            "Epoch: [66/100][0/190]\tTrain_Loss 7.76308 (7.76308)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.53133(0.53133)\tTrain_Kappa 0.23582(0.23582)\tTrain_MF1 0.31111(0.31111)\tTrain_Precision 0.31565(0.31565)\tTrain_Sensitivity 0.33495(0.33495)\tTrain_Specificity 0.84287(0.84287)\tTime 0.147s (0.147s)\tSpeed 217.2 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [66/100][100/190]\tTrain_Loss 6.22620 (5.59818)\tTrain_Acc 0.65625 (0.58478)\tTrain_G-Mean 0.70349(0.60669)\tTrain_Kappa 0.50562(0.35564)\tTrain_MF1 0.52222(0.41471)\tTrain_Precision 0.59060(0.48765)\tTrain_Sensitivity 0.55176(0.42913)\tTrain_Specificity 0.89695(0.86789)\tTime 0.139s (0.142s)\tSpeed 230.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [66/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.360584069553174, Training Accuracy      : 0.5902255639097744, Training G-Mean      : 0.6160176121229105\n",
            "Training Kappa      : 0.3687379623911483,Training MF1     : 0.4267081284052446, Training Precision      : 0.49440200178246774, Training Sensitivity      : 0.4400983879911273, Training Specificity      : 0.8709553436307536\n",
            "Validation Results : \n",
            "Validation Loss   : 1.670277997299477, Validation Accuracy : 0.4350925925925926, Validation G-Mean      : 0.6060275985536739\n",
            "Validation Kappa     : 0.25178067009928035, Validation MF1      : 0.3566651555123152, Validation Precision      : 0.43426915197460736,  Validation Sensitivity      : 0.4332129643471154, Validation Specificity      : 0.8532903295976146\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :6.9706682982268156\n",
            "Class wise sensitivity W: 0.28897142437873063, S1: 0.04567901293436686, S2: 0.6791803969277276, S3: 0.7160493852915587, R: 0.43618460220319255\n",
            "Class wise specificity W: 0.9872293185304712, S1: 0.9820104462129099, S2: 0.569892512427436, S3: 0.911870660605254, R: 0.815448710212001\n",
            "Class wise F1  W: 0.42244219228073404, S1: 0.0641975325566751, S2: 0.4804817874122549, S3: 0.44557357055169566, R: 0.3706306947602166\n",
            "===========================================================Training Epoch : [67/100] ===========================================================================================================>\n",
            "Epoch: [67/100][0/190]\tTrain_Loss 5.19321 (5.19321)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.55359(0.55359)\tTrain_Kappa 0.19048(0.19048)\tTrain_MF1 0.36020(0.36020)\tTrain_Precision 0.40362(0.40362)\tTrain_Sensitivity 0.36813(0.36813)\tTrain_Specificity 0.83249(0.83249)\tTime 0.138s (0.138s)\tSpeed 231.1 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [67/100][100/190]\tTrain_Loss 5.06812 (5.15938)\tTrain_Acc 0.59375 (0.59684)\tTrain_G-Mean 0.50265(0.61999)\tTrain_Kappa 0.26502(0.38224)\tTrain_MF1 0.29683(0.43364)\tTrain_Precision 0.36133(0.49877)\tTrain_Sensitivity 0.29681(0.44513)\tTrain_Specificity 0.85126(0.87411)\tTime 0.138s (0.140s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [67/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.15477644769769, Training Accuracy      : 0.5994360902255639, Training G-Mean      : 0.6214465410590901\n",
            "Training Kappa      : 0.38390260951243876,Training MF1     : 0.4381124145106264, Training Precision      : 0.5052055489546372, Training Sensitivity      : 0.44648778668359695, Training Specificity      : 0.8740570210939961\n",
            "Validation Results : \n",
            "Validation Loss   : 2.446170683260317, Validation Accuracy : 0.34268518518518515, Validation G-Mean      : 0.5365447093126468\n",
            "Validation Kappa     : 0.13027191862677792, Validation MF1      : 0.2579646054793287, Validation Precision      : 0.3370218410260147,  Validation Sensitivity      : 0.3546955205224179, Validation Specificity      : 0.825993893764637\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :10.8043879226402\n",
            "Class wise sensitivity W: 0.13539272795120874, S1: 0.0, S2: 0.6120266677052887, S3: 0.7537037045867355, R: 0.27235450236885633\n",
            "Class wise specificity W: 0.9944802279825564, S1: 1.0, S2: 0.36914024308875754, S3: 0.8655273274139121, R: 0.9008216703379596\n",
            "Class wise F1  W: 0.22358931490668543, S1: 0.0, S2: 0.38324433868681945, S3: 0.3729358944627974, R: 0.31005347934034133\n",
            "===========================================================Training Epoch : [68/100] ===========================================================================================================>\n",
            "Epoch: [68/100][0/190]\tTrain_Loss 6.84170 (6.84170)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.54492(0.54492)\tTrain_Kappa 0.27581(0.27581)\tTrain_MF1 0.33651(0.33651)\tTrain_Precision 0.41667(0.41667)\tTrain_Sensitivity 0.34848(0.34848)\tTrain_Specificity 0.85208(0.85208)\tTime 0.138s (0.138s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [68/100][100/190]\tTrain_Loss 6.36279 (5.31435)\tTrain_Acc 0.46875 (0.59127)\tTrain_G-Mean 0.50512(0.60588)\tTrain_Kappa 0.19764(0.36600)\tTrain_MF1 0.29571(0.42298)\tTrain_Precision 0.33333(0.49928)\tTrain_Sensitivity 0.30447(0.42653)\tTrain_Specificity 0.83801(0.87012)\tTime 0.138s (0.140s)\tSpeed 232.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [68/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 5.22645828849391, Training Accuracy      : 0.5964285714285714, Training G-Mean      : 0.614097675413508\n",
            "Training Kappa      : 0.3756213885396853,Training MF1     : 0.43035417879882615, Training Precision      : 0.501292701904711, Training Sensitivity      : 0.436662922838801, Training Specificity      : 0.8723884796311977\n",
            "Validation Results : \n",
            "Validation Loss   : 1.5610525166546856, Validation Accuracy : 0.4703240740740741, Validation G-Mean      : 0.625454328930053\n",
            "Validation Kappa     : 0.30360482613760664, Validation MF1      : 0.3820610580620943, Validation Precision      : 0.42451705932617184,  Validation Sensitivity      : 0.457837329197813, Validation Specificity      : 0.8637787933702822\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :6.837479732654713\n",
            "Class wise sensitivity W: 0.36452991101476884, S1: 0.0, S2: 0.6436602545005304, S3: 0.7283950646718343, R: 0.5526014158019313\n",
            "Class wise specificity W: 0.9820602999793159, S1: 0.9986282587051392, S2: 0.6589023890318694, S3: 0.9031152658992343, R: 0.7761877532358523\n",
            "Class wise F1  W: 0.5104868908723196, S1: 0.0, S2: 0.4913111174548114, S3: 0.4941104253133138, R: 0.4143968566700264\n",
            "================================================================================================\n",
            "                                          Saving Best Model (ACC)                                     \n",
            "================================================================================================\n",
            "================================================================================================\n",
            "                                          Saving Best Model (Kappa)                                    \n",
            "================================================================================================\n",
            "===========================================================Training Epoch : [69/100] ===========================================================================================================>\n",
            "Epoch: [69/100][0/190]\tTrain_Loss 3.95371 (3.95371)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.62784(0.62784)\tTrain_Kappa 0.42081(0.42081)\tTrain_MF1 0.48667(0.48667)\tTrain_Precision 0.56737(0.56737)\tTrain_Sensitivity 0.44405(0.44405)\tTrain_Specificity 0.88769(0.88769)\tTime 0.142s (0.142s)\tSpeed 225.9 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [69/100][100/190]\tTrain_Loss 6.17459 (4.99149)\tTrain_Acc 0.53125 (0.60767)\tTrain_G-Mean 0.64630(0.63666)\tTrain_Kappa 0.31330(0.40065)\tTrain_MF1 0.44000(0.45413)\tTrain_Precision 0.44333(0.53511)\tTrain_Sensitivity 0.48590(0.46520)\tTrain_Specificity 0.85965(0.87701)\tTime 0.139s (0.141s)\tSpeed 230.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [69/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.95104158426586, Training Accuracy      : 0.6085056390977444, Training G-Mean      : 0.6384308581142468\n",
            "Training Kappa      : 0.40275381617608735,Training MF1     : 0.45749870742622173, Training Precision      : 0.5366278425486463, Training Sensitivity      : 0.4678093754068804, Training Specificity      : 0.8775578577267494\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2434577147165933, Validation Accuracy : 0.339537037037037, Validation G-Mean      : 0.5524557444667013\n",
            "Validation Kappa     : 0.13496806874478584, Validation MF1      : 0.2728118495256813, Validation Precision      : 0.35584531269139713,  Validation Sensitivity      : 0.3722958447756591, Validation Specificity      : 0.8274517774581909\n",
            "Validation T Acc     : 0.8564814814814815, Val_KD_Loss :10.236689320317021\n",
            "Class wise sensitivity W: 0.30948388548912825, S1: 0.0, S2: 0.5173673061309038, S3: 0.8734567938027559, R: 0.1611712384555075\n",
            "Class wise specificity W: 0.9837774700588651, S1: 0.9988425925925926, S2: 0.3779161682835332, S3: 0.8180919532422666, R: 0.9586307031136972\n",
            "Class wise F1  W: 0.44396615359518266, S1: 0.0, S2: 0.32606951036938914, S3: 0.3792809132072661, R: 0.2147426704565684\n",
            "===========================================================Training Epoch : [70/100] ===========================================================================================================>\n",
            "Epoch: [70/100][0/190]\tTrain_Loss 6.60134 (6.60134)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.61701(0.61701)\tTrain_Kappa 0.37430(0.37430)\tTrain_MF1 0.42437(0.42437)\tTrain_Precision 0.44667(0.44667)\tTrain_Sensitivity 0.43667(0.43667)\tTrain_Specificity 0.87182(0.87182)\tTime 0.139s (0.139s)\tSpeed 230.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [70/100][100/190]\tTrain_Loss 6.47743 (4.79649)\tTrain_Acc 0.62500 (0.60829)\tTrain_G-Mean 0.69022(0.63563)\tTrain_Kappa 0.44748(0.39861)\tTrain_MF1 0.54545(0.45444)\tTrain_Precision 0.62632(0.52088)\tTrain_Sensitivity 0.53810(0.46462)\tTrain_Specificity 0.88535(0.87781)\tTime 0.139s (0.140s)\tSpeed 230.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [70/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.781425626654374, Training Accuracy      : 0.6144736842105263, Training G-Mean      : 0.6411289109540174\n",
            "Training Kappa      : 0.41137717174241156,Training MF1     : 0.46190728749099547, Training Precision      : 0.5293626768965467, Training Sensitivity      : 0.470693849850642, Training Specificity      : 0.8797010706600392\n",
            "Validation Results : \n",
            "Validation Loss   : 1.7239727444118924, Validation Accuracy : 0.4126388888888889, Validation G-Mean      : 0.6049406786343055\n",
            "Validation Kappa     : 0.23836525289478863, Validation MF1      : 0.3417013412824383, Validation Precision      : 0.3881631965438525,  Validation Sensitivity      : 0.4339450928899977, Validation Specificity      : 0.850389254534686\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :7.245628392254865\n",
            "Class wise sensitivity W: 0.30471987553216795, S1: 0.0, S2: 0.49922074570699976, S3: 0.8043209888316967, R: 0.5614638543791242\n",
            "Class wise specificity W: 0.9875126922572101, S1: 0.9883928563859727, S2: 0.6423492564095391, S3: 0.8603915505939059, R: 0.7732999170268023\n",
            "Class wise F1  W: 0.4431485456449014, S1: 0.0, S2: 0.4052911834032447, S3: 0.4440605259603924, R: 0.41600645140365317\n",
            "===========================================================Training Epoch : [71/100] ===========================================================================================================>\n",
            "Epoch: [71/100][0/190]\tTrain_Loss 4.89373 (4.89373)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.68226(0.68226)\tTrain_Kappa 0.48178(0.48178)\tTrain_MF1 0.50936(0.50936)\tTrain_Precision 0.53778(0.53778)\tTrain_Sensitivity 0.52071(0.52071)\tTrain_Specificity 0.89393(0.89393)\tTime 0.139s (0.139s)\tSpeed 231.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [71/100][100/190]\tTrain_Loss 5.87825 (4.77767)\tTrain_Acc 0.40625 (0.61015)\tTrain_G-Mean 0.59345(0.64380)\tTrain_Kappa 0.12518(0.41194)\tTrain_MF1 0.35351(0.46366)\tTrain_Precision 0.42848(0.53205)\tTrain_Sensitivity 0.43013(0.47471)\tTrain_Specificity 0.81880(0.87992)\tTime 0.138s (0.140s)\tSpeed 232.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [71/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.737843994090432, Training Accuracy      : 0.6164943609022556, Training G-Mean      : 0.6461698829664857\n",
            "Training Kappa      : 0.4193232281613001,Training MF1     : 0.46751948132326715, Training Precision      : 0.5347835475206378, Training Sensitivity      : 0.4770318367528288, Training Specificity      : 0.8816108072745172\n",
            "Validation Results : \n",
            "Validation Loss   : 2.131174511379666, Validation Accuracy : 0.3734259259259259, Validation G-Mean      : 0.5491882165916058\n",
            "Validation Kappa     : 0.1662035589031881, Validation MF1      : 0.296067339071521, Validation Precision      : 0.3732896412412326,  Validation Sensitivity      : 0.3678870512931435, Validation Specificity      : 0.8348553387103257\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :9.466735380667227\n",
            "Class wise sensitivity W: 0.27610485559260406, S1: 0.0, S2: 0.599629176987542, S3: 0.7160493852915587, R: 0.2476518385940128\n",
            "Class wise specificity W: 0.9912990265422397, S1: 0.9943965033248618, S2: 0.40044603027679304, S3: 0.8540325495931838, R: 0.9341025838145504\n",
            "Class wise F1  W: 0.40972860157489777, S1: 0.0, S2: 0.37500697043206954, S3: 0.4023254602043717, R: 0.29327566314626624\n",
            "===========================================================Training Epoch : [72/100] ===========================================================================================================>\n",
            "Epoch: [72/100][0/190]\tTrain_Loss 5.44930 (5.44930)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.68920(0.68920)\tTrain_Kappa 0.50562(0.50562)\tTrain_MF1 0.54962(0.54962)\tTrain_Precision 0.64333(0.64333)\tTrain_Sensitivity 0.53095(0.53095)\tTrain_Specificity 0.89462(0.89462)\tTime 0.138s (0.138s)\tSpeed 231.6 samples/s\tData 0.003s (0.003s)\t\n",
            "Epoch: [72/100][100/190]\tTrain_Loss 4.63882 (4.64839)\tTrain_Acc 0.62500 (0.62345)\tTrain_G-Mean 0.60057(0.64120)\tTrain_Kappa 0.27684(0.41646)\tTrain_MF1 0.36420(0.46391)\tTrain_Precision 0.38116(0.53837)\tTrain_Sensitivity 0.42000(0.47102)\tTrain_Specificity 0.85876(0.88095)\tTime 0.139s (0.140s)\tSpeed 230.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [72/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.712265197854293, Training Accuracy      : 0.6199718045112782, Training G-Mean      : 0.6402051142001499\n",
            "Training Kappa      : 0.41899646570325433,Training MF1     : 0.46364057827936955, Training Precision      : 0.5327917402355294, Training Sensitivity      : 0.46971171206549595, Training Specificity      : 0.8812159971657562\n",
            "Validation Results : \n",
            "Validation Loss   : 2.3647748717555293, Validation Accuracy : 0.38152777777777774, Validation G-Mean      : 0.5555988640397167\n",
            "Validation Kappa     : 0.17903926912517837, Validation MF1      : 0.29501272119857647, Validation Precision      : 0.3938795166986959,  Validation Sensitivity      : 0.37408252750281934, Validation Specificity      : 0.8376034681443814\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :10.513618416256374\n",
            "Class wise sensitivity W: 0.3463558187639272, S1: 0.0, S2: 0.6277311245600382, S3: 0.7487654332761411, R: 0.14756026091399016\n",
            "Class wise specificity W: 0.9868411752912734, S1: 0.9855528076489767, S2: 0.3950085739294688, S3: 0.8465364774068197, R: 0.974078306445369\n",
            "Class wise F1  W: 0.4923250923554103, S1: 0.0, S2: 0.38652946717209286, S3: 0.3846946320048085, R: 0.21151441446057073\n",
            "===========================================================Training Epoch : [73/100] ===========================================================================================================>\n",
            "Epoch: [73/100][0/190]\tTrain_Loss 5.20414 (5.20414)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.67217(0.67217)\tTrain_Kappa 0.37167(0.37167)\tTrain_MF1 0.44715(0.44715)\tTrain_Precision 0.46667(0.46667)\tTrain_Sensitivity 0.51477(0.51477)\tTrain_Specificity 0.87769(0.87769)\tTime 0.139s (0.139s)\tSpeed 230.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [73/100][100/190]\tTrain_Loss 3.94724 (4.51062)\tTrain_Acc 0.62500 (0.62005)\tTrain_G-Mean 0.64786(0.65048)\tTrain_Kappa 0.40741(0.42537)\tTrain_MF1 0.48687(0.47216)\tTrain_Precision 0.57000(0.53387)\tTrain_Sensitivity 0.47917(0.48312)\tTrain_Specificity 0.87593(0.88237)\tTime 0.140s (0.140s)\tSpeed 228.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [73/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.647552755004481, Training Accuracy      : 0.6172697368421053, Training G-Mean      : 0.644777981808214\n",
            "Training Kappa      : 0.4187375955591088,Training MF1     : 0.46519885400408184, Training Precision      : 0.5314507414165297, Training Sensitivity      : 0.4752588762735068, Training Specificity      : 0.8814195040966332\n",
            "Validation Results : \n",
            "Validation Loss   : 1.7686964803271823, Validation Accuracy : 0.45791666666666664, Validation G-Mean      : 0.5832103280815385\n",
            "Validation Kappa     : 0.2644497984045612, Validation MF1      : 0.34904988982059343, Validation Precision      : 0.4162402140321555,  Validation Sensitivity      : 0.4021484622800792, Validation Specificity      : 0.8558510817863323\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :8.066846494321469\n",
            "Class wise sensitivity W: 0.435291001090297, S1: 0.006172839690137793, S2: 0.6942784653769599, S3: 0.6685185211676138, R: 0.20648148407538733\n",
            "Class wise specificity W: 0.9723505929664329, S1: 0.9840013693880152, S2: 0.471060029886387, S3: 0.8963228861490885, R: 0.9555205305417379\n",
            "Class wise F1  W: 0.5682348200568447, S1: 0.009259259259259259, S2: 0.4534449753937898, S3: 0.4356962050552721, R: 0.2786141893378011\n",
            "===========================================================Training Epoch : [74/100] ===========================================================================================================>\n",
            "Epoch: [74/100][0/190]\tTrain_Loss 3.48568 (3.48568)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.73721(0.73721)\tTrain_Kappa 0.52109(0.52109)\tTrain_MF1 0.65222(0.65222)\tTrain_Precision 0.80444(0.80444)\tTrain_Sensitivity 0.60404(0.60404)\tTrain_Specificity 0.89973(0.89973)\tTime 0.139s (0.139s)\tSpeed 230.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [74/100][100/190]\tTrain_Loss 3.77097 (4.62702)\tTrain_Acc 0.81250 (0.61943)\tTrain_G-Mean 0.74168(0.64784)\tTrain_Kappa 0.63567(0.42768)\tTrain_MF1 0.64341(0.46962)\tTrain_Precision 0.75833(0.53090)\tTrain_Sensitivity 0.60000(0.47787)\tTrain_Specificity 0.91683(0.88333)\tTime 0.141s (0.140s)\tSpeed 227.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [74/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.601359671040585, Training Accuracy      : 0.6166353383458647, Training G-Mean      : 0.6423959167812047\n",
            "Training Kappa      : 0.41747687798633737,Training MF1     : 0.46006344307410113, Training Precision      : 0.5219348514864319, Training Sensitivity      : 0.4712828611543302, Training Specificity      : 0.8817223722683759\n",
            "Validation Results : \n",
            "Validation Loss   : 2.990578289385195, Validation Accuracy : 0.2653240740740741, Validation G-Mean      : 0.4843011634705566\n",
            "Validation Kappa     : 0.07417061373841813, Validation MF1      : 0.2019090602243388, Validation Precision      : 0.3157363057136536,  Validation Sensitivity      : 0.294266806156547, Validation Specificity      : 0.8161951084931691\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :14.336056214791757\n",
            "Class wise sensitivity W: 0.2903874993876175, S1: 0.0, S2: 0.3041387699820377, S3: 0.8086419745727822, R: 0.06816578684029756\n",
            "Class wise specificity W: 0.9943518506156074, S1: 0.9973544986159714, S2: 0.4573215500072197, S3: 0.6520161297586229, R: 0.9799315134684244\n",
            "Class wise F1  W: 0.4307134206648226, S1: 0.0, S2: 0.2240473073389795, S3: 0.253025425253091, R: 0.1017591478648009\n",
            "===========================================================Training Epoch : [75/100] ===========================================================================================================>\n",
            "Epoch: [75/100][0/190]\tTrain_Loss 3.76807 (3.76807)\tTrain_Acc 0.78125 (0.78125)\tTrain_G-Mean 0.76384(0.76384)\tTrain_Kappa 0.68362(0.68362)\tTrain_MF1 0.62509(0.62509)\tTrain_Precision 0.66833(0.66833)\tTrain_Sensitivity 0.62000(0.62000)\tTrain_Specificity 0.94104(0.94104)\tTime 0.139s (0.139s)\tSpeed 229.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [75/100][100/190]\tTrain_Loss 4.28801 (4.69849)\tTrain_Acc 0.59375 (0.61912)\tTrain_G-Mean 0.47861(0.65334)\tTrain_Kappa 0.16633(0.42638)\tTrain_MF1 0.32222(0.47493)\tTrain_Precision 0.43913(0.53760)\tTrain_Sensitivity 0.27879(0.48751)\tTrain_Specificity 0.82165(0.88280)\tTime 0.138s (0.141s)\tSpeed 231.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [75/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.631027697262011, Training Accuracy      : 0.6205357142857143, Training G-Mean      : 0.6485621267676833\n",
            "Training Kappa      : 0.42114130873154215,Training MF1     : 0.4687558899741422, Training Precision      : 0.5303494211560797, Training Sensitivity      : 0.481124441921711, Training Specificity      : 0.8818224686541057\n",
            "Validation Results : \n",
            "Validation Loss   : 2.662926775437814, Validation Accuracy : 0.3106481481481481, Validation G-Mean      : 0.5128138394778662\n",
            "Validation Kappa     : 0.11501389463558237, Validation MF1      : 0.23157448569933572, Validation Precision      : 0.3257893158605806,  Validation Sensitivity      : 0.32275087458116036, Validation Specificity      : 0.8256421601330792\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :12.116920117978696\n",
            "Class wise sensitivity W: 0.3515855656177909, S1: 0.0, S2: 0.36797287673861895, S3: 0.8197530883329885, R: 0.07444284221640339\n",
            "Class wise specificity W: 0.9913860802297239, S1: 0.9963759426717405, S2: 0.4473179446326362, S3: 0.7127000203839055, R: 0.9804308127473902\n",
            "Class wise F1  W: 0.4962535522602223, S1: 0.0, S2: 0.26315738867830346, S3: 0.29049743215243023, R: 0.10796405540572272\n",
            "===========================================================Training Epoch : [76/100] ===========================================================================================================>\n",
            "Epoch: [76/100][0/190]\tTrain_Loss 3.37106 (3.37106)\tTrain_Acc 0.81250 (0.81250)\tTrain_G-Mean 0.74074(0.74074)\tTrain_Kappa 0.65217(0.65217)\tTrain_MF1 0.60381(0.60381)\tTrain_Precision 0.63939(0.63939)\tTrain_Sensitivity 0.59000(0.59000)\tTrain_Specificity 0.93000(0.93000)\tTime 0.139s (0.139s)\tSpeed 231.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [76/100][100/190]\tTrain_Loss 3.54732 (4.64517)\tTrain_Acc 0.71875 (0.63026)\tTrain_G-Mean 0.76175(0.64814)\tTrain_Kappa 0.55692(0.42461)\tTrain_MF1 0.57698(0.47030)\tTrain_Precision 0.61000(0.53223)\tTrain_Sensitivity 0.64167(0.47975)\tTrain_Specificity 0.90431(0.88160)\tTime 0.138s (0.140s)\tSpeed 232.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [76/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.716320677807457, Training Accuracy      : 0.6222509398496241, Training G-Mean      : 0.6466265430443026\n",
            "Training Kappa      : 0.42148733852922715,Training MF1     : 0.46720268448716706, Training Precision      : 0.5308131231289165, Training Sensitivity      : 0.4779512198975213, Training Specificity      : 0.8814786488602034\n",
            "Validation Results : \n",
            "Validation Loss   : 2.3157584490599454, Validation Accuracy : 0.39129629629629625, Validation G-Mean      : 0.5267382956388871\n",
            "Validation Kappa     : 0.17453924191595635, Validation MF1      : 0.2788665585495808, Validation Precision      : 0.3714019941786924,  Validation Sensitivity      : 0.3368017962133443, Validation Specificity      : 0.83829238028438\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :10.288488070170084\n",
            "Class wise sensitivity W: 0.34787447309052505, S1: 0.0, S2: 0.6850175118004834, S3: 0.5348324554937857, R: 0.1162845406819273\n",
            "Class wise specificity W: 0.9891460891123172, S1: 0.9871950701430992, S2: 0.3804614659812715, S3: 0.8791868112705372, R: 0.9554724649146751\n",
            "Class wise F1  W: 0.49528062619544844, S1: 0.0, S2: 0.41731948791830625, S3: 0.3216129607624478, R: 0.1601197178717013\n",
            "===========================================================Training Epoch : [77/100] ===========================================================================================================>\n",
            "Epoch: [77/100][0/190]\tTrain_Loss 5.81955 (5.81955)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.66644(0.66644)\tTrain_Kappa 0.45191(0.45191)\tTrain_MF1 0.49524(0.49524)\tTrain_Precision 0.58807(0.58807)\tTrain_Sensitivity 0.49939(0.49939)\tTrain_Specificity 0.88938(0.88938)\tTime 0.138s (0.138s)\tSpeed 231.6 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [77/100][100/190]\tTrain_Loss 4.69548 (4.86442)\tTrain_Acc 0.56250 (0.61355)\tTrain_G-Mean 0.49019(0.64080)\tTrain_Kappa 0.23680(0.41239)\tTrain_MF1 0.28714(0.46414)\tTrain_Precision 0.32619(0.53786)\tTrain_Sensitivity 0.28170(0.47046)\tTrain_Specificity 0.85298(0.87989)\tTime 0.141s (0.140s)\tSpeed 227.0 samples/s\tData 0.004s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [77/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.846386457744398, Training Accuracy      : 0.612218045112782, Training G-Mean      : 0.6389725085374116\n",
            "Training Kappa      : 0.4070683439010262,Training MF1     : 0.4594663936997714, Training Precision      : 0.5264340063772702, Training Sensitivity      : 0.46837210416793834, Training Specificity      : 0.8785709744378141\n",
            "Validation Results : \n",
            "Validation Loss   : 1.9258428502965856, Validation Accuracy : 0.4197222222222222, Validation G-Mean      : 0.5526098509920906\n",
            "Validation Kappa     : 0.21047595304679018, Validation MF1      : 0.3202310275148462, Validation Precision      : 0.43808558252122665,  Validation Sensitivity      : 0.3682669826127864, Validation Specificity      : 0.844376386646871\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :8.390889485677084\n",
            "Class wise sensitivity W: 0.32965728254229937, S1: 0.13240740751778637, S2: 0.7974947911721689, S3: 0.4358024713065889, R: 0.14597296052508885\n",
            "Class wise specificity W: 0.9893673592143588, S1: 0.9434368941518996, S2: 0.3782012070770617, S3: 0.9321230738251297, R: 0.9787533989659062\n",
            "Class wise F1  W: 0.4689895543787215, S1: 0.11290684949468684, S2: 0.4659905919322261, S3: 0.3374485660482336, R: 0.2158195757203632\n",
            "===========================================================Training Epoch : [78/100] ===========================================================================================================>\n",
            "Epoch: [78/100][0/190]\tTrain_Loss 4.57700 (4.57700)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.62734(0.62734)\tTrain_Kappa 0.29020(0.29020)\tTrain_MF1 0.39147(0.39147)\tTrain_Precision 0.37500(0.37500)\tTrain_Sensitivity 0.45895(0.45895)\tTrain_Specificity 0.85751(0.85751)\tTime 0.140s (0.140s)\tSpeed 229.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [78/100][100/190]\tTrain_Loss 6.30511 (4.91796)\tTrain_Acc 0.43750 (0.61448)\tTrain_G-Mean 0.54369(0.64061)\tTrain_Kappa 0.16763(0.40859)\tTrain_MF1 0.30620(0.45973)\tTrain_Precision 0.34091(0.52667)\tTrain_Sensitivity 0.35607(0.47098)\tTrain_Specificity 0.83017(0.87874)\tTime 0.141s (0.141s)\tSpeed 226.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [78/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.916293436602542, Training Accuracy      : 0.6129934210526315, Training G-Mean      : 0.6385584150030917\n",
            "Training Kappa      : 0.4046777086452962,Training MF1     : 0.45625770802560595, Training Precision      : 0.520580040294873, Training Sensitivity      : 0.4683730001747611, Training Specificity      : 0.8779965784047783\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2791432539621987, Validation Accuracy : 0.3223611111111111, Validation G-Mean      : 0.5434829797368251\n",
            "Validation Kappa     : 0.14529125828417874, Validation MF1      : 0.27220135529836015, Validation Precision      : 0.3482767259081204,  Validation Sensitivity      : 0.36353360116481775, Validation Specificity      : 0.8306799060768552\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :10.249435424804688\n",
            "Class wise sensitivity W: 0.28039682822095024, S1: 0.0, S2: 0.25867034688040064, S3: 0.7716049397433246, R: 0.5069958909794137\n",
            "Class wise specificity W: 0.9941578485347606, S1: 1.0, S2: 0.5716587647243783, S3: 0.7429669411094101, R: 0.8446159760157267\n",
            "Class wise F1  W: 0.4228140976693895, S1: 0.0, S2: 0.21220027903715769, S3: 0.2995060152477688, R: 0.4264863845374849\n",
            "===========================================================Training Epoch : [79/100] ===========================================================================================================>\n",
            "Epoch: [79/100][0/190]\tTrain_Loss 5.24898 (5.24898)\tTrain_Acc 0.75000 (0.75000)\tTrain_G-Mean 0.71279(0.71279)\tTrain_Kappa 0.63059(0.63059)\tTrain_MF1 0.58790(0.58790)\tTrain_Precision 0.70351(0.70351)\tTrain_Sensitivity 0.55111(0.55111)\tTrain_Specificity 0.92190(0.92190)\tTime 0.140s (0.140s)\tSpeed 228.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [79/100][100/190]\tTrain_Loss 4.58567 (4.55586)\tTrain_Acc 0.59375 (0.61788)\tTrain_G-Mean 0.69749(0.63635)\tTrain_Kappa 0.35504(0.41557)\tTrain_MF1 0.47943(0.45869)\tTrain_Precision 0.49048(0.52309)\tTrain_Sensitivity 0.56250(0.46333)\tTrain_Specificity 0.86487(0.88075)\tTime 0.139s (0.140s)\tSpeed 229.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [79/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.601493768942984, Training Accuracy      : 0.6144501879699248, Training G-Mean      : 0.6367426129096465\n",
            "Training Kappa      : 0.40940738993063536,Training MF1     : 0.4559517209780841, Training Precision      : 0.5194817228850566, Training Sensitivity      : 0.46453610467283335, Training Specificity      : 0.8793323080633817\n",
            "Validation Results : \n",
            "Validation Loss   : 2.247046015880726, Validation Accuracy : 0.3605555555555555, Validation G-Mean      : 0.542071628995715\n",
            "Validation Kappa     : 0.16957835200141425, Validation MF1      : 0.29427759272080883, Validation Precision      : 0.37886408473606453,  Validation Sensitivity      : 0.3579447899703627, Validation Specificity      : 0.8362956709331937\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :9.89655116752342\n",
            "Class wise sensitivity W: 0.3407864013203868, S1: 0.012345679380275585, S2: 0.43371654826181905, S3: 0.6743386255370246, R: 0.32853669535230706\n",
            "Class wise specificity W: 0.9857789918228432, S1: 0.9842418123174597, S2: 0.5272865736926043, S3: 0.7721483420442652, R: 0.9120226347887957\n",
            "Class wise F1  W: 0.48273405101564193, S1: 0.018518518518518517, S2: 0.3034024034385328, S3: 0.3103113179957425, R: 0.35642167263560826\n",
            "===========================================================Training Epoch : [80/100] ===========================================================================================================>\n",
            "Epoch: [80/100][0/190]\tTrain_Loss 5.99567 (5.99567)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.66314(0.66314)\tTrain_Kappa 0.43949(0.43949)\tTrain_MF1 0.50125(0.50125)\tTrain_Precision 0.52333(0.52333)\tTrain_Sensitivity 0.49804(0.49804)\tTrain_Specificity 0.88296(0.88296)\tTime 0.137s (0.137s)\tSpeed 232.8 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [80/100][100/190]\tTrain_Loss 4.25590 (4.61552)\tTrain_Acc 0.65625 (0.62005)\tTrain_G-Mean 0.65937(0.64041)\tTrain_Kappa 0.48311(0.41114)\tTrain_MF1 0.48544(0.46187)\tTrain_Precision 0.63818(0.52954)\tTrain_Sensitivity 0.48857(0.46960)\tTrain_Specificity 0.88988(0.87927)\tTime 0.141s (0.140s)\tSpeed 227.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [80/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.625007571672138, Training Accuracy      : 0.6127584586466165, Training G-Mean      : 0.6431910938593619\n",
            "Training Kappa      : 0.4100772033516331,Training MF1     : 0.4657443397923522, Training Precision      : 0.5313094018634996, Training Sensitivity      : 0.47383376083091716, Training Specificity      : 0.8791481998405959\n",
            "Validation Results : \n",
            "Validation Loss   : 1.9249020505834509, Validation Accuracy : 0.3875, Validation G-Mean      : 0.5451756211714875\n",
            "Validation Kappa     : 0.19918508255201903, Validation MF1      : 0.3070623437011683, Validation Precision      : 0.37134279890192873,  Validation Sensitivity      : 0.3620954910362209, Validation Specificity      : 0.8429422694223899\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :8.46784370916861\n",
            "Class wise sensitivity W: 0.31514849072253265, S1: 0.0, S2: 0.4925805749716582, S3: 0.6129629634044789, R: 0.3897854260824345\n",
            "Class wise specificity W: 0.9824878374735514, S1: 0.9908238229928193, S2: 0.5434125474205723, S3: 0.8304868914462902, R: 0.8675002477787159\n",
            "Class wise F1  W: 0.4554140551222695, S1: 0.0, S2: 0.35845181152776434, S3: 0.3492719266149733, R: 0.37217392524083454\n",
            "===========================================================Training Epoch : [81/100] ===========================================================================================================>\n",
            "Epoch: [81/100][0/190]\tTrain_Loss 5.86558 (5.86558)\tTrain_Acc 0.71875 (0.71875)\tTrain_G-Mean 0.71825(0.71825)\tTrain_Kappa 0.57771(0.57771)\tTrain_MF1 0.54615(0.54615)\tTrain_Precision 0.60351(0.60351)\tTrain_Sensitivity 0.56571(0.56571)\tTrain_Specificity 0.91190(0.91190)\tTime 0.140s (0.140s)\tSpeed 228.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [81/100][100/190]\tTrain_Loss 5.65723 (4.67345)\tTrain_Acc 0.62500 (0.61665)\tTrain_G-Mean 0.68321(0.64974)\tTrain_Kappa 0.47684(0.42318)\tTrain_MF1 0.51293(0.47477)\tTrain_Precision 0.58611(0.54876)\tTrain_Sensitivity 0.52348(0.48287)\tTrain_Specificity 0.89167(0.88227)\tTime 0.138s (0.140s)\tSpeed 231.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [81/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.53950802276009, Training Accuracy      : 0.6207001879699248, Training G-Mean      : 0.651748803567633\n",
            "Training Kappa      : 0.4252047312427817,Training MF1     : 0.4755062130407285, Training Precision      : 0.5380705872178079, Training Sensitivity      : 0.48526589184215224, Training Specificity      : 0.8824970598440421\n",
            "Validation Results : \n",
            "Validation Loss   : 2.468694527943929, Validation Accuracy : 0.3352777777777778, Validation G-Mean      : 0.5380643696150192\n",
            "Validation Kappa     : 0.14099202636745148, Validation MF1      : 0.27023354575589853, Validation Precision      : 0.34408767422040304,  Validation Sensitivity      : 0.35408688264864463, Validation Specificity      : 0.8307729451744646\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :11.464669386545816\n",
            "Class wise sensitivity W: 0.3489298768065594, S1: 0.0, S2: 0.3619660238424937, S3: 0.8271604952988801, R: 0.23237801729528992\n",
            "Class wise specificity W: 0.9893268413013883, S1: 0.9971125545325102, S2: 0.5037312794614721, S3: 0.752850040241524, R: 0.9108440103354277\n",
            "Class wise F1  W: 0.49934724580358575, S1: 0.0, S2: 0.28205484289813926, S3: 0.31299421577541914, R: 0.2567714243023484\n",
            "===========================================================Training Epoch : [82/100] ===========================================================================================================>\n",
            "Epoch: [82/100][0/190]\tTrain_Loss 4.92491 (4.92491)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.59972(0.59972)\tTrain_Kappa 0.36874(0.36874)\tTrain_MF1 0.41873(0.41873)\tTrain_Precision 0.48381(0.48381)\tTrain_Sensitivity 0.41333(0.41333)\tTrain_Specificity 0.87015(0.87015)\tTime 0.140s (0.140s)\tSpeed 228.4 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [82/100][100/190]\tTrain_Loss 6.52701 (4.55401)\tTrain_Acc 0.53125 (0.63119)\tTrain_G-Mean 0.59806(0.66833)\tTrain_Kappa 0.34516(0.44941)\tTrain_MF1 0.42307(0.49733)\tTrain_Precision 0.46889(0.56161)\tTrain_Sensitivity 0.41212(0.50640)\tTrain_Specificity 0.86789(0.88711)\tTime 0.138s (0.140s)\tSpeed 232.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [82/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.518915710951153, Training Accuracy      : 0.6267387218045113, Training G-Mean      : 0.6592407858703679\n",
            "Training Kappa      : 0.4365133787008636,Training MF1     : 0.4837820187995307, Training Precision      : 0.5438074124173113, Training Sensitivity      : 0.49482990380180525, Training Specificity      : 0.8850661384745645\n",
            "Validation Results : \n",
            "Validation Loss   : 2.492995955325939, Validation Accuracy : 0.3141203703703703, Validation G-Mean      : 0.535008286079184\n",
            "Validation Kappa     : 0.12632543774907562, Validation MF1      : 0.2583145350217819, Validation Precision      : 0.3326693561893922,  Validation Sensitivity      : 0.353532766688753, Validation Specificity      : 0.8267010496722327\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :11.283780963332564\n",
            "Class wise sensitivity W: 0.2945995413594776, S1: 0.0, S2: 0.35536881177513685, S3: 0.8413580258687338, R: 0.27633745444041713\n",
            "Class wise specificity W: 0.980335306238245, S1: 0.9968710078133477, S2: 0.5129662167142939, S3: 0.7224221582765933, R: 0.9209105593186838\n",
            "Class wise F1  W: 0.42255920116548185, S1: 0.0, S2: 0.2588290947454947, S3: 0.2942813441709236, R: 0.31590303502700945\n",
            "===========================================================Training Epoch : [83/100] ===========================================================================================================>\n",
            "Epoch: [83/100][0/190]\tTrain_Loss 3.37223 (3.37223)\tTrain_Acc 0.75000 (0.75000)\tTrain_G-Mean 0.78706(0.78706)\tTrain_Kappa 0.63944(0.63944)\tTrain_MF1 0.66298(0.66298)\tTrain_Precision 0.82294(0.82294)\tTrain_Sensitivity 0.66905(0.66905)\tTrain_Specificity 0.92589(0.92589)\tTime 0.139s (0.139s)\tSpeed 229.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [83/100][100/190]\tTrain_Loss 4.05150 (4.45102)\tTrain_Acc 0.78125 (0.62036)\tTrain_G-Mean 0.75351(0.64627)\tTrain_Kappa 0.65957(0.42073)\tTrain_MF1 0.63480(0.45959)\tTrain_Precision 0.70075(0.52087)\tTrain_Sensitivity 0.61111(0.47726)\tTrain_Specificity 0.92910(0.88213)\tTime 0.139s (0.140s)\tSpeed 230.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [83/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.4709264554475485, Training Accuracy      : 0.6260338345864661, Training G-Mean      : 0.6541343570804614\n",
            "Training Kappa      : 0.43293494635169233,Training MF1     : 0.46989410273338594, Training Precision      : 0.5262639028775068, Training Sensitivity      : 0.48677882709785497, Training Specificity      : 0.8845610656706909\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2165759095439204, Validation Accuracy : 0.34050925925925923, Validation G-Mean      : 0.5549513122991442\n",
            "Validation Kappa     : 0.15455041770290115, Validation MF1      : 0.2904914051846222, Validation Precision      : 0.3868512045454096,  Validation Sensitivity      : 0.37514431311024554, Validation Specificity      : 0.8325389316788429\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :9.799372655374032\n",
            "Class wise sensitivity W: 0.33941228522194755, S1: 0.014814815035572759, S2: 0.3717884032262696, S3: 0.8796296318372091, R: 0.270076430230229\n",
            "Class wise specificity W: 0.9853225129622, S1: 0.9846792574282046, S2: 0.49754613306787276, S3: 0.7618675474767331, R: 0.9332792074592026\n",
            "Class wise F1  W: 0.4878926514475434, S1: 0.022927690435338904, S2: 0.26990223124071405, S3: 0.34021020818639686, R: 0.33152424461311764\n",
            "===========================================================Training Epoch : [84/100] ===========================================================================================================>\n",
            "Epoch: [84/100][0/190]\tTrain_Loss 4.93402 (4.93402)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.57357(0.57357)\tTrain_Kappa 0.37910(0.37910)\tTrain_MF1 0.36675(0.36675)\tTrain_Precision 0.38381(0.38381)\tTrain_Sensitivity 0.37619(0.37619)\tTrain_Specificity 0.87452(0.87452)\tTime 0.143s (0.143s)\tSpeed 223.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [84/100][100/190]\tTrain_Loss 5.18202 (4.50657)\tTrain_Acc 0.62500 (0.60953)\tTrain_G-Mean 0.73259(0.62923)\tTrain_Kappa 0.40094(0.39985)\tTrain_MF1 0.61068(0.44873)\tTrain_Precision 0.61444(0.51944)\tTrain_Sensitivity 0.61111(0.45511)\tTrain_Specificity 0.87821(0.87738)\tTime 0.137s (0.140s)\tSpeed 233.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [84/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.58689536295439, Training Accuracy      : 0.6131578947368421, Training G-Mean      : 0.6366142024967142\n",
            "Training Kappa      : 0.40721818537600474,Training MF1     : 0.456686403672946, Training Precision      : 0.527266000885713, Training Sensitivity      : 0.4650596397330887, Training Specificity      : 0.8786830791360452\n",
            "Validation Results : \n",
            "Validation Loss   : 3.0106323383472584, Validation Accuracy : 0.2529166666666667, Validation G-Mean      : 0.47850177927332765\n",
            "Validation Kappa     : 0.06078391080581725, Validation MF1      : 0.19319874809847942, Validation Precision      : 0.29949433160600847,  Validation Sensitivity      : 0.28634846955537796, Validation Specificity      : 0.8137555959048095\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :13.816278916818124\n",
            "Class wise sensitivity W: 0.2904819534332664, S1: 0.0, S2: 0.25938208401203156, S3: 0.8148148170223942, R: 0.0670634933091976\n",
            "Class wise specificity W: 0.9853876917450516, S1: 0.9975281159083048, S2: 0.48679598834779525, S3: 0.6379073604389474, R: 0.9611588230839482\n",
            "Class wise F1  W: 0.4230913547454057, S1: 0.0, S2: 0.19563775288837928, S3: 0.2467328577681824, R: 0.10053177509042951\n",
            "===========================================================Training Epoch : [85/100] ===========================================================================================================>\n",
            "Epoch: [85/100][0/190]\tTrain_Loss 4.24935 (4.24935)\tTrain_Acc 0.43750 (0.43750)\tTrain_G-Mean 0.64937(0.64937)\tTrain_Kappa 0.20222(0.20222)\tTrain_MF1 0.39335(0.39335)\tTrain_Precision 0.38480(0.38480)\tTrain_Sensitivity 0.50556(0.50556)\tTrain_Specificity 0.83408(0.83408)\tTime 0.140s (0.140s)\tSpeed 228.5 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [85/100][100/190]\tTrain_Loss 2.94835 (4.53803)\tTrain_Acc 0.62500 (0.60675)\tTrain_G-Mean 0.55466(0.64054)\tTrain_Kappa 0.39623(0.40688)\tTrain_MF1 0.38044(0.45082)\tTrain_Precision 0.48333(0.51077)\tTrain_Sensitivity 0.35000(0.47071)\tTrain_Specificity 0.87900(0.87951)\tTime 0.138s (0.140s)\tSpeed 231.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [85/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.5605177791495075, Training Accuracy      : 0.6130639097744361, Training G-Mean      : 0.6425563969663828\n",
            "Training Kappa      : 0.41053006505741363,Training MF1     : 0.45731863890823526, Training Precision      : 0.5190671456801265, Training Sensitivity      : 0.47311582050825374, Training Specificity      : 0.8796573459788372\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8671926657358804, Validation Accuracy : 0.2675, Validation G-Mean      : 0.48164485909970645\n",
            "Validation Kappa     : 0.07486389360948265, Validation MF1      : 0.21058488907637415, Validation Precision      : 0.3052870044001826,  Validation Sensitivity      : 0.2941144384719707, Validation Specificity      : 0.8164992410827566\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :13.414428887543854\n",
            "Class wise sensitivity W: 0.32491382901315335, S1: 0.009259259259259259, S2: 0.25929739629780807, S3: 0.7876543226065459, R: 0.0894473851830871\n",
            "Class wise specificity W: 0.985442046765928, S1: 0.9949160196163036, S2: 0.49046024845706093, S3: 0.6644914426185466, R: 0.9471864479559439\n",
            "Class wise F1  W: 0.4575483004252116, S1: 0.014814815035572759, S2: 0.1962500407739922, S3: 0.2575623172300833, R: 0.12674897191701112\n",
            "===========================================================Training Epoch : [86/100] ===========================================================================================================>\n",
            "Epoch: [86/100][0/190]\tTrain_Loss 5.21530 (5.21530)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.64488(0.64488)\tTrain_Kappa 0.41431(0.41431)\tTrain_MF1 0.44527(0.44527)\tTrain_Precision 0.49789(0.49789)\tTrain_Sensitivity 0.46905(0.46905)\tTrain_Specificity 0.88662(0.88662)\tTime 0.140s (0.140s)\tSpeed 228.9 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [86/100][100/190]\tTrain_Loss 3.34182 (4.50820)\tTrain_Acc 0.71875 (0.62902)\tTrain_G-Mean 0.71374(0.65294)\tTrain_Kappa 0.57270(0.43206)\tTrain_MF1 0.54032(0.47443)\tTrain_Precision 0.59167(0.53412)\tTrain_Sensitivity 0.55417(0.48628)\tTrain_Specificity 0.91925(0.88379)\tTime 0.138s (0.140s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [86/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.52751427951612, Training Accuracy      : 0.6209351503759398, Training G-Mean      : 0.6471990350629138\n",
            "Training Kappa      : 0.422416614572606,Training MF1     : 0.46769373511013257, Training Precision      : 0.5287517360637064, Training Sensitivity      : 0.478712742101205, Training Specificity      : 0.8818377724760458\n",
            "Validation Results : \n",
            "Validation Loss   : 2.8883517494908086, Validation Accuracy : 0.2474537037037037, Validation G-Mean      : 0.4722888048514809\n",
            "Validation Kappa     : 0.04757557090973125, Validation MF1      : 0.19128618411443854, Validation Precision      : 0.2869103187212238,  Validation Sensitivity      : 0.281488657604765, Validation Specificity      : 0.8109793603420257\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :13.105087845413774\n",
            "Class wise sensitivity W: 0.30300471517774796, S1: 0.0, S2: 0.21956202718946669, S3: 0.8104938290737294, R: 0.07438271658288108\n",
            "Class wise specificity W: 0.9873859374611466, S1: 0.9972983576633312, S2: 0.4435324326709465, S3: 0.6571572798269766, R: 0.9695227940877279\n",
            "Class wise F1  W: 0.44406881486928024, S1: 0.0, S2: 0.1673473878591149, S3: 0.24877453678184086, R: 0.09624018106195661\n",
            "===========================================================Training Epoch : [87/100] ===========================================================================================================>\n",
            "Epoch: [87/100][0/190]\tTrain_Loss 3.88827 (3.88827)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.59769(0.59769)\tTrain_Kappa 0.34796(0.34796)\tTrain_MF1 0.45102(0.45102)\tTrain_Precision 0.55000(0.55000)\tTrain_Sensitivity 0.41294(0.41294)\tTrain_Specificity 0.86509(0.86509)\tTime 0.143s (0.143s)\tSpeed 224.5 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [87/100][100/190]\tTrain_Loss 4.42781 (4.83171)\tTrain_Acc 0.56250 (0.60736)\tTrain_G-Mean 0.51579(0.64329)\tTrain_Kappa 0.27154(0.40730)\tTrain_MF1 0.32359(0.46222)\tTrain_Precision 0.38841(0.53522)\tTrain_Sensitivity 0.31214(0.47414)\tTrain_Specificity 0.85229(0.87887)\tTime 0.138s (0.140s)\tSpeed 232.0 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [87/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.627863537637811, Training Accuracy      : 0.6135103383458647, Training G-Mean      : 0.6440147931396798\n",
            "Training Kappa      : 0.41201256507856054,Training MF1     : 0.4638820776657055, Training Precision      : 0.5342951845495325, Training Sensitivity      : 0.47510770589897494, Training Specificity      : 0.8798502844415212\n",
            "Validation Results : \n",
            "Validation Loss   : 3.0796526538001165, Validation Accuracy : 0.26416666666666666, Validation G-Mean      : 0.5078204062704851\n",
            "Validation Kappa     : 0.08691783175191443, Validation MF1      : 0.22103797208379816, Validation Precision      : 0.32949237983535834,  Validation Sensitivity      : 0.3185714942989527, Validation Specificity      : 0.8196166976734441\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :14.829623964097765\n",
            "Class wise sensitivity W: 0.29717130131191677, S1: 0.0, S2: 0.22372849616739485, S3: 0.8796296296296297, R: 0.19232804438582174\n",
            "Class wise specificity W: 0.9914950618037471, S1: 0.9928748519332321, S2: 0.5595161616802216, S3: 0.6232628888554044, R: 0.930934524094617\n",
            "Class wise F1  W: 0.43900371756818557, S1: 0.0, S2: 0.1779680296226784, S3: 0.2519712271513762, R: 0.23624688607675057\n",
            "===========================================================Training Epoch : [88/100] ===========================================================================================================>\n",
            "Epoch: [88/100][0/190]\tTrain_Loss 4.09960 (4.09960)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.64529(0.64529)\tTrain_Kappa 0.44986(0.44986)\tTrain_MF1 0.46818(0.46818)\tTrain_Precision 0.50000(0.50000)\tTrain_Sensitivity 0.46857(0.46857)\tTrain_Specificity 0.88866(0.88866)\tTime 0.139s (0.139s)\tSpeed 229.7 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [88/100][100/190]\tTrain_Loss 4.80561 (4.46975)\tTrain_Acc 0.50000 (0.61448)\tTrain_G-Mean 0.60278(0.64438)\tTrain_Kappa 0.33072(0.41704)\tTrain_MF1 0.39752(0.46025)\tTrain_Precision 0.44667(0.51258)\tTrain_Sensitivity 0.42063(0.47530)\tTrain_Specificity 0.86381(0.88145)\tTime 0.138s (0.140s)\tSpeed 232.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [88/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.5883282121859095, Training Accuracy      : 0.6133693609022556, Training G-Mean      : 0.6394815920381947\n",
            "Training Kappa      : 0.4108749696801873,Training MF1     : 0.4569209937672865, Training Precision      : 0.5132182550587151, Training Sensitivity      : 0.46872873405876914, Training Specificity      : 0.8798697017682225\n",
            "Validation Results : \n",
            "Validation Loss   : 2.245749168925815, Validation Accuracy : 0.3084722222222222, Validation G-Mean      : 0.5143305097024383\n",
            "Validation Kappa     : 0.10435092272932948, Validation MF1      : 0.2460043477239432, Validation Precision      : 0.34660231294455357,  Validation Sensitivity      : 0.32862356001580195, Validation Specificity      : 0.8221398292868225\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :9.897821302767154\n",
            "Class wise sensitivity W: 0.243263421787156, S1: 0.0, S2: 0.42386698529676153, S3: 0.779629632278725, R: 0.19635776071636765\n",
            "Class wise specificity W: 0.9938962305033648, S1: 0.9960773587226868, S2: 0.41645081451645605, S3: 0.7853064846109461, R: 0.9189682580806591\n",
            "Class wise F1  W: 0.3715654683333856, S1: 0.0, S2: 0.2854623802834087, S3: 0.3420692538773572, R: 0.23092463612556458\n",
            "===========================================================Training Epoch : [89/100] ===========================================================================================================>\n",
            "Epoch: [89/100][0/190]\tTrain_Loss 4.48435 (4.48435)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.61641(0.61641)\tTrain_Kappa 0.44023(0.44023)\tTrain_MF1 0.47165(0.47165)\tTrain_Precision 0.71818(0.71818)\tTrain_Sensitivity 0.42889(0.42889)\tTrain_Specificity 0.88591(0.88591)\tTime 0.142s (0.142s)\tSpeed 226.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [89/100][100/190]\tTrain_Loss 4.29368 (4.40588)\tTrain_Acc 0.75000 (0.62252)\tTrain_G-Mean 0.73689(0.65075)\tTrain_Kappa 0.54930(0.42827)\tTrain_MF1 0.60450(0.47538)\tTrain_Precision 0.63810(0.54000)\tTrain_Sensitivity 0.59333(0.48302)\tTrain_Specificity 0.91519(0.88314)\tTime 0.138s (0.140s)\tSpeed 231.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [89/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.464555301164326, Training Accuracy      : 0.6198073308270677, Training G-Mean      : 0.6510117987613833\n",
            "Training Kappa      : 0.42231356290516553,Training MF1     : 0.47510057328562993, Training Precision      : 0.5380996097545875, Training Sensitivity      : 0.48469961120894073, Training Specificity      : 0.8816244197362347\n",
            "Validation Results : \n",
            "Validation Loss   : 3.2471428094086825, Validation Accuracy : 0.2426851851851852, Validation G-Mean      : 0.4681190473484566\n",
            "Validation Kappa     : 0.05411459332092013, Validation MF1      : 0.1823569318762532, Validation Precision      : 0.2976591456267568,  Validation Sensitivity      : 0.27673320764744724, Validation Specificity      : 0.8120080787826467\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :15.741389415882251\n",
            "Class wise sensitivity W: 0.258835819584352, S1: 0.0, S2: 0.2685251020722919, S3: 0.802469136538329, R: 0.053835980042263316\n",
            "Class wise specificity W: 0.9915556333683155, S1: 0.9987228601067154, S2: 0.46642730909365193, S3: 0.6256285077995725, R: 0.9777060835449783\n",
            "Class wise F1  W: 0.3924821140589537, S1: 0.0, S2: 0.19806082160384567, S3: 0.2382896950951329, R: 0.08295202862333369\n",
            "===========================================================Training Epoch : [90/100] ===========================================================================================================>\n",
            "Epoch: [90/100][0/190]\tTrain_Loss 3.55304 (3.55304)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.67071(0.67071)\tTrain_Kappa 0.39932(0.39932)\tTrain_MF1 0.45889(0.45889)\tTrain_Precision 0.52857(0.52857)\tTrain_Sensitivity 0.51504(0.51504)\tTrain_Specificity 0.87344(0.87344)\tTime 0.137s (0.137s)\tSpeed 233.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [90/100][100/190]\tTrain_Loss 4.49002 (4.57252)\tTrain_Acc 0.53125 (0.60829)\tTrain_G-Mean 0.59343(0.63768)\tTrain_Kappa 0.28889(0.40750)\tTrain_MF1 0.44000(0.45786)\tTrain_Precision 0.54333(0.53236)\tTrain_Sensitivity 0.41333(0.46611)\tTrain_Specificity 0.85200(0.87899)\tTime 0.137s (0.140s)\tSpeed 233.5 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [90/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.508657301099677, Training Accuracy      : 0.616329887218045, Training G-Mean      : 0.6433676760001912\n",
            "Training Kappa      : 0.4162319708765288,Training MF1     : 0.4617588931322098, Training Precision      : 0.5331281654928862, Training Sensitivity      : 0.47349866673350294, Training Specificity      : 0.880905937081889\n",
            "Validation Results : \n",
            "Validation Loss   : 2.015210094275298, Validation Accuracy : 0.4109722222222222, Validation G-Mean      : 0.5916870748671765\n",
            "Validation Kappa     : 0.23164951546448656, Validation MF1      : 0.33172610689092563, Validation Precision      : 0.3861676898819429,  Validation Sensitivity      : 0.4179974387089411, Validation Specificity      : 0.8482925529833193\n",
            "Validation T Acc     : 0.8548611111111112, Val_KD_Loss :8.586705013557717\n",
            "Class wise sensitivity W: 0.2844507396221161, S1: 0.0, S2: 0.6165086019922186, S3: 0.790123458261843, R: 0.398904393668528\n",
            "Class wise specificity W: 0.9932379347306711, S1: 0.9960739789185701, S2: 0.5359616654890554, S3: 0.8681736676781265, R: 0.8480155181001734\n",
            "Class wise F1  W: 0.4116499716484988, S1: 0.0, S2: 0.43230882397404424, S3: 0.4441439288633841, R: 0.3705278099687011\n",
            "===========================================================Training Epoch : [91/100] ===========================================================================================================>\n",
            "Epoch: [91/100][0/190]\tTrain_Loss 3.18466 (3.18466)\tTrain_Acc 0.68750 (0.68750)\tTrain_G-Mean 0.69058(0.69058)\tTrain_Kappa 0.51145(0.51145)\tTrain_MF1 0.52828(0.52828)\tTrain_Precision 0.57619(0.57619)\tTrain_Sensitivity 0.53000(0.53000)\tTrain_Specificity 0.89981(0.89981)\tTime 0.140s (0.140s)\tSpeed 229.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [91/100][100/190]\tTrain_Loss 3.09907 (4.26574)\tTrain_Acc 0.68750 (0.64295)\tTrain_G-Mean 0.71885(0.66870)\tTrain_Kappa 0.53890(0.45704)\tTrain_MF1 0.55519(0.49278)\tTrain_Precision 0.55000(0.54941)\tTrain_Sensitivity 0.57000(0.50643)\tTrain_Specificity 0.90656(0.88864)\tTime 0.137s (0.140s)\tSpeed 233.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [91/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.259185481071472, Training Accuracy      : 0.6325892857142857, Training G-Mean      : 0.6582091357506037\n",
            "Training Kappa      : 0.4420024201993571,Training MF1     : 0.4776343420461605, Training Precision      : 0.5348508542776107, Training Sensitivity      : 0.49230800433378485, Training Specificity      : 0.8859084198035686\n",
            "Validation Results : \n",
            "Validation Loss   : 2.661121125574465, Validation Accuracy : 0.27296296296296296, Validation G-Mean      : 0.48879621623149333\n",
            "Validation Kappa     : 0.07879080553763612, Validation MF1      : 0.21634510678273655, Validation Precision      : 0.32259334742471024,  Validation Sensitivity      : 0.29995360871156057, Validation Specificity      : 0.8166764458020528\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :12.021346586721915\n",
            "Class wise sensitivity W: 0.2966922027645288, S1: 0.0, S2: 0.29244973593288, S3: 0.7839506180198105, R: 0.12667548684058366\n",
            "Class wise specificity W: 0.9863568610615201, S1: 0.9947763593108566, S2: 0.464717083507114, S3: 0.688378267818027, R: 0.9491536573127464\n",
            "Class wise F1  W: 0.4342963750715609, S1: 0.0, S2: 0.21227054518681984, S3: 0.2655712018410365, R: 0.1695874118142658\n",
            "===========================================================Training Epoch : [92/100] ===========================================================================================================>\n",
            "Epoch: [92/100][0/190]\tTrain_Loss 4.38987 (4.38987)\tTrain_Acc 0.53125 (0.53125)\tTrain_G-Mean 0.58959(0.58959)\tTrain_Kappa 0.32107(0.32107)\tTrain_MF1 0.34214(0.34214)\tTrain_Precision 0.30526(0.30526)\tTrain_Sensitivity 0.40385(0.40385)\tTrain_Specificity 0.86076(0.86076)\tTime 0.138s (0.138s)\tSpeed 232.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [92/100][100/190]\tTrain_Loss 3.30178 (4.06117)\tTrain_Acc 0.62500 (0.64325)\tTrain_G-Mean 0.71202(0.67532)\tTrain_Kappa 0.42081(0.46524)\tTrain_MF1 0.47100(0.49888)\tTrain_Precision 0.49444(0.54286)\tTrain_Sensitivity 0.57294(0.51522)\tTrain_Specificity 0.88486(0.89077)\tTime 0.135s (0.140s)\tSpeed 236.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [92/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.190365439967105, Training Accuracy      : 0.6365131578947368, Training G-Mean      : 0.6659756593622066\n",
            "Training Kappa      : 0.45319636360936183,Training MF1     : 0.48799952194878915, Training Precision      : 0.5381006511732153, Training Sensitivity      : 0.5025377789767166, Training Specificity      : 0.8886679600414478\n",
            "Validation Results : \n",
            "Validation Loss   : 2.600601726108127, Validation Accuracy : 0.31212962962962965, Validation G-Mean      : 0.5042050985514597\n",
            "Validation Kappa     : 0.10817760670042924, Validation MF1      : 0.23457613488038384, Validation Precision      : 0.32205154305254974,  Validation Sensitivity      : 0.318350088927481, Validation Specificity      : 0.823245668521634\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :11.872591230604383\n",
            "Class wise sensitivity W: 0.28746686075572614, S1: 0.0, S2: 0.48638234701421523, S3: 0.7487654332761411, R: 0.06913580359132201\n",
            "Class wise specificity W: 0.9910613430870904, S1: 0.9838757735711557, S2: 0.39102468280880537, S3: 0.7644095972732261, R: 0.9858569458678916\n",
            "Class wise F1  W: 0.42126333823910467, S1: 0.0, S2: 0.31690328706193854, S3: 0.3268657210800383, R: 0.10784832802083757\n",
            "===========================================================Training Epoch : [93/100] ===========================================================================================================>\n",
            "Epoch: [93/100][0/190]\tTrain_Loss 6.15743 (6.15743)\tTrain_Acc 0.50000 (0.50000)\tTrain_G-Mean 0.64093(0.64093)\tTrain_Kappa 0.29477(0.29477)\tTrain_MF1 0.43873(0.43873)\tTrain_Precision 0.57167(0.57167)\tTrain_Sensitivity 0.47917(0.47917)\tTrain_Specificity 0.85731(0.85731)\tTime 0.139s (0.139s)\tSpeed 230.9 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [93/100][100/190]\tTrain_Loss 3.15287 (4.21464)\tTrain_Acc 0.75000 (0.62748)\tTrain_G-Mean 0.69287(0.66205)\tTrain_Kappa 0.59937(0.44070)\tTrain_MF1 0.54286(0.48930)\tTrain_Precision 0.60952(0.54595)\tTrain_Sensitivity 0.52500(0.49840)\tTrain_Specificity 0.91441(0.88623)\tTime 0.138s (0.140s)\tSpeed 232.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [93/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.204551514826323, Training Accuracy      : 0.6328947368421053, Training G-Mean      : 0.6631601886956615\n",
            "Training Kappa      : 0.4452881400275368,Training MF1     : 0.49075015518226134, Training Precision      : 0.5514703885191363, Training Sensitivity      : 0.49993342345482417, Training Specificity      : 0.8864158239176398\n",
            "Validation Results : \n",
            "Validation Loss   : 2.2325975674170033, Validation Accuracy : 0.32435185185185184, Validation G-Mean      : 0.5156508791271927\n",
            "Validation Kappa     : 0.11839324179300813, Validation MF1      : 0.25509840106522597, Validation Precision      : 0.3565760045691774,  Validation Sensitivity      : 0.32863767262962135, Validation Specificity      : 0.8244107237568606\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :10.06177466003983\n",
            "Class wise sensitivity W: 0.23527635468377006, S1: 0.0, S2: 0.4956839007359964, S3: 0.7148148174639102, R: 0.19741329026442986\n",
            "Class wise specificity W: 0.996178717524917, S1: 0.9957591780909786, S2: 0.3928585714764065, S3: 0.8060113544817324, R: 0.9312457972102695\n",
            "Class wise F1  W: 0.3621719887963048, S1: 0.0, S2: 0.32546928745728954, S3: 0.3410623393676899, R: 0.24678838970484557\n",
            "===========================================================Training Epoch : [94/100] ===========================================================================================================>\n",
            "Epoch: [94/100][0/190]\tTrain_Loss 3.53070 (3.53070)\tTrain_Acc 0.65625 (0.65625)\tTrain_G-Mean 0.60222(0.60222)\tTrain_Kappa 0.39932(0.39932)\tTrain_MF1 0.44004(0.44004)\tTrain_Precision 0.58377(0.58377)\tTrain_Sensitivity 0.41647(0.41647)\tTrain_Specificity 0.87081(0.87081)\tTime 0.141s (0.141s)\tSpeed 227.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [94/100][100/190]\tTrain_Loss 4.57823 (4.50227)\tTrain_Acc 0.56250 (0.61355)\tTrain_G-Mean 0.63090(0.64701)\tTrain_Kappa 0.37079(0.41075)\tTrain_MF1 0.41686(0.46303)\tTrain_Precision 0.42579(0.51811)\tTrain_Sensitivity 0.45667(0.47864)\tTrain_Specificity 0.87160(0.87985)\tTime 0.139s (0.140s)\tSpeed 230.6 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [94/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.3839688326183115, Training Accuracy      : 0.6146851503759398, Training G-Mean      : 0.6454974256292341\n",
            "Training Kappa      : 0.4140359845319094,Training MF1     : 0.46302529570303463, Training Precision      : 0.5190361715461083, Training Sensitivity      : 0.47639130219032894, Training Specificity      : 0.880761703676299\n",
            "Validation Results : \n",
            "Validation Loss   : 3.451199116530242, Validation Accuracy : 0.25939814814814816, Validation G-Mean      : 0.47795094485569123\n",
            "Validation Kappa     : 0.05607975839425941, Validation MF1      : 0.1934538646980568, Validation Precision      : 0.2854038067437984,  Validation Sensitivity      : 0.2895751805769073, Validation Specificity      : 0.8114107037032092\n",
            "Validation T Acc     : 0.8542129629629629, Val_KD_Loss :16.283834210148566\n",
            "Class wise sensitivity W: 0.20421081312276698, S1: 0.0, S2: 0.39049048335463915, S3: 0.7716049397433246, R: 0.08156966666380565\n",
            "Class wise specificity W: 0.9966329954288624, S1: 1.0, S2: 0.37611331211196053, S3: 0.7096968160735236, R: 0.9746103949016995\n",
            "Class wise F1  W: 0.31681553577935256, S1: 0.0, S2: 0.26439706870803126, S3: 0.2763859353683613, R: 0.109670783634539\n",
            "===========================================================Training Epoch : [95/100] ===========================================================================================================>\n",
            "Epoch: [95/100][0/190]\tTrain_Loss 3.50369 (3.50369)\tTrain_Acc 0.71875 (0.71875)\tTrain_G-Mean 0.78527(0.78527)\tTrain_Kappa 0.56627(0.56627)\tTrain_MF1 0.70190(0.70190)\tTrain_Precision 0.80684(0.80684)\tTrain_Sensitivity 0.68250(0.68250)\tTrain_Specificity 0.90351(0.90351)\tTime 0.142s (0.142s)\tSpeed 225.8 samples/s\tData 0.005s (0.005s)\t\n",
            "Epoch: [95/100][100/190]\tTrain_Loss 3.82250 (4.31496)\tTrain_Acc 0.75000 (0.61850)\tTrain_G-Mean 0.71935(0.64593)\tTrain_Kappa 0.60976(0.42013)\tTrain_MF1 0.53658(0.46517)\tTrain_Precision 0.55333(0.53027)\tTrain_Sensitivity 0.55980(0.47708)\tTrain_Specificity 0.92436(0.88172)\tTime 0.139s (0.140s)\tSpeed 230.8 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [95/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.417004565188759, Training Accuracy      : 0.6208646616541353, Training G-Mean      : 0.6486358546872437\n",
            "Training Kappa      : 0.4232454464913449,Training MF1     : 0.4691557469964028, Training Precision      : 0.5358934471717008, Training Sensitivity      : 0.48050095180147595, Training Specificity      : 0.8822005016866489\n",
            "Validation Results : \n",
            "Validation Loss   : 2.786692566341824, Validation Accuracy : 0.25824074074074077, Validation G-Mean      : 0.47850215413402025\n",
            "Validation Kappa     : 0.06453390854663767, Validation MF1      : 0.2062645313916383, Validation Precision      : 0.31636307377506195,  Validation Sensitivity      : 0.29122171390939644, Validation Specificity      : 0.8139233521841192\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :12.690547978436506\n",
            "Class wise sensitivity W: 0.28291467522029523, S1: 0.0, S2: 0.3047020951354945, S3: 0.7716049397433246, R: 0.09688685944786778\n",
            "Class wise specificity W: 0.9963398686161747, S1: 1.0, S2: 0.43182632603027205, S3: 0.6763717311399954, R: 0.965078835134153\n",
            "Class wise F1  W: 0.4137688574415666, S1: 0.0, S2: 0.2151123862023707, S3: 0.2678975595368279, R: 0.13454385377742625\n",
            "===========================================================Training Epoch : [96/100] ===========================================================================================================>\n",
            "Epoch: [96/100][0/190]\tTrain_Loss 6.36927 (6.36927)\tTrain_Acc 0.56250 (0.56250)\tTrain_G-Mean 0.64484(0.64484)\tTrain_Kappa 0.39378(0.39378)\tTrain_MF1 0.45333(0.45333)\tTrain_Precision 0.50526(0.50526)\tTrain_Sensitivity 0.47468(0.47468)\tTrain_Specificity 0.87600(0.87600)\tTime 0.142s (0.142s)\tSpeed 226.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [96/100][100/190]\tTrain_Loss 3.97065 (4.39210)\tTrain_Acc 0.71875 (0.60798)\tTrain_G-Mean 0.63712(0.63961)\tTrain_Kappa 0.52159(0.40129)\tTrain_MF1 0.49051(0.45983)\tTrain_Precision 0.60545(0.52388)\tTrain_Sensitivity 0.45157(0.47010)\tTrain_Specificity 0.89891(0.87769)\tTime 0.136s (0.140s)\tSpeed 235.3 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [96/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.375974763067145, Training Accuracy      : 0.6111842105263158, Training G-Mean      : 0.6415609909489706\n",
            "Training Kappa      : 0.4085438656748953,Training MF1     : 0.46136736918436855, Training Precision      : 0.5238191386116176, Training Sensitivity      : 0.4718719496774049, Training Specificity      : 0.879150334204498\n",
            "Validation Results : \n",
            "Validation Loss   : 2.382623451727408, Validation Accuracy : 0.32935185185185184, Validation G-Mean      : 0.5313480867246179\n",
            "Validation Kappa     : 0.1195098912457311, Validation MF1      : 0.24566394510092554, Validation Precision      : 0.32254816462044367,  Validation Sensitivity      : 0.3498430779134785, Validation Specificity      : 0.825216287208928\n",
            "Validation T Acc     : 0.8561574074074074, Val_KD_Loss :10.372107947314227\n",
            "Class wise sensitivity W: 0.25482715307562437, S1: 0.009259259259259259, S2: 0.6027215675071433, S3: 0.8364197545581393, R: 0.045987655167226436\n",
            "Class wise specificity W: 0.994722220632765, S1: 0.9736691889939485, S2: 0.33903729722455694, S3: 0.8362529101195159, R: 0.9823998190738537\n",
            "Class wise F1  W: 0.3790540165371365, S1: 0.014814815035572759, S2: 0.3707664233666879, S3: 0.39825237073280195, R: 0.06543209983242883\n",
            "===========================================================Training Epoch : [97/100] ===========================================================================================================>\n",
            "Epoch: [97/100][0/190]\tTrain_Loss 3.33471 (3.33471)\tTrain_Acc 0.59375 (0.59375)\tTrain_G-Mean 0.67276(0.67276)\tTrain_Kappa 0.45621(0.45621)\tTrain_MF1 0.48566(0.48566)\tTrain_Precision 0.51556(0.51556)\tTrain_Sensitivity 0.50778(0.50778)\tTrain_Specificity 0.89135(0.89135)\tTime 0.140s (0.140s)\tSpeed 229.0 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [97/100][100/190]\tTrain_Loss 3.91657 (4.13640)\tTrain_Acc 0.59375 (0.62809)\tTrain_G-Mean 0.64681(0.65413)\tTrain_Kappa 0.38370(0.43376)\tTrain_MF1 0.46340(0.47221)\tTrain_Precision 0.45965(0.52884)\tTrain_Sensitivity 0.47905(0.48770)\tTrain_Specificity 0.87332(0.88434)\tTime 0.137s (0.140s)\tSpeed 233.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [97/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.231298060166209, Training Accuracy      : 0.6295347744360902, Training G-Mean      : 0.6557557749210146\n",
            "Training Kappa      : 0.43830405584957915,Training MF1     : 0.4752846237075956, Training Precision      : 0.5370891373251614, Training Sensitivity      : 0.4895728466463715, Training Specificity      : 0.8854619680737196\n",
            "Validation Results : \n",
            "Validation Loss   : 3.669587152975577, Validation Accuracy : 0.21625, Validation G-Mean      : 0.46898318444240245\n",
            "Validation Kappa     : 0.03313103221748392, Validation MF1      : 0.1617455158520628, Validation Precision      : 0.26447930018658994,  Validation Sensitivity      : 0.27617999820245637, Validation Specificity      : 0.8072436997184047\n",
            "Validation T Acc     : 0.8558333333333334, Val_KD_Loss :17.106530048229075\n",
            "Class wise sensitivity W: 0.2358696544887843, S1: 0.0, S2: 0.228055024864497, S3: 0.8864197554411711, R: 0.030555556217829388\n",
            "Class wise specificity W: 0.9934640526771545, S1: 0.9972983576633312, S2: 0.48688229808100947, S3: 0.5704309377405379, R: 0.9881428524299904\n",
            "Class wise F1  W: 0.3628184124827385, S1: 0.0, S2: 0.1704594393571218, S3: 0.22577306573037748, R: 0.049676661690076195\n",
            "===========================================================Training Epoch : [98/100] ===========================================================================================================>\n",
            "Epoch: [98/100][0/190]\tTrain_Loss 4.02335 (4.02335)\tTrain_Acc 0.46875 (0.46875)\tTrain_G-Mean 0.40511(0.40511)\tTrain_Kappa -0.01873(-0.01873)\tTrain_MF1 0.17468(0.17468)\tTrain_Precision 0.15507(0.15507)\tTrain_Sensitivity 0.20667(0.20667)\tTrain_Specificity 0.79409(0.79409)\tTime 0.141s (0.141s)\tSpeed 227.3 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [98/100][100/190]\tTrain_Loss 6.38892 (4.22759)\tTrain_Acc 0.46875 (0.62593)\tTrain_G-Mean 0.57301(0.65819)\tTrain_Kappa 0.28702(0.43940)\tTrain_MF1 0.35273(0.47695)\tTrain_Precision 0.41905(0.53565)\tTrain_Sensitivity 0.38333(0.49250)\tTrain_Specificity 0.85655(0.88594)\tTime 0.139s (0.140s)\tSpeed 230.4 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [98/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.285791896518908, Training Accuracy      : 0.6205357142857143, Training G-Mean      : 0.6517162899374065\n",
            "Training Kappa      : 0.4256759183786042,Training MF1     : 0.46871507523875483, Training Precision      : 0.5213148143260101, Training Sensitivity      : 0.48484577542857105, Training Specificity      : 0.8830951219169721\n",
            "Validation Results : \n",
            "Validation Loss   : 2.689206083615621, Validation Accuracy : 0.28847222222222224, Validation G-Mean      : 0.4964213931452044\n",
            "Validation Kappa     : 0.0722911321881004, Validation MF1      : 0.2111446346949648, Validation Precision      : 0.3093450707142,  Validation Sensitivity      : 0.30706735871456287, Validation Specificity      : 0.8152131652390514\n",
            "Validation T Acc     : 0.8545370370370371, Val_KD_Loss :12.16280389715124\n",
            "Class wise sensitivity W: 0.24133952927810173, S1: 0.0, S2: 0.4622071396421503, S3: 0.761111111552627, R: 0.07067901309993532\n",
            "Class wise specificity W: 0.9966931210623847, S1: 0.9987228601067154, S2: 0.3486614172105436, S3: 0.7688500660437124, R: 0.9631383617719015\n",
            "Class wise F1  W: 0.3698175930314594, S1: 0.0, S2: 0.28993970735205543, S3: 0.29484888083404964, R: 0.10111699225725951\n",
            "===========================================================Training Epoch : [99/100] ===========================================================================================================>\n",
            "Epoch: [99/100][0/190]\tTrain_Loss 3.67370 (3.67370)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.64643(0.64643)\tTrain_Kappa 0.35678(0.35678)\tTrain_MF1 0.42937(0.42937)\tTrain_Precision 0.46377(0.46377)\tTrain_Sensitivity 0.48314(0.48314)\tTrain_Specificity 0.86490(0.86490)\tTime 0.138s (0.138s)\tSpeed 231.1 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [99/100][100/190]\tTrain_Loss 4.45568 (4.09608)\tTrain_Acc 0.56250 (0.63304)\tTrain_G-Mean 0.65879(0.65303)\tTrain_Kappa 0.38292(0.43762)\tTrain_MF1 0.46035(0.47120)\tTrain_Precision 0.56111(0.52891)\tTrain_Sensitivity 0.49670(0.48538)\tTrain_Specificity 0.87377(0.88604)\tTime 0.136s (0.140s)\tSpeed 235.1 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [99/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.305856387238753, Training Accuracy      : 0.6220864661654135, Training G-Mean      : 0.6512256282846286\n",
            "Training Kappa      : 0.4290594040177866,Training MF1     : 0.4705781046810904, Training Precision      : 0.5289899270471775, Training Sensitivity      : 0.4835497961467818, Training Specificity      : 0.8840472538220253\n",
            "Validation Results : \n",
            "Validation Loss   : 2.229822944711756, Validation Accuracy : 0.3428703703703704, Validation G-Mean      : 0.5491621445987159\n",
            "Validation Kappa     : 0.15774990469549602, Validation MF1      : 0.28693412597532625, Validation Precision      : 0.39068879435459775,  Validation Sensitivity      : 0.36639227442167416, Validation Specificity      : 0.8337036521346481\n",
            "Validation T Acc     : 0.8555092592592592, Val_KD_Loss :9.95136691905834\n",
            "Class wise sensitivity W: 0.2607690716783206, S1: 0.07222222343639091, S2: 0.5093757140415686, S3: 0.8024691387459084, R: 0.1871252242061827\n",
            "Class wise specificity W: 0.9952991450274432, S1: 0.9333962378678499, S2: 0.5141774415969849, S3: 0.7899236038879112, R: 0.9357218322930513\n",
            "Class wise F1  W: 0.39655102182317664, S1: 0.07742504609955682, S2: 0.36314697563648224, S3: 0.3588870658918663, R: 0.23866052042554925\n",
            "===========================================================Training Epoch : [100/100] ===========================================================================================================>\n",
            "Epoch: [100/100][0/190]\tTrain_Loss 4.56793 (4.56793)\tTrain_Acc 0.62500 (0.62500)\tTrain_G-Mean 0.72665(0.72665)\tTrain_Kappa 0.50643(0.50643)\tTrain_MF1 0.57617(0.57617)\tTrain_Precision 0.64000(0.64000)\tTrain_Sensitivity 0.58556(0.58556)\tTrain_Specificity 0.90174(0.90174)\tTime 0.140s (0.140s)\tSpeed 229.2 samples/s\tData 0.004s (0.004s)\t\n",
            "Epoch: [100/100][100/190]\tTrain_Loss 5.97697 (4.59630)\tTrain_Acc 0.46875 (0.61603)\tTrain_G-Mean 0.47613(0.63735)\tTrain_Kappa 0.20584(0.40921)\tTrain_MF1 0.27487(0.45358)\tTrain_Precision 0.33579(0.52253)\tTrain_Sensitivity 0.26889(0.46682)\tTrain_Specificity 0.84309(0.87952)\tTime 0.138s (0.140s)\tSpeed 232.7 samples/s\tData 0.003s (0.003s)\t\n",
            "26\n",
            "===========================================================Epoch : [100/100]  Evaluation ===========================================================================================================>\n",
            "Training Results : \n",
            "Training Loss     : 4.583072555692572, Training Accuracy      : 0.6131343984962405, Training G-Mean      : 0.6395277811563593\n",
            "Training Kappa      : 0.41015203759812535,Training MF1     : 0.45526122510433187, Training Precision      : 0.5232775879376813, Training Sensitivity      : 0.4686666249052475, Training Specificity      : 0.880234541202847\n",
            "Validation Results : \n",
            "Validation Loss   : 3.7581818633609347, Validation Accuracy : 0.2325925925925926, Validation G-Mean      : 0.4709462640293482\n",
            "Validation Kappa     : 0.05833586169225792, Validation MF1      : 0.1686366096690849, Validation Precision      : 0.2577362092832725,  Validation Sensitivity      : 0.2788735051397924, Validation Specificity      : 0.8134051175029191\n",
            "Validation T Acc     : 0.8551851851851852, Val_KD_Loss :18.591439423737704\n",
            "Class wise sensitivity W: 0.30072135091931734, S1: 0.0, S2: 0.19981901237258204, S3: 0.8802469151991384, R: 0.013580247207924171\n",
            "Class wise specificity W: 0.9852497202378733, S1: 0.9987654310685617, S2: 0.5495637235818086, S3: 0.5334467126263512, R: 1.0\n",
            "Class wise F1  W: 0.433792719134578, S1: 0.0, S2: 0.1742824978298611, S3: 0.2121801409456465, R: 0.022927690435338904\n",
            "========================================Finished Training ===========================================\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "best_val_acc = 0\n",
        "best_val_kappa = 0\n",
        "for epoch_idx in range(n_epochs):  # loop over the dataset multiple times\n",
        "    run['train/epoch/learning_Rate'].log(optimizer_s.param_groups[0][\"lr\"]) \n",
        "    Net_s.train()\n",
        "    Net_t.eval()        ### Check whether weights of the teacher gets updated\n",
        "    print(f'===========================================================Training Epoch : [{epoch_idx+1}/{n_epochs}] ===========================================================================================================>')\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    val_losses = AverageMeter()\n",
        "    val_kd_loss = AverageMeter()\n",
        "    \n",
        "    train_accuracy = AverageMeter()\n",
        "    val_accuracy = AverageMeter()\n",
        "    val_teach_accuracy = AverageMeter()\n",
        "\n",
        "    train_sensitivity = AverageMeter()\n",
        "    val_sensitivity = AverageMeter()\n",
        "    \n",
        "    train_specificity = AverageMeter()\n",
        "    val_specificity = AverageMeter()\n",
        "\n",
        "    train_gmean = AverageMeter()\n",
        "    val_gmean = AverageMeter()\n",
        "\n",
        "    train_kappa = AverageMeter()\n",
        "    val_kappa = AverageMeter()\n",
        "\n",
        "    train_f1_score = AverageMeter()\n",
        "    val_f1_score = AverageMeter()\n",
        "\n",
        "    train_precision = AverageMeter()\n",
        "    val_precision = AverageMeter()\n",
        "\n",
        "    class1_sens = AverageMeter()\n",
        "    class2_sens = AverageMeter()\n",
        "    class3_sens = AverageMeter()\n",
        "    class4_sens = AverageMeter()\n",
        "    class5_sens = AverageMeter()\n",
        "\n",
        "    class1_spec = AverageMeter()\n",
        "    class2_spec = AverageMeter()\n",
        "    class3_spec = AverageMeter()\n",
        "    class4_spec = AverageMeter()\n",
        "    class5_spec = AverageMeter()\n",
        "\n",
        "    class1_f1 = AverageMeter()\n",
        "    class2_f1 = AverageMeter()\n",
        "    class3_f1 = AverageMeter()\n",
        "    class4_f1 = AverageMeter()\n",
        "    class5_f1 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for batch_idx, data_input in enumerate(train_data_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        data_time.update(time.time() - end)\n",
        "        psg, labels = data_input\n",
        "        sig1 = psg[:,:,0,:]# L-R\n",
        "        sig2 = psg[:,:,1,:]# L\n",
        "        sig3 = psg[:,:,2,:]# R\n",
        "        sig4 = psg[:,:,3,:]# c3-01\n",
        "        sig5 = psg[:,:,4,:]# c4-o2\n",
        "        sig6 = psg[:,:,5,:]# eog\n",
        "        cur_batch_size = len(sig1)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "          targets,_,_ = Net_t(sig4.float().to(device), sig5.float().to(device), sig6.float().to(device),finetune = True)\n",
        "        \n",
        "        optimizer_s.zero_grad()\n",
        "        outputs,_,_ = Net_s(sig1.float().to(device), sig2.float().to(device), sig3.float().to(device),finetune = True)\n",
        "        # print(outputs.shape, targets.shape)\n",
        "\n",
        "        \n",
        "        loss =  criterion_ce(outputs.cpu(),labels) + criterion_mse(outputs.cpu(), targets.cpu())\n",
        "        \n",
        "        \n",
        "        # loss = loss_function(outputs.cpu(), targets.cpu(),weights)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer_s.step()\n",
        "        # scheduler.step()\n",
        "        \n",
        "        losses.update(loss.data.item())\n",
        "        train_accuracy.update(accuracy(outputs.cpu(), labels))\n",
        "\n",
        "        _,_,_,_,sens,spec,f1, prec = confusion_matrix(outputs.cpu(), labels, 5, cur_batch_size)\n",
        "        train_sensitivity.update(sens)\n",
        "        train_specificity.update(spec)\n",
        "        train_f1_score.update(f1)\n",
        "        train_precision.update(prec)\n",
        "        train_gmean.update(g_mean(sens, spec))\n",
        "        train_kappa.update(kappa(outputs.cpu(), labels))\n",
        "        # print(outputs.shape, labels.shape)\n",
        "\n",
        "        run['train/epoch/batch_loss'].log(losses.val)     #1\n",
        "        run['train/epoch/batch_accuracy'].log(train_accuracy.val)\n",
        "        run['epoch'].log(epoch_idx)\n",
        "        \n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            \n",
        "            msg = 'Epoch: [{0}/{3}][{1}/{2}]\\t' \\\n",
        "                  'Train_Loss {loss.val:.5f} ({loss.avg:.5f})\\t'\\\n",
        "                  'Train_Acc {train_acc.val:.5f} ({train_acc.avg:.5f})\\t'\\\n",
        "                  'Train_G-Mean {train_gmean.val:.5f}({train_gmean.avg:.5f})\\t'\\\n",
        "                  'Train_Kappa {train_kap.val:.5f}({train_kap.avg:.5f})\\t'\\\n",
        "                  'Train_MF1 {train_mf1.val:.5f}({train_mf1.avg:.5f})\\t'\\\n",
        "                  'Train_Precision {train_prec.val:.5f}({train_prec.avg:.5f})\\t'\\\n",
        "                  'Train_Sensitivity {train_sens.val:.5f}({train_sens.avg:.5f})\\t'\\\n",
        "                  'Train_Specificity {train_spec.val:.5f}({train_spec.avg:.5f})\\t'\\\n",
        "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t' \\\n",
        "                  'Speed {speed:.1f} samples/s\\t' \\\n",
        "                  'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t'.format(\n",
        "                      epoch_idx+1, batch_idx, len(train_data_loader),n_epochs, batch_time=batch_time,\n",
        "                      speed=data_input[0].size(0)/batch_time.val,\n",
        "                      data_time=data_time, loss=losses, train_acc = train_accuracy,\n",
        "                      train_sens =train_sensitivity, train_spec = train_specificity, train_gmean = train_gmean,\n",
        "                      train_kap = train_kappa, train_mf1 = train_f1_score, train_prec = train_precision)\n",
        "            print(msg)\n",
        "\n",
        "\n",
        "    #evaluation\n",
        "    with torch.no_grad():\n",
        "      Net_s.eval()\n",
        "      Net_t.eval()\n",
        "      for batch_val_idx, data_val in enumerate(val_data_loader):\n",
        "        val_psg, val_labels = data_val\n",
        "        val_sig1 = val_psg[:,:,0,:]\n",
        "        val_sig2 = val_psg[:,:,1,:]\n",
        "        val_sig3 = val_psg[:,:,2,:]\n",
        "        cur_val_batch_size = len(val_sig1)\n",
        "        pred,_,_ = Net_s(val_sig1.float().to(device), val_sig2.float().to(device), val_sig3.float().to(device))\n",
        "        val_loss = criterion_ce(pred.cpu(), val_labels)#.to(device))\n",
        "        val_losses.update(val_loss.data.item())\n",
        "        val_accuracy.update(accuracy(pred.cpu(), val_labels))\n",
        "\n",
        "        sens_list,spec_list,f1_list,prec_list, sens,spec,f1,prec = confusion_matrix(pred.cpu(), val_labels,  5, cur_val_batch_size)\n",
        "        val_sensitivity.update(sens)\n",
        "        val_specificity.update(spec)\n",
        "        val_f1_score.update(f1)\n",
        "        val_precision.update(prec)\n",
        "        val_gmean.update(g_mean(sens, spec))\n",
        "        val_kappa.update(kappa(pred.cpu(), val_labels))\n",
        "\n",
        "\n",
        "\n",
        "        class1_sens.update(sens_list[0])\n",
        "        class2_sens.update(sens_list[1])\n",
        "        class3_sens.update(sens_list[2])\n",
        "        class4_sens.update(sens_list[3])\n",
        "        class5_sens.update(sens_list[4])\n",
        "\n",
        "        class1_spec.update(spec_list[0])\n",
        "        class2_spec.update(spec_list[1])\n",
        "        class3_spec.update(spec_list[2])\n",
        "        class4_spec.update(spec_list[3])\n",
        "        class5_spec.update(spec_list[4])\n",
        "\n",
        "        class1_f1.update(f1_list[0])\n",
        "        class2_f1.update(f1_list[1])\n",
        "        class3_f1.update(f1_list[2])\n",
        "        class4_f1.update(f1_list[3])\n",
        "        class5_f1.update(f1_list[4])\n",
        "\n",
        "        val_targets,_,_ = Net_t(val_psg[:,:,3,:].float().to(device), val_psg[:,:,4,:].float().to(device), val_psg[:,:,5,:].float().to(device),finetune = True)\n",
        "        val_teach_accuracy.update(accuracy(val_targets.cpu(), val_labels))\n",
        "        val_kd_loss.update(criterion_mse(pred.cpu(),val_targets.cpu()).data.item())\n",
        "      print(batch_val_idx)\n",
        "\n",
        "     \n",
        "\n",
        "      print(f'===========================================================Epoch : [{epoch_idx+1}/{n_epochs}]  Evaluation ===========================================================================================================>')\n",
        "      print(\"Training Results : \")\n",
        "      print(f\"Training Loss     : {losses.avg}, Training Accuracy      : {train_accuracy.avg}, Training G-Mean      : {train_gmean.avg}\") \n",
        "      print(f\"Training Kappa      : {train_kappa.avg},Training MF1     : {train_f1_score.avg}, Training Precision      : {train_precision.avg}, Training Sensitivity      : {train_sensitivity.avg}, Training Specificity      : {train_specificity.avg}\")\n",
        "      \n",
        "      print(\"Validation Results : \")\n",
        "      print(f\"Validation Loss   : {val_losses.avg}, Validation Accuracy : {val_accuracy.avg}, Validation G-Mean      : {val_gmean.avg}\") \n",
        "      print(f\"Validation Kappa     : {val_kappa.avg}, Validation MF1      : {val_f1_score.avg}, Validation Precision      : {val_precision.avg},  Validation Sensitivity      : {val_sensitivity.avg}, Validation Specificity      : {val_specificity.avg}\")\n",
        "      print(f\"Validation T Acc     : {val_teach_accuracy.avg}, Val_KD_Loss :{val_kd_loss.avg}\")\n",
        "\n",
        "      print(f\"Class wise sensitivity W: {class1_sens.avg}, S1: {class2_sens.avg}, S2: {class3_sens.avg}, S3: {class4_sens.avg}, R: {class5_sens.avg}\")\n",
        "      print(f\"Class wise specificity W: {class1_spec.avg}, S1: {class2_spec.avg}, S2: {class3_spec.avg}, S3: {class4_spec.avg}, R: {class5_spec.avg}\")\n",
        "      print(f\"Class wise F1  W: {class1_f1.avg}, S1: {class2_f1.avg}, S2: {class3_f1.avg}, S3: {class4_f1.avg}, R: {class5_f1.avg}\")\n",
        "\n",
        "      run['train/epoch/epoch_train_loss'].log(losses.avg)\n",
        "      run['train/epoch/epoch_val_loss'].log(val_losses.avg)\n",
        "      run['train/epoch/epoch_val_kd_loss'].log(val_kd_loss.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_accuracy'].log(train_accuracy.avg)\n",
        "      run['train/epoch/epoch_val_accuracy'].log(val_accuracy.avg)\n",
        "      run['train/epoch/epoch_val_teach_accuracy'].log(val_teach_accuracy.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_sensitivity'].log(train_sensitivity.avg)\n",
        "      run['train/epoch/epoch_val_sensitivity'].log(val_sensitivity.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_specificity'].log(train_specificity.avg)\n",
        "      run['train/epoch/epoch_val_specificity'].log(val_specificity.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_G-Mean'].log(train_gmean.avg)\n",
        "      run['train/epoch/epoch_val_G-Mean'].log(val_gmean.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_Kappa'].log(train_kappa.avg)\n",
        "      run['train/epoch/epoch_val_Kappa'].log(val_kappa.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_MF1 Score'].log(train_f1_score.avg)\n",
        "      run['train/epoch/epoch_val_MF1 Score'].log(val_f1_score.avg)\n",
        "\n",
        "      run['train/epoch/epoch_train_Precision'].log(train_precision.avg)\n",
        "      run['train/epoch/epoch_val_Precision'].log(val_precision.avg)\n",
        "\n",
        "      #################################\n",
        "      \n",
        "      run['train/epoch/epoch_val_Class wise sensitivity W'].log(class1_sens.avg)\n",
        "      run['train/epoch/epoch_val_Class wise sensitivity S1'].log(class2_sens.avg)\n",
        "      run['train/epoch/epoch_val_Class wise sensitivity S2'].log(class3_sens.avg)\n",
        "      run['train/epoch/epoch_val_Class wise sensitivity S3'].log(class4_sens.avg)\n",
        "      run['train/epoch/epoch_val_Class wise sensitivity R'].log(class5_sens.avg)\n",
        "\n",
        "      run['train/epoch/epoch_val_Class wise specificity W'].log(class1_spec.avg)\n",
        "      run['train/epoch/epoch_val_Class wise specificity S1'].log(class2_spec.avg)\n",
        "      run['train/epoch/epoch_val_Class wise specificity S2'].log(class3_spec.avg)\n",
        "      run['train/epoch/epoch_val_Class wise specificity S3'].log(class4_spec.avg)\n",
        "      run['train/epoch/epoch_val_Class wise specificity R'].log(class5_spec.avg)\n",
        "\n",
        "      run['train/epoch/epoch_val_Class wise F1 Score W'].log(class1_f1.avg)\n",
        "      run['train/epoch/epoch_val_Class wise F1 Score S1'].log(class2_f1.avg)\n",
        "      run['train/epoch/epoch_val_Class wise F1 Score S2'].log(class3_f1.avg)\n",
        "      run['train/epoch/epoch_val_Class wise F1 Score S3'].log(class4_f1.avg)\n",
        "      run['train/epoch/epoch_val_Class wise F1 Score R'].log(class5_f1.avg)\n",
        "\n",
        "      #if val_accuracy.avg > best_val_acc or (epoch_idx+1)%10==0 or val_kappa.avg > best_val_kappa:\n",
        "      if val_accuracy.avg > best_val_acc or val_kappa.avg > best_val_kappa:\n",
        "          if val_accuracy.avg > best_val_acc:\n",
        "            run['model/bestmodel_acc'].log(epoch_idx+1)\n",
        "            best_val_acc = val_accuracy.avg\n",
        "            print(\"================================================================================================\")\n",
        "            print(\"                                          Saving Best Model (ACC)                                     \")\n",
        "            print(\"================================================================================================\")\n",
        "            torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_acc.pth.tar')\n",
        "\n",
        "          if val_kappa.avg > best_val_kappa:\n",
        "            run['model/bestmodel_kappa'].log(epoch_idx+1)\n",
        "            best_val_kappa = val_kappa.avg\n",
        "            print(\"================================================================================================\")\n",
        "            print(\"                                          Saving Best Model (Kappa)                                    \")\n",
        "            print(\"================================================================================================\")\n",
        "            torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_kappa.pth.tar')\n",
        "\n",
        "          run['model/best_acc'].log(val_accuracy.avg)\n",
        "          run['model/best_kappa'].log(val_kappa.avg)\n",
        "          #torch.save(Net_t, f'/content/drive/MyDrive/EarEEG/{experiment}/teacher_checkpoint_model_epoch_{epoch_idx+1}.pth.tar')\n",
        "          #torch.save(Net_s, f'/content/drive/MyDrive/EarEEG/{experiment}/student_checkpoint_model_epoch_{epoch_idx+1}.pth.tar')\n",
        "    # lr_scheduler.step()\n",
        "print('========================================Finished Training ===========================================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLJj1FAUM60d",
        "outputId": "9b4ffeea-88a4-4136-b6a4-165899af99c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished Training\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 126 operations to synchronize with Neptune. Do not kill this process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All 126 operations synced, thanks for waiting!\n"
          ]
        }
      ],
      "source": [
        "print(\"finished Training\")\n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl2xHsiDw02P"
      },
      "source": [
        "# MISC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqZctTn7w02P",
        "outputId": "5be2cf84-9e29-4a6a-d876-7b21a51ac8bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<HDF5 dataset \"pred\": shape (196350, 5), type \"<f4\">"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# h5f = h5py.File('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Sleep_edfx/all_labels_main.h5', 'w')\n",
        "# h5f.create_dataset('label', data= main_all_labels)\n",
        "\n",
        "# h5f = h5py.File('/home/jathu/fyp_g15_sleep_monitoring/Experiments/Sleep_edfx/all_pred_main.h5', 'w')\n",
        "# h5f.create_dataset('pred', data=main_all_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFyPLqf2w02Q",
        "outputId": "b5afc5b7-5d69-4eb7-e6d9-5cd64ae9acc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[350.,  25.,   4.,   3.,   7.],\n",
            "        [ 37.,  61.,  37.,   0.,  14.],\n",
            "        [ 10.,  31., 498.,  30.,  19.],\n",
            "        [  6.,   1.,  50., 265.,   0.],\n",
            "        [ 22.,  45.,  34.,   1., 358.]])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.8029350104821803\n",
            "Kappa 0.7442550718851264\n",
            "Macro F1 Score 0.752279645204544\n",
            "G Mean 0.8478154799859162\n",
            "Sensitivity 0.7565811276435852\n",
            "Specificity 0.9500515699386597\n",
            "Class wise F1 Score [0.859950840473175, 0.3910256326198578, 0.8224607706069946, 0.8534621596336365, 0.8344988226890564]\n",
            "Class wise Sensitivity Score [0.859950840473175, 0.3910256326198578, 0.8224607706069946, 0.8534621596336365, 0.8344988226890564]\n",
            "Class wise Specificity Score [0.9737019538879395, 0.9495701789855957, 0.929961085319519, 0.9645742774009705, 0.9324503540992737]\n"
          ]
        }
      ],
      "source": [
        "#5 Class\n",
        "sens_l,spec_l,f1_l,prec_l, sens,spec,f1,prec = confusion_matrix(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels),\n",
        "                                                5, labels_val_main.shape[0], print_conf_mat=True)\n",
        "\n",
        "g = g_mean(sens, spec)\n",
        "\n",
        "acc = accuracy(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels))\n",
        "\n",
        "kap = kappa(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels))\n",
        "\n",
        "print(f\"Accuracy {acc}\")\n",
        "print(f\"Kappa {kap}\")\n",
        "print(f\"Macro F1 Score {f1}\")\n",
        "print(f\"G Mean {g}\")\n",
        "print(f\"Sensitivity {sens}\")\n",
        "print(f\"Specificity {spec}\")\n",
        "print(f\"Class wise F1 Score {f1_l}\")\n",
        "print(f\"Class wise Sensitivity Score {f1_l}\")\n",
        "print(f\"Class wise Specificity Score {spec_l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zo1w2WCw02Q",
        "outputId": "96992f4c-2104-46de-d4e6-229791f23791"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2835729725.py, line 6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_32732/2835729725.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n",
            "\n",
            "\u001b[0;31m    if predss\u001b[0m\n",
            "\n",
            "\u001b[0m             ^\u001b[0m\n",
            "\n",
            "\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#3 Class\n",
        "label_3 = []\n",
        "pred_3 = [] \n",
        "for i in range (main_all_pred.shape[0]):\n",
        "    predss = np.argmax(main_all_pred[i], 1)\n",
        "    if predss:\n",
        "\n",
        "sens_l,spec_l,f1_l,prec_l, sens,spec,f1,prec = confusion_matrix(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels),\n",
        "                                                5, labels_val_main.shape[0], print_conf_mat=True)\n",
        "\n",
        "g = g_mean(sens, spec)\n",
        "\n",
        "acc = accuracy(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels))\n",
        "\n",
        "kap = kappa(torch.from_numpy(main_all_pred), torch.from_numpy(main_all_labels))\n",
        "\n",
        "print(f\"Accuracy {acc}\")\n",
        "print(f\"Kappa {kap}\")\n",
        "print(f\"Macro F1 Score {f1}\")\n",
        "print(f\"G Mean {g}\")\n",
        "print(f\"Sensitivity {sens}\")\n",
        "print(f\"Specificity {spec}\")\n",
        "print(f\"Class wise F1 Score {f1_l}\")\n",
        "print(f\"Class wise Sensitivity Score {f1_l}\")\n",
        "print(f\"Class wise Specificity Score {spec_l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykhSGsLQz9Cp"
      },
      "outputs": [],
      "source": [
        "# #for val Data\n",
        "# test_features, test_labels = next(iter(val_data_loader))\n",
        "# print(f\"Feature batch shape: {test_features.size()}\")\n",
        "# print(f\"Labels batch shape: {test_labels.size()}\")\n",
        "# sig = test_features[0].squeeze()\n",
        "# print(sig.shape)\n",
        "# label = test_labels[0]\n",
        "# plt.figure(figsize = (20,5))\n",
        "# plt.plot(sig)\n",
        "# plt.title(f\"Sleep Stage EEG for Class :{label}\")\n",
        "# plt.show()\n",
        "# print(f\"Label: {label}\")\n",
        "# print(f\"Minimum :{sig.min()}\")\n",
        "# print(f\"Maximum :{sig.max()}\")\n",
        "# print(f\"Mean :{torch.mean(sig)}\")\n",
        "# print(f\"Standard Deviation :{torch.std(sig)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmsfHNsbz9Cq"
      },
      "outputs": [],
      "source": [
        "# d_num = 0\n",
        "# input = torch.reshape(test_features,(batch_size,1,3000)).to(device)\n",
        "# sig = test_features[d_num].squeeze()\n",
        "# print(input.shape)\n",
        "\n",
        "\n",
        "# pred,enc1,atten_feat = test_model(input.float())\n",
        "\n",
        "# atten = atten_feat[d_num].squeeze()\n",
        "# enc1 = enc1[d_num].squeeze()\n",
        "\n",
        "# # atten = torch.softmax(atten, dim=-1)\n",
        "# # enc1 =  torch.softmax(enc1, dim=-1)\n",
        "# print(atten.shape, enc1.shape)\n",
        "\n",
        "# atten = torch.sum(atten, dim = 0)\n",
        "# enc1 = torch.sum(enc1, dim = 0)\n",
        "# print(atten.shape, enc1.shape)\n",
        "\n",
        "# atten = atten.detach().cpu().numpy()\n",
        "# enc1 = enc1.detach().cpu().numpy()\n",
        "# # print(atten)\n",
        "\n",
        "# atten_new = np.zeros((3000,))\n",
        "# enc1_new = np.zeros((3000,))\n",
        "# for i in range(len(atten)):\n",
        "#   atten_new[i*25:i*25+25] = atten[i]\n",
        "#   enc1_new[i*25:i*25+25] = enc1[i]\n",
        "# print(atten_new.shape, enc1_new.shape)\n",
        "# atten_new = atten_new / atten.max()\n",
        "# enc1_new = enc1_new / atten.max()\n",
        "\n",
        "# plt.figure(figsize = (20,10))\n",
        "# plt.plot(sig, label = \"sig\")\n",
        "# plt.plot(atten_new+2, label = \"Attention layer output\")\n",
        "# plt.plot(enc1_new*10-2, label = \"Encoder output\")\n",
        "# plt.axhline(2, color='black')\n",
        "# plt.axhline(0, color='black')\n",
        "# plt.axhline(-2, color='black')\n",
        "# plt.legend()\n",
        "# plt.title(f\"Sleep Stage EEG for Class :{label}  (before softmax)\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMcLZWKVz9Cq"
      },
      "outputs": [],
      "source": [
        "# #With softmax\n",
        "# d_num = 0\n",
        "# input = torch.reshape(test_features,(batch_size,1,3000)).to(device)\n",
        "# sig = test_features[d_num].squeeze()\n",
        "# print(input.shape)\n",
        "\n",
        "\n",
        "# pred,enc1,atten_feat = test_model(input.float())\n",
        "# pred = pred[d_num].squeeze()\n",
        "# pred =  torch.softmax(pred, dim=-1)\n",
        "# pred = torch.argmax(pred)\n",
        "# print(f\"Prediction :{pred}\")\n",
        "# atten = atten_feat[d_num].squeeze()\n",
        "# enc1 = enc1[d_num].squeeze()\n",
        "\n",
        "# atten = torch.softmax(atten, dim=-1)\n",
        "# enc1 =  torch.softmax(enc1, dim=-1)\n",
        "# print(atten.shape, enc1.shape)\n",
        "\n",
        "# atten = torch.sum(atten, dim = 0)\n",
        "# enc1 = torch.sum(enc1, dim = 0)\n",
        "# print(atten.shape, enc1.shape)\n",
        "\n",
        "# atten = atten.detach().cpu().numpy()\n",
        "# enc1 = enc1.detach().cpu().numpy()\n",
        "# # print(atten)\n",
        "\n",
        "# atten_new = np.zeros((3000,))\n",
        "# enc1_new = np.zeros((3000,))\n",
        "# for i in range(len(atten)):\n",
        "#   atten_new[i*25:i*25+25] = atten[i]\n",
        "#   enc1_new[i*25:i*25+25] = enc1[i]\n",
        "# print(atten_new.shape, enc1_new.shape)\n",
        "# atten_new = atten_new / atten.max()\n",
        "# enc1_new = enc1_new / atten.max()\n",
        "\n",
        "# plt.figure(figsize = (20,10))\n",
        "# plt.plot(sig, label = \"sig\")\n",
        "# plt.plot(atten_new*2+2, label = \"Attention layer output\")\n",
        "# plt.plot(enc1_new*10-3, label = \"Encoder output\")\n",
        "# plt.axhline(2, color='black')\n",
        "# plt.axhline(0, color='black')\n",
        "# plt.axhline(-2, color='black')\n",
        "# plt.legend()\n",
        "# plt.title(f\"Sleep Stage EEG for Class :{label} (with softmax)\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QF67O7Uz9Cr"
      },
      "outputs": [],
      "source": [
        "# class EarEEG_MultiChan_Dataset(Dataset):\n",
        "#     def __init__(self, psg_file, label_file, device, mean_l = None, sd_l = None,\n",
        "#                 accept_L = None,accept_R = None, transform=None, target_transform=None, sub_wise_norm = False):\n",
        "#         \"\"\"\n",
        "      \n",
        "#         \"\"\"\n",
        "#         # Get the data\n",
        "#         for i in range(len(psg_file)):\n",
        "#           if i == 0:\n",
        "#             if accept_L.all():\n",
        "#               psg_signal = read_h5py(psg_file[i])\n",
        "#               ear_eeg_L = psg_signal[accept_L[i]]\n",
        "#               ear_eeg_R = psg_signal[accept_R[i]]\n",
        "#               ear_eeg_avg = np.mean(ear_eeg_L,0)-np.mean(ear_eeg_R,0)\n",
        "#               ear_eeg_avg = np.reshape(ear_eeg_avg,(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[1]))\n",
        "#               eog =  np.reshape(psg_signal[16] - psg_signal[4],(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[-1]))\n",
        "#               emg = np.reshape(psg_signal[22],(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[-1]))\n",
        "#               self.psg = np.concatenate((ear_eeg_avg,eog,emg),axis = 1) \n",
        "#               self.labels = read_h5py(label_file[i])\n",
        "#             else:  \n",
        "#               self.psg = read_h5py(psg_file[i])\n",
        "#               self.labels = read_h5py(label_file[i])\n",
        "#           else:\n",
        "#             if accept_L.all():\n",
        "#               psg_signal = read_h5py(psg_file[i])\n",
        "#               ear_eeg_L = psg_signal[accept_L[i]]\n",
        "#               ear_eeg_R = psg_signal[accept_R[i]]\n",
        "#               ear_eeg_avg = np.mean(ear_eeg_L,0)-np.mean(ear_eeg_R,0)\n",
        "#               ear_eeg_avg = np.reshape(ear_eeg_avg,(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[1]))\n",
        "#               eog =  np.reshape(psg_signal[16] - psg_signal[4],(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[-1]))\n",
        "#               emg = np.reshape(psg_signal[22],(ear_eeg_avg.shape[0],1,ear_eeg_avg.shape[-1]))\n",
        "#               psg_comb = np.concatenate((ear_eeg_avg,eog,emg),axis = 1) \n",
        "#               self.psg = np.concatenate((self.psg,psg_comb),axis = 0)\n",
        "#               self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "#             else:\n",
        "#               self.psg = np.concatenate((self.psg, read_h5py(psg_file[i])),axis = 1)\n",
        "#               self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "#         self.labels = torch.from_numpy(self.labels)\n",
        "#         print(f\"Data shape : {self.psg.shape}\")\n",
        "#         print(f\"Labels shape : {self.labels.shape}\")\n",
        "\n",
        "#         if sub_wise_norm == True:\n",
        "#           print(f\"Reading Subject wise mean and sd\")\n",
        "#           for i in range(len(mean_l)):\n",
        "#             if i == 0:\n",
        "#               self.mean_l  = read_h5py(mean_l[i])\n",
        "#               self.sd_l = read_h5py(sd_l[i])\n",
        "#             else:\n",
        "#               self.mean_l = np.concatenate((self.mean_l, read_h5py(mean_l[i])),axis = 1)\n",
        "#               self.sd_l = np.concatenate((self.sd_l, read_h5py(sd_l[i])),axis = 1)\n",
        "\n",
        "#           print(f\"Shapes of Mean  : {self.mean_l.shape}\")\n",
        "#           print(f\"Shapes of Sd    : {self.sd_l.shape}\")\n",
        "#         else:     \n",
        "#           self.mean = mean_l\n",
        "#           self.sd = sd_l\n",
        "#           print(f\"Mean : {self.mean} and SD {self.sd}\")  \n",
        "\n",
        "#         self.sub_wise_norm = sub_wise_norm\n",
        "#         self.device = device\n",
        "#         self.transform = transform\n",
        "#         self.target_transform = target_transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         psg_data = self.psg[idx]         \n",
        "#         # print(data.shape)\n",
        "#         label = self.labels[idx,]\n",
        "        \n",
        "#         if self.sub_wise_norm ==True:\n",
        "#           psg_data = (psg_data - self.mean_l[idx]) / self.sd_l[idx]\n",
        "#         elif self.mean and self.sd:\n",
        "#           psg_data = (psg_data - self.mean_l) / self.sd_l\n",
        "\n",
        "#         if self.transform:\n",
        "#             psg_data = self.transform(psg_data)\n",
        "#         if self.target_transform:\n",
        "#             label = self.target_transform(label)\n",
        "#         return psg_data, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECE_Y6U2w02R"
      },
      "outputs": [],
      "source": [
        "# left_ear = [0,1,2,7,8,9]\n",
        "# right_ear = [3,4,5,6,10,11]\n",
        "# map = [5,6,7,8,9,10,11,17,18,19,20,21]\n",
        "# accept_left_ear = []\n",
        "# accept_right_ear = []\n",
        "\n",
        "# for i in range(len(rejection_list)):\n",
        "#     l_temp = []\n",
        "#     r_temp = []\n",
        "#     for j in range(len(rejection_list[i])):\n",
        "#         if j in left_ear and rejection_list[i][j] == True:\n",
        "#             l_temp.append(map[j])\n",
        "#         if j in right_ear and rejection_list[i][j] == True:\n",
        "#             r_temp.append(map[j])\n",
        "    \n",
        "#     accept_left_ear.append(l_temp)\n",
        "#     accept_right_ear.append(r_temp)\n",
        "\n",
        "# print(accept_left_ear)\n",
        "# print(accept_right_ear)\n",
        "\n",
        "# [train_L_acc_list, val_L_acc_list] = split_data(accept_left_ear,train_data_list,val_data_list)\n",
        "# print(train_L_acc_list)\n",
        "# print(val_L_acc_list)\n",
        "\n",
        "# [train_R_acc_list, val_R_acc_list] = split_data(accept_right_ear,train_data_list,val_data_list)\n",
        "# print(train_R_acc_list)\n",
        "# print(val_R_acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyl7C96aw02R"
      },
      "outputs": [],
      "source": [
        "# class Transformer_Enc_Layer(Module):\n",
        "#     r\"\"\"\n",
        "#     Args:\n",
        "#         d_model: the number of expected features in the input (required).\n",
        "#         nhead: the number of heads in the multiheadattention models (required).\n",
        "#         dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "#         dropout: the dropout value (default=0.1).\n",
        "#         activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "#         layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
        "#         batch_first: If ``True``, then the input and output tensors are provided\n",
        "#             as (batch, seq, feature). Default: ``False``.\n",
        "\n",
        "#     Examples::\n",
        "#         >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "#         >>> src = torch.rand(10, 32, 512)\n",
        "#         >>> out = encoder_layer(src)\n",
        "\n",
        "#     Alternatively, when ``batch_first`` is ``True``:\n",
        "#         >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
        "#         >>> src = torch.rand(32, 10, 512)\n",
        "#         >>> out = encoder_layer(src)\n",
        "#     \"\"\"\n",
        "#     __constants__ = ['batch_first']\n",
        "\n",
        "#     def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "#                  layer_norm_eps=1e-5, batch_first=False,\n",
        "#                  device=None, dtype=None) -> None:\n",
        "#         factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "#         super(Transformer_Enc_Layer, self).__init__()\n",
        "#         self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "#                                             **factory_kwargs)\n",
        "#         # Implementation of Feedforward model\n",
        "#         self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "#         self.dropout = Dropout(dropout)\n",
        "#         self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "\n",
        "#         self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "#         self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "#         self.dropout1 = Dropout(dropout)\n",
        "#         self.dropout2 = Dropout(dropout)\n",
        "\n",
        "#         self.activation = _get_activation_fn(activation)\n",
        "\n",
        "#     def __setstate__(self, state):\n",
        "#         if 'activation' not in state:\n",
        "#             state['activation'] = F.relu\n",
        "#         super(Transformer_Enc_Layer, self).__setstate__(state)\n",
        "\n",
        "#     def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "#         r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "#         Args:\n",
        "#             src: the sequence to the encoder layer (required).\n",
        "#             src_mask: the mask for the src sequence (optional).\n",
        "#             src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "\n",
        "#         Shape:\n",
        "#             see the docs in Transformer class.\n",
        "#         \"\"\"\n",
        "#         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "#                               key_padding_mask=src_key_padding_mask)[0]\n",
        "#         atten_feat = src2\n",
        "#         src = src + self.dropout1(src2)\n",
        "#         src = self.norm1(src)\n",
        "#         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "#         src = src + self.dropout2(src2)\n",
        "#         src = self.norm2(src)\n",
        "#         return src, atten_feat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXFR5W63w02R"
      },
      "outputs": [],
      "source": [
        "# class Cross_Transformer_Enc_Layer(Module):\n",
        "#     r\"\"\"\n",
        "#     Args:\n",
        "#         d_model: the number of expected features in the input (required).\n",
        "#         nhead: the number of heads in the multiheadattention models (required).\n",
        "#         dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "#         dropout: the dropout value (default=0.1).\n",
        "#         activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "#         layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
        "#         batch_first: If ``True``, then the input and output tensors are provided\n",
        "#             as (batch, seq, feature). Default: ``False``.\n",
        "\n",
        "#     Examples::\n",
        "#         >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "#         >>> src = torch.rand(10, 32, 512)\n",
        "#         >>> out = encoder_layer(src)\n",
        "\n",
        "#     Alternatively, when ``batch_first`` is ``True``:\n",
        "#         >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
        "#         >>> src = torch.rand(32, 10, 512)\n",
        "#         >>> out = encoder_layer(src)\n",
        "#     \"\"\"\n",
        "#     __constants__ = ['batch_first']\n",
        "\n",
        "#     def __init__(self, d_model, nhead, dim_feedforward=512,seq_length = 30, dropout=0.1, activation=\"relu\",\n",
        "#                  layer_norm_eps=1e-5, batch_first=False,\n",
        "#                  device=None, dtype=None) -> None:\n",
        "#         factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "#         super(Cross_Transformer_Enc_Layer, self).__init__()\n",
        "#           ###### Attention and cross attention block\n",
        "#         self.self_attn_eeg = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "#                                             **factory_kwargs)\n",
        "#         self.linear_eeg = Linear(d_model,1)\n",
        "\n",
        "#         self.self_attn_eog = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "#                                             **factory_kwargs)\n",
        "#         self.linear_eog = Linear(d_model,1)\n",
        "\n",
        "#         self.self_attn_2eeg = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "#                                            **factory_kwargs)\n",
        "#         self.linear_2eeg = Linear(d_model,1)\n",
        "#         ############################# Cross#######################################\n",
        "#         self.self_attn_cross = MultiheadAttention(seq_length, 10, dropout=dropout, batch_first=batch_first,\n",
        "#                                            **factory_kwargs)\n",
        "#         self.linear_eeg2 = Linear(1,d_model)\n",
        "#         self.dropout_eeg = Dropout(dropout)\n",
        "#         self.norm_eeg = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "\n",
        "#         self.linear_eog2 = Linear(1,d_model)\n",
        "#         self.dropout_eog = Dropout(dropout)\n",
        "#         self.norm_eog = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "\n",
        "#         self.linear_2eeg2 = Linear(1,d_model)\n",
        "#         self.dropout_2eeg = Dropout(dropout)\n",
        "#         self.norm_2eeg = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "#         # Implementation of Feedforward model\n",
        "\n",
        "#         self.linear_eeg_ff1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "#         self.dropout_eeg_ff = Dropout(dropout)\n",
        "#         self.linear_eeg_ff2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "#         self.dropout_eeg_ff2 = Dropout(dropout)\n",
        "#         self.norm_eeg_ff = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "\n",
        "\n",
        "#         self.linear_eog_ff1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "#         self.dropout_eog_ff = Dropout(dropout)\n",
        "#         self.linear_eog_ff2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "#         self.dropout_eog_ff2= Dropout(dropout)\n",
        "#         self.norm_eog_ff = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "\n",
        "#         self.linear_2eeg_ff1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "#         self.dropout_2eeg_ff = Dropout(dropout)\n",
        "#         self.linear_2eeg_ff2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "#         self.dropout_2eeg_ff2 = Dropout(dropout)\n",
        "#         self.norm_2eeg_ff = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        \n",
        "#         self.activation = _get_activation_fn(activation)\n",
        "\n",
        "#     def __setstate__(self, state):\n",
        "#         if 'activation' not in state:\n",
        "#             state['activation'] = F.relu\n",
        "#         super(Cross_Transformer_Enc_Layer, self).__setstate__(state)\n",
        "\n",
        "#     def forward(self, src_eeg: Tensor,src_eog: Tensor,src_2eeg: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "#         r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "#         Args:\n",
        "#             src: the sequence to the encoder layer (required).\n",
        "#             src_mask: the mask for the src sequence (optional).\n",
        "#             src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "\n",
        "#         Shape:\n",
        "#             see the docs in Transformer class.\n",
        "#         \"\"\"\n",
        "#         ## Self Attention\n",
        "#         src_eeg_2 = self.self_attn_eeg(src_eeg, src_eeg, src_eeg, attn_mask=src_mask,\n",
        "#                               key_padding_mask=src_key_padding_mask)[0]\n",
        "#         # print(f\"EEG befor self attention linear: {src_eeg_2.shape}\")\n",
        "#         src_eeg_2 = self.linear_eeg(src_eeg_2)\n",
        "#         # print(f\"EEG after self attention: {src_eeg_2.shape}\")\n",
        "\n",
        "#         src_eog_2 = self.self_attn_eog(src_eog, src_eog, src_eog, attn_mask=src_mask,\n",
        "#                               key_padding_mask=src_key_padding_mask)[0]\n",
        "#         src_eog_2 = self.linear_eog(src_eog_2)\n",
        "#         # print(f\"EOG after self attention: {src_eog_2.shape}\")\n",
        "        \n",
        "#         src_2eeg_2 = self.self_attn_2eeg(src_2eeg, src_2eeg, src_2eeg, attn_mask=src_mask,\n",
        "#                               key_padding_mask=src_key_padding_mask)[0]\n",
        "#         src_2eeg_2 = self.linear_2eeg(src_2eeg_2)\n",
        "#         # print(f\"EMG after self attention: {src_emg_2.shape}\")\n",
        "\n",
        "#         ## Cross Attention\n",
        "#         cross_in = torch.cat((src_eeg_2,src_eog_2,src_2eeg_2), dim = 2)\n",
        "#         cross_in = torch.moveaxis(cross_in,-1,1)\n",
        "#         # print(f\"Cross Input Shape: {cross_in.shape}\")\n",
        "#         cross_out = self.self_attn_cross(cross_in, cross_in, cross_in, attn_mask=src_mask,\n",
        "#                               key_padding_mask=src_key_padding_mask)[0]\n",
        "#         # print(f\"Cross Output Shape: {cross_out.shape}\")\n",
        "\n",
        "#         eeg_2 = self.linear_eeg2(cross_out[:,0,:].view(cross_out.shape[0],cross_out.shape[2],1))\n",
        "#         eeg_2 = src_eeg + self.dropout_eeg(eeg_2)\n",
        "#         eeg_2 = self.norm_eeg(eeg_2)\n",
        "#         # print(f\"EEG after cross attention and linear: {eeg_2.shape}\")\n",
        "\n",
        "#         eog_2 = self.linear_eog2(cross_out[:,1,:].view(cross_out.shape[0],cross_out.shape[2],1))\n",
        "#         eog_2 = src_eog + self.dropout_eog(eog_2)\n",
        "#         eog_2 = self.norm_eog(eog_2)\n",
        "\n",
        "#         eeg2_2 = self.linear_2eeg2(cross_out[:,2,:].view(cross_out.shape[0],cross_out.shape[2],1))\n",
        "#         eeg2_2 = src_2eeg + self.dropout_2eeg(eeg2_2)\n",
        "#         eeg2_2 = self.norm_2eeg(eeg2_2)\n",
        "\n",
        "#         #### Feed forward\n",
        "#         eeg3 = self.linear_eeg_ff2(self.dropout_eeg_ff(self.activation(self.linear_eeg_ff1(eeg_2))))\n",
        "#         eeg3 = eeg_2 + self.dropout_eeg_ff2(eeg3)\n",
        "#         eeg3 = self.norm_eeg_ff(eeg3)\n",
        "#         ## print(f\"EEG after ff: {eeg3.shape}\")\n",
        "\n",
        "#         eog3 = self.linear_eog_ff2(self.dropout_eog_ff(self.activation(self.linear_eog_ff1(eog_2))))\n",
        "#         eog3 = eog_2 + self.dropout_eog_ff2(eog3)\n",
        "#         eog3 = self.norm_eog_ff(eog3)\n",
        "\n",
        "#         eeg2_3 = self.linear_2eeg_ff2(self.dropout_2eeg_ff(self.activation(self.linear_2eeg_ff1(eeg2_2))))\n",
        "#         eeg2_3 = eeg2_2 + self.dropout_2eeg_ff2(eeg2_3)\n",
        "#         eeg2_3 = self.norm_2eeg_ff(eeg2_3)\n",
        "\n",
        "\n",
        "#         return eeg3,eog3,eeg2_3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quJ73YbXw02S"
      },
      "outputs": [],
      "source": [
        "## Original\n",
        "# def EEG_process(data,rejected_list,i=0):\n",
        "#     _,_,_,_,_,ELA,ELE,ELI,ERA,ERG,ERE,ERI,_,_,_,_,_,ELB,ELG,ELK,ERB,ERK,_ = data\n",
        "#     reject = rejected_list[i]\n",
        "#     ear_eeg = [ELA,ELE,ELI,ERA,ERG,ERE,ERI,ELB,ELG,ELK,ERB,ERK]\n",
        "    \n",
        "#     for j in range (len(reject)):\n",
        "#         ear_eeg[j]=ear_eeg[j]*reject[j]\n",
        "    \n",
        "\n",
        "#     Left_ear = (ear_eeg[0] + ear_eeg[1] + ear_eeg[2] + ear_eeg[7] + ear_eeg[8] + ear_eeg[9])/np.count_nonzero([reject[0],reject[1],reject[2],reject[7],reject[8],reject[9]])   # (ELA + ELE + ELI + ELB + ELG + ELK)/6\n",
        "#     Right_ear = (ear_eeg[3] + ear_eeg[4] + ear_eeg[5] + ear_eeg[6] + ear_eeg[10] + ear_eeg[11])/np.count_nonzero([reject[3],reject[4],reject[5],reject[6],reject[10],reject[11]]) # (ERA + ERG + ERE + ERI + ERB + ERK)/6\n",
        "#     L_R = Left_ear - Right_ear\n",
        "\n",
        "#     if np.count_nonzero([reject[0],reject[7]]) != 0 :\n",
        "#         L_E = (ear_eeg[0]+ear_eeg[7])/np.count_nonzero([reject[0],reject[7]]) - (ear_eeg[1]+ear_eeg[2]+ear_eeg[8]+ear_eeg[9])/np.count_nonzero([reject[1],reject[2],reject[8],reject[9]]) #(ELA + ELB)/2 - (ELE + ELI + ELG + ELK)/4\n",
        "#     else:\n",
        "#         L_E = np.zeros(data.shape[1])\n",
        "#     if np.count_nonzero([reject[3],reject[10]]) != 0 :\n",
        "#         R_E = (ear_eeg[3]+ear_eeg[10])/np.count_nonzero([reject[3],reject[10]]) - (ear_eeg[4]+ear_eeg[5]+ear_eeg[6]+ear_eeg[11])/np.count_nonzero([reject[4],reject[5],reject[6],reject[11]]) # (ERA + ERB)/2 - (ERE + ERI + ERG + ERK)/4\n",
        "#     else:\n",
        "#         R_E = np.zeros(data.shape[1])\n",
        "\n",
        "#     return L_R, L_E, R_E\n",
        "\n",
        "# class EarEEG_MultiChan_Dataset(Dataset):\n",
        "#     def __init__(self, psg_file, label_file, device, mean_l = None, sd_l = None,\n",
        "#                 reject_list = None, transform=None, target_transform=None, sub_wise_norm = False):\n",
        "#         \"\"\"\n",
        "      \n",
        "#         \"\"\"\n",
        "#         # Get the data\n",
        "#         for i in range(len(psg_file)):\n",
        "#           if i == 0:\n",
        "#             if reject_list.any():\n",
        "#               psg_signal = read_h5py(psg_file[i])\n",
        "#               L_R, L_E, R_E = EEG_process(psg_signal,reject_list,i=i)\n",
        "#               # print(L_R.shape,L_E.shape,R_E.shape)\n",
        "#               # break\n",
        "#               L_R = np.reshape(L_R,(L_R.shape[0],1,L_R.shape[1]))\n",
        "#               L_E = np.reshape(L_E,(L_E.shape[0],1,L_E.shape[1]))\n",
        "#               R_E = np.reshape(R_E,(R_E.shape[0],1,R_E.shape[1]))\n",
        "\n",
        "#               # L_R = (L_R - np.mean(L_R))/np.std(L_R)\n",
        "#               # L_E = (L_E - np.mean(L_E))/np.std(L_E)\n",
        "#               # R_E = (R_E - np.mean(R_E))/np.std(R_E)\n",
        "#               eog =  np.reshape(psg_signal[16] - psg_signal[4],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "#               emg = np.reshape(psg_signal[22],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "#               # self.psg = np.concatenate((L_R,eog,emg),axis = 1) \n",
        "#               self.psg = np.concatenate((L_R,L_E,R_E),axis = 1) \n",
        "#               # self.psg = np.concatenate((L_E,R_E,eog),axis = 1) \n",
        "#               self.labels = read_h5py(label_file[i])\n",
        "#             else:  \n",
        "#               self.psg = read_h5py(psg_file[i])\n",
        "#               self.labels = read_h5py(label_file[i])\n",
        "#           else:\n",
        "#             if reject_list.any():\n",
        "#               psg_signal = read_h5py(psg_file[i])\n",
        "#               L_R, L_E, R_E = EEG_process(psg_signal,reject_list,i=i)\n",
        "#               L_R = np.reshape(L_R,(L_R.shape[0],1,L_R.shape[1]))\n",
        "#               L_E = np.reshape(L_E,(L_E.shape[0],1,L_E.shape[1]))\n",
        "#               R_E = np.reshape(R_E,(R_E.shape[0],1,R_E.shape[1]))\n",
        "\n",
        "#               # L_R = (L_R - np.mean(L_R))/np.std(L_R)\n",
        "#               # L_E = (L_E - np.mean(L_E))/np.std(L_E)\n",
        "#               # R_E = (R_E - np.mean(R_E))/np.std(R_E)\n",
        "#               eog =  np.reshape(psg_signal[16] - psg_signal[4],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "#               emg = np.reshape(psg_signal[22],(L_R.shape[0],1,L_R.shape[-1]))\n",
        "#               # psg_comb = np.concatenate((L_R,eog,emg),axis = 1) \n",
        "#               psg_comb = np.concatenate((L_R,L_E,R_E),axis = 1) \n",
        "#               self.psg = np.concatenate((self.psg,psg_comb),axis = 0)\n",
        "#               self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "#             else:\n",
        "#               self.psg = np.concatenate((self.psg, read_h5py(psg_file[i])),axis = 1)\n",
        "#               self.labels = np.concatenate((self.labels, read_h5py(label_file[i])),axis = 0)\n",
        "\n",
        "#         self.labels = torch.from_numpy(self.labels)\n",
        "#         print(f\"Data shape : {self.psg.shape}\")\n",
        "#         print(f\"Labels shape : {self.labels.shape}\")\n",
        "#         bin_labels = np.bincount(self.labels)\n",
        "#         print(f\"Labels count: {bin_labels/self.labels.shape[0]}\")\n",
        "#         print(f\"Labels count weights: {1/(bin_labels/self.labels.shape[0])}\")\n",
        "        \n",
        "#         if sub_wise_norm == True:\n",
        "#           print(f\"Reading Subject wise mean and sd\")\n",
        "#           for i in range(len(mean_l)):\n",
        "#             if i == 0:\n",
        "#               self.mean_l  = read_h5py(mean_l[i])\n",
        "#               self.sd_l = read_h5py(sd_l[i])\n",
        "#             else:\n",
        "#               self.mean_l = np.concatenate((self.mean_l, read_h5py(mean_l[i])),axis = 1)\n",
        "#               self.sd_l = np.concatenate((self.sd_l, read_h5py(sd_l[i])),axis = 1)\n",
        "\n",
        "#           print(f\"Shapes of Mean  : {self.mean_l.shape}\")\n",
        "#           print(f\"Shapes of Sd    : {self.sd_l.shape}\")\n",
        "#         else:     \n",
        "#           self.mean = mean_l\n",
        "#           self.sd = sd_l\n",
        "#           print(f\"Mean : {self.mean} and SD {self.sd}\")  \n",
        "\n",
        "#         self.sub_wise_norm = sub_wise_norm\n",
        "#         self.device = device\n",
        "#         self.transform = transform\n",
        "#         self.target_transform = target_transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         psg_data = self.psg[idx]         \n",
        "#         # print(data.shape)\n",
        "#         label = self.labels[idx,]\n",
        "        \n",
        "#         if self.sub_wise_norm ==True:\n",
        "#           psg_data = (psg_data - self.mean_l[idx]) / self.sd_l[idx]\n",
        "#         elif self.mean and self.sd:\n",
        "#           psg_data = (psg_data - self.mean_l) / self.sd_l\n",
        "\n",
        "#         if self.transform:\n",
        "#             psg_data = self.transform(psg_data)\n",
        "#         if self.target_transform:\n",
        "#             label = self.target_transform(label)\n",
        "#         return psg_data, label\n",
        "# train_dataset = EarEEG_MultiChan_Dataset(psg_file = train_psg_list, \n",
        "#                                        label_file = train_label_list, device = device, \n",
        "#                                     #    mean_l = val_mean_list, sd_l = val_sd_list,\n",
        "#                                        reject_list = train_reject_list,\n",
        "#                                        sub_wise_norm = False,\n",
        "#                                        transform=transforms.Compose([\n",
        "#                                         transforms.ToTensor(),\n",
        "#                                         # #  transforms.Normalize(\n",
        "#                                         #     (0.5,), (0.5,))\n",
        "#                                         ]) )\n",
        "# val_dataset = EarEEG_MultiChan_Dataset(psg_file = val_psg_list, \n",
        "#                                        label_file = val_label_list, device = device, \n",
        "#                                     #    mean_l = val_mean_list, sd_l = val_sd_list,\n",
        "#                                        reject_list = val_reject_list,\n",
        "#                                        sub_wise_norm = False,\n",
        "#                                        transform=transforms.Compose([\n",
        "#                                         transforms.ToTensor(),\n",
        "#                                         # #  transforms.Normalize(\n",
        "#                                         #     (0.5,), (0.5,))\n",
        "#                                         ]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsBUkvjHw02T"
      },
      "outputs": [],
      "source": [
        "# def plot_confusion_matrix(cm,\n",
        "#                           target_names,\n",
        "#                           title='Confusion matrix',\n",
        "#                           cmap=None,\n",
        "#                           normalize=True):\n",
        "\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import numpy as np\n",
        "#     import itertools\n",
        "\n",
        "#     accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "#     misclass = 1 - accuracy\n",
        "\n",
        "#     if cmap is None:\n",
        "#         cmap = plt.get_cmap('Blues')\n",
        "\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.imshow(cm, interpolation='nearest', cmap=cmap,vmin =100 ,vmax = 15000)\n",
        "#     plt.title(title,fontsize = 20)\n",
        "#     # plt.colorbar()\n",
        "\n",
        "#     if target_names is not None:\n",
        "#         tick_marks = np.arange(len(target_names))\n",
        "#         plt.xticks(tick_marks, target_names, rotation=45, fontsize = 15)\n",
        "#         plt.yticks(tick_marks, target_names,fontsize = 15)\n",
        "\n",
        "#     if normalize:\n",
        "#         cm2 = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "#     thresh = cm.max() / 3 if normalize else cm.max() / 2\n",
        "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#         if normalize:\n",
        "#             plt.text(j, i, \"{:,}\\n{:0.2f}%\".format(int(cm[i, j]),cm2[i, j]*100),\n",
        "#                      horizontalalignment=\"center\", verticalalignment=\"center\",\n",
        "#                      color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 13)\n",
        "#         else:\n",
        "#             plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "#                      horizontalalignment=\"center\",\n",
        "#                      color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.ylabel('True label',fontsize = 18)\n",
        "#     plt.xlabel('Predicted label',fontsize = 18)#\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8whYXokzw02T"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# lr = 0.001\n",
        "# beta_1 =  0.9    \n",
        "# beta_2 =  0.999\n",
        "# eps = 1e-9\n",
        "# n_epochs = 50\n",
        "# weights = torch.tensor([1., 4., 2., 3., 3.])#[1.,2.,1.,2.,2.])#[1., 4., 2., 3., 3.])  #valset 1: ([1, 3., 1., 5.5., 2.5])                     #([1., 2., 1., 2., 2.])\n",
        "# print(f\"weights: {weights}\")\n",
        "# criterion = nn.CrossEntropyLoss(weight=weights)  #### Check the Criterion Jathu\n",
        "# optimizer = torch.optim.Adam(Net_s.parameters(), lr=lr, betas=(beta_1, beta_2),eps = eps, weight_decay = 0.0001)\n",
        "\n",
        "\n",
        "# pred = Net_t(psg_data[:,:,0,:].float().to(device), psg_data[:,:,1,:].float().to(device), psg_data[:,:,2,:].float().to(device))#train_features_2.float())  #,enc1,atten_feat\n",
        "# print(pred.shape)\n",
        "# # print(torch.argmax(pred, 1))\n",
        "# pred = Net_s(psg_data[:,:,0,:].float().to(device), psg_data[:,:,1,:].float().to(device), psg_data[:,:,2,:].float().to(device))#train_features_2.float())  #,enc1,atten_feat\n",
        "# print(pred.shape)\n",
        "# # this is for Neptune\n",
        "# parameters = {\n",
        "#     \"Subject \" : \"9\" ,\n",
        "#     \"Experiment done by\" : \"Mithunjha\",\n",
        "#     \"Experiment\" : \"Knowledge distillation - teacher & STUDENT pretrained\" ,\n",
        "#     'Model Type' : \"scalp (C3-O1,C4-O2,EOG) and ear EEG (L,R,EOG)\",\n",
        "#     'filt_ch' : 64,\n",
        "#     'Batch Size': batch_size,\n",
        "#     'Loss': f\"Weighted Categorical Loss {weights}\", \n",
        "#     'Optimizer' : \"Adam\",        \n",
        "#     'Learning Rate': lr,\n",
        "#     'eps' : eps,\n",
        "#     'n_epochs': n_epochs,\n",
        "#     'val_subject' : val_data_list[0]+1 \n",
        "# }\n",
        "# run['model/parameters'] = parameters\n",
        "# run['model/model_architecture'] = Net_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAv7VEk-z9Ch"
      },
      "source": [
        "### Classification Model Cross Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUbfwxu4z9Ch"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import MultiheadAttention\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Q-b3yRz9Ci"
      },
      "outputs": [],
      "source": [
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04tRDTvyz9Ci"
      },
      "outputs": [],
      "source": [
        "#input ==> 32, 1, 1, 3000,, b==> batch, e==> embedding, s==> seq length\n",
        "class Window_Embedding(nn.Module): \n",
        "    def __init__(self, in_channels: int = 1, window_size: int = 50, emb_size: int = 64):\n",
        "        super(Window_Embedding, self).__init__()\n",
        "\n",
        "        self.projection_1 =  nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//4, kernel_size = window_size, stride = window_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(emb_size//4),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        self.projection_2 =  nn.Sequential(#################\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//8, kernel_size = 5, stride = 5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//8, emb_size//4, kernel_size = 5, stride = 5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//4, (emb_size-emb_size//4)//2, kernel_size = 2, stride = 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d((emb_size-emb_size//4)//2),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        \n",
        "        self.projection_3 =  nn.Sequential(#################\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(in_channels, emb_size//4, kernel_size = 25, stride = 25),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(emb_size//4, (emb_size-emb_size//4)//2, kernel_size =2, stride = 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d((emb_size-emb_size//4)//2),\n",
        "            # Rearrange('b e s -> b s e'),\n",
        "            )\n",
        "        \n",
        "        \n",
        "        self.projection_4 = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains, in=>B,1,3000 out=>B,64,60\n",
        "            nn.Conv1d(emb_size, emb_size, kernel_size = 1, stride = 1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(emb_size),\n",
        "            Rearrange('b e s -> b s e'),)\n",
        "            \n",
        "        #in=>B,64,60 out=>B,64,61\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "        self.arrange1 = Rearrange('b s e -> s b e')\n",
        "        #in=>61,B,64 out=>61,B,64\n",
        "        self.pos = PositionalEncoding(d_model=emb_size)\n",
        "        #in=>61,B,64 out=>B,61,64\n",
        "        self.arrange2 = Rearrange('s b e -> b s e ')\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x.squeeze().unsqueeze(dim = 1)\n",
        "        # print(x.shape)\n",
        "        b,_, _ = x.shape\n",
        "        x_1 = self.projection_1(x)  ########################\n",
        "        x_2 = self.projection_2(x) ###########\n",
        "        x_3 = self.projection_3(x) \n",
        "        # print(x_local.shape,x_global.shape)\n",
        "        x = torch.cat([x_1,x_2,x_3],dim = 1)##### 2)\n",
        "        x = self.projection_4(x) \n",
        "        # print(x.shape)\n",
        "        cls_tokens = repeat(self.cls_token, '() s e -> b s e', b=b)\n",
        "        # print(cls_tokens.shape)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # print(x.shape)\n",
        "        # add position embedding\n",
        "        x = self.arrange1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.pos(x)\n",
        "        # print(x.shape)\n",
        "        x = self.arrange2(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AQnR8slw02K"
      },
      "outputs": [],
      "source": [
        "#input ==>(b,s,e)=>(32, 61, 64,) \n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Intra_modal_atten(nn.Module): \n",
        "    def __init__(self, d_model=64, nhead=8, dropout=0.1,\n",
        "                 layer_norm_eps=1e-5, window_size = 25, First = True,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        super(Intra_modal_atten, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "       \n",
        "        if First == True:\n",
        "            self.window_embed = Window_Embedding(in_channels = 1, window_size = window_size, emb_size = d_model)\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)  \n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True,\n",
        "                                            **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout) \n",
        "        self.First = First\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.First == True:\n",
        "            src = self.window_embed(x)\n",
        "        else:\n",
        "            src = x\n",
        "        # print(src.shape)\n",
        "        # src = self.norm(src)  #####\n",
        "        # print(src.shape)\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout(src2)\n",
        "        out = self.norm(out)   ########\n",
        "        return out                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG_SevORw02K"
      },
      "outputs": [],
      "source": [
        "##Cross Modal Attention\n",
        "#input ==>(b,s,e)=>(32, 2, 64,) ==> Class tokens of EEG and EOG after intra modal attention\n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Cross_modal_atten(nn.Module): \n",
        "    def __init__(self, d_model=64, nhead=8, dropout=0.1,\n",
        "                 layer_norm_eps=1e-5, First = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "\n",
        "        super(Cross_modal_atten, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "\n",
        "        if First == True:\n",
        "            self.cls_token = nn.Parameter(torch.randn(1,1, d_model)) ######\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)  \n",
        "        self.cross_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True,\n",
        "                                            **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout) \n",
        "        self.First = First\n",
        "\n",
        "    def forward(self, x1: Tensor,x2: Tensor) -> Tensor:\n",
        "        # print(x1.shape,x2.shape)\n",
        "        if len(x1.shape) == 2:\n",
        "            x = torch.cat([x1.unsqueeze(dim=1), x2.unsqueeze(dim=1)], dim=1)\n",
        "        else:\n",
        "            x = torch.cat([x1, x2.unsqueeze(dim=1)], dim=1)\n",
        "        # print(x.shape)\n",
        "        b,_, _ = x.shape\n",
        "        if self.First == True:\n",
        "            cls_tokens = repeat(self.cls_token, '() s e -> b s e', b=b)  ######\n",
        "            # print(cls_tokens.shape)\n",
        "            # prepend the cls token to the input\n",
        "            src = torch.cat([cls_tokens, x], dim=1)  #####\n",
        "        else:\n",
        "            src = x\n",
        "        # print(src.shape)\n",
        "        # src = self.norm(src)#####(src)\n",
        "        # print(src.shape)\n",
        "        src2 = self.cross_attn(src, src, src)[0]\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout(src2)\n",
        "        out = self.norm(out)\n",
        "        return out  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO5AZv4Dw02K"
      },
      "outputs": [],
      "source": [
        "##Feed Forward Networks\n",
        "#input ==>(b,s,e)=>(32, 61, 64,) \n",
        "# b==> batch, s==> seq length, e==> embedding, \n",
        "class Feed_forward(nn.Module): \n",
        "    def __init__(self, d_model=64,dropout=0.1,dim_feedforward=512,\n",
        "                 layer_norm_eps=1e-5,\n",
        "                 device=None, dtype=None) -> None:\n",
        "\n",
        "        super(Feed_forward, self).__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "\n",
        "        self.norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # print(x.shape)\n",
        "        # src = self.norm(x)  ######\n",
        "        src = x\n",
        "        # print(src.shape)\n",
        "        src2 = self.linear2(self.dropout1(self.relu(self.linear1(src))))\n",
        "        # print(src2.shape)\n",
        "        out = src + self.dropout2(src2)\n",
        "        out = self.norm(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hcrjJ92w02K"
      },
      "outputs": [],
      "source": [
        "# Best Model so far fine tuning\n",
        "class Cross_Transformer_Network(nn.Module):\n",
        "    def __init__(self,d_model = 64, dim_feedforward=512,window_size = 25): #  filt_ch = 4\n",
        "        super(Cross_Transformer_Network, self).__init__()\n",
        "        \n",
        "        self.eeg_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1,\n",
        "                                            window_size =window_size, First = True )\n",
        "        self.eog_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "                                            window_size =window_size, First = True )\n",
        "        self.eeg2_atten = Intra_modal_atten(d_model=d_model, nhead=8, dropout=0.1, \n",
        "                                            window_size =window_size, First = True )\n",
        "        \n",
        "        self.cross_atten = Cross_modal_atten(d_model=d_model, nhead=8, dropout=0.1, First = True )\n",
        "        \n",
        "        self.eeg_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "        self.eog_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "        self.eeg2_ff = Feed_forward(d_model = d_model,dropout=0.1,dim_feedforward = dim_feedforward)\n",
        "\n",
        "\n",
        "\n",
        "        self.mlp    = nn.Sequential(nn.Flatten(),\n",
        "                                    nn.Linear(d_model*3,5))  ##################\n",
        "        # \n",
        "\n",
        "    def forward(self, eeg: Tensor,eog: Tensor,eeg2: Tensor,finetune = True): \n",
        "        self_eeg = self.eeg_atten(eeg)\n",
        "        self_eog = self.eog_atten(eog)\n",
        "        self_eeg2 = self.eeg2_atten(eeg)\n",
        "        # print(self_eeg.shape,self_eeg2.shape)\n",
        "        self_eeg_new = torch.cat((self_eeg[:,0,:].unsqueeze(dim=1),self_eeg2[:,0,:].unsqueeze(dim=1)), dim=1)\n",
        "        cross = self.cross_atten(self_eeg_new,self_eog[:,0,:])\n",
        "\n",
        "        cross_cls = cross[:,0,:].unsqueeze(dim=1)\n",
        "        cross_eeg = cross[:,1,:].unsqueeze(dim=1)\n",
        "        cross_eog = cross[:,2,:].unsqueeze(dim=1)\n",
        "\n",
        "        eeg_new =  torch.cat([cross_cls, self_eeg[:,1:,:]], dim=1)\n",
        "        eog_new =  torch.cat([cross_cls, self_eog[:,1:,:]], dim=1)\n",
        "        eeg2_new =  torch.cat([cross_cls, self_eeg2[:,1:,:]], dim=1)\n",
        "\n",
        "        ff_eeg = self.eeg_ff(eeg_new)\n",
        "        ff_eog = self.eog_ff(eog_new)\n",
        "        ff_eeg2 = self.eeg2_ff(eeg2_new)\n",
        "\n",
        "        \n",
        "\n",
        "        # cls_out = torch.cat([cross_cls[:,0,:],ff_eeg[:,0,:], ff_eog[:,0,:]], dim=1).unsqueeze(dim=1) ######\n",
        "        cls_out = torch.cat([ff_eeg[:,0,:], ff_eog[:,0,:],ff_eeg2[:,0,:]], dim=1).unsqueeze(dim=1) \n",
        "\n",
        "        feat_list = [cross_cls,ff_eeg,ff_eog,ff_eeg2]\n",
        "        if finetune == True:\n",
        "            out = self.mlp(cls_out)  #########\n",
        "            return out,cls_out,feat_list\n",
        "        else:\n",
        "            return cls_out#feat_list"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y3VRBFn7w02N",
        "BGlbwPN1z9Co"
      ],
      "name": "New - Knowledge_Distil_V2 JP Edit.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "94c91063cbfafe7dd443522ce7f45eaee09f9c12e6866a8b2d3ce13a69535fc6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('sleep_monitoring': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}